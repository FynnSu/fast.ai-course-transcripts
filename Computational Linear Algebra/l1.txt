 Yeah, so I took a kind of very theoretical linear algebra class my first semester of college. It was all proof-based, so no computations basically, and I loved it. It convinced me to be a math major instead of computer science. In hindsight it might not have been the most practical introduction, but I really enjoyed it. I did a math PhD and I took numerical linear algebra my first semester and really liked the course. Numerical linear algebra is a very different perspective on linear algebra in general because it's all about kind of how computers do it. And that kind of in many ways blew my mind to have this like very different perspective on a subject that I liked. And it was still, it was a great course. I'll be kind of teaching with a very different approach and focus though from a traditional course. I wanted to highlight I had an internship my last year of graduate school in healthcare economics and I think that's the first place where I was really using linear algebra in like this is a business where these people are using linear algebra every day. And we'll actually see an example pretty similar to some of the stuff I did in a moment. Yeah I worked as a quant for two years. I was, oh question? Oh that was Steven Marrer. Oh that's awesome. Very cool. Yeah that's so great. So yeah I was a quant for two years which is a lot of working with data and I would say linear algebra kind of yeah having tables of data and that convinced me to become a data scientist. I was at Uber and then I taught software engineering at Hackbright. That's, it was mostly software development. I did get to overhaul their machine learning and collaborative filtering lectures which was exciting. And then one year ago Jeremy and I started fast AI to kind of make deep learning easier to use. So going into teaching this, it's a different, I have a different approach than most math courses. It's gonna be top-down so I'm in a bottom-up approach which is kind of the more common one in math. That's where you first learn all these separate components that you'll need and then you kind of build things of increasing complexity as time goes on and you know more components. And that's kind of tough because people lose motivation, they don't have a sense of the big picture. And what I'll be doing instead is kind of starting with kind of doing interesting things using algorithms and then we'll kind of go into more depth and break down the pieces. And I apologize if you heard, I gave a talk on this at the Friday seminar a few weeks ago. So sorry if this is repeat. But there's a wonderful book called Making Learning Whole where a Harvard professor uses an analogy with baseball and says you know we don't require kids to learn all the, memorize all the formal rules of baseball before they're allowed to play. You know we let them play and then over time they learn more and more of the formal rules. Yeah and so kind of don't worry if at first you don't understand everything that's going on. That's kind of the point. And focus on what things do as opposed to what they are. So with these matrix decompositions it's really important to know what type of matrices are you getting back from the decomposition. And then over time we'll kind of get into how would you program them. So for the course I have two textbooks. Neither of these is required and I've asked Kirsten to buy a few copies to have on hand. So my number one choice is Numerical Linear Algebra by Trevathan. It's a really well-written book and I'll say kind of when I'm referencing parts of it. And then kind of a secondary book that I liked is Numerical Methods by Greenbaum and Chartier. And this is actually intended for kind of senior undergrad courses and it includes numerical linear algebra but it also includes Monte Carlo methods, numerical differentiation. I think it's a really interesting book and it has a lot of applications. It's a fun fact. Chartier is actually, he's a math professor but he trained as a mime under Marcel Marceau and I saw him perform at a math conference on how to kind of use miming to teach math. But it's a very accessible book. So I was gonna hold office hours from 2 to 4 on Friday afternoons after the seminar. If that doesn't work feel free to email me about another time. Yeah my email is rachel at fast.ai and there's a class slack channel although I haven't done anything with that yet but I'll send you invites. There's the link for the github. Oh and then I wanted to, so this is important to note, kind of a difficult thing about Jupyter notebooks is even just running them changes the code and so you can get merge conflicts if you've cloned the repository and so it's up to you if you want to deal with those or it might just be easier to kind of download the notebooks. And there are places that I kind of leave blank for exercises. So the idea is you will be doing some coding in them as well. And I just included these to check if you have MathJax running which renders LaTeX and I believe if you're using Anaconda that's automatically installed. But let me definitely let me know and sooner rather than later if you're having any trouble with the setup for this or any of the imports or anything because I definitely want you to be able to run these notebooks at home. So here is a, actually I should check, any questions so far? Okay here's a grading room work. So there'll be some homework assignments and I'll give you always a full week to do the homework from when I when I give it. I want losing your writing assignment for this course and you can choose the topics and actually I'll say a little bit more about that in a moment but that's kind of broken into three pieces just your proposal, your draft, and then the final and then there'll be a final exam. So no cheating or plagiarism is allowed and you know we have the standard honor code from USF which I'm sure you're really familiar with from this point. For laptops please avoid surfing the web or using social media or messaging during class. Well and then here's a syllabus. I'll just say kind of that um the way it'll work is kind of each lesson is mostly centered around an application and then we'll kind of dive into algorithms and tech or techniques that are used for that application but it's kind of this almost application first approach. So we have the introductory lesson which is a little bit unusual in that it'll be less code and it's more kind of introducing concepts but then we'll talk about topic modeling using NMF and SVD, background removal with robust PCA, compressed sensing for CT scans which are kind of some really interesting looking kind of CT scan like pictures, predicting health outcomes, this is on a diabetes data set, page rank to go through eigendecompositions and how you program those, and then finally the QR factorization which will have shown up in almost all of the previous lessons as a tool. Okay so about the writing assignment, yeah writing about technical concepts is really valuable. I hope that you'll publish it as a blog post, you don't have to, and if you do it's a really good name to kind of get your way to get your name out there, something to show to future employers or when you're applying for jobs. Technical writing is also important when you're creating documentation at work, kind of explaining and sharing your ideas with co-workers, applying to speak at conferences, and practicing for interviews, kind of just even practicing like how do you explain what you're doing. So I have a list of ideas, oh and I actually, oh yeah this is, I might have an updated version, hold on a moment, yeah this is the old version. Well escape is not taking me out of full time. F11? Okay well I'm running this from a different repository. Let me just go here. Oh no, that's the issue. And with all these ideas, these are just suggestions, if you have a different idea that's not on the list, feel free to ask me about it. I really want it to be something that you're interested in learning more about. So I have a list of, there are so many numerical linear algebra algorithms and so we'll get into most of the core ones, but there are a lot that we won't have time to cover so you could choose one of those. So here's a list of several of them. You could also speed up an algorithm of your choice and that could either be something that we've covered in the course or not using the GPU and PyTorch and we'll be talking about how to do that in the first lesson. Number, Cython, which we'll cover, parallelization, or randomized projections. So randomized algorithms are a really interesting area that give you a kind of a lot of speed up. You could also find an interesting academic paper that you've been wanting to read and summarize and implement it. And then here, let me go to this link, and there's something called the matrix factorization jungle, which is just a handy web page that someone put together with kind of a list of a ton of different matrix algorithms. And so if you scroll, they're kind of whole sections on different concepts. And then I posted a link to, oh one other thing I wanted to say, so you can do your blog post or your writing assignment as a Jupyter notebook. I mean you can also do it as a Kaggle kernel and you can, Kaggle kernels kind of are Jupyter notebooks, but that's another kind of great way to share your work and you can also share data sets that way. And then I've linked to several number of technical blog posts that I like, and I think that's something that's kind of good to get into the habit of, and maybe after you graduate and have more time, but just kind of reading other people's blog posts. And so for the proposal, that's just gonna be a brief paragraph about what you want to do. I should also say it could be like an experiment you want to do, like if you're curious, you know, how does, I don't know, changing this factor affect this algorithm, you can propose that. And then four sources. Any questions? Okay, so I'll try, I'll try to review some linear algebra in class. If there are things that you feel like you need extra review in, these are some resources I recommend. One is three blue, one brown. This I would recommend to everybody for any reason. It's just they're really fantastic videos, and I'll probably show one in class in a later lesson, but it's a very, very kind of geometric and visual perspective on linear algebra, and so it's very different from how most linear algebra courses are taught. The guy who created these wrote his own graphics library because he wanted to do things that he wasn't able to do otherwise, but they're really beautiful. And so if you're a visual learner, I would definitely recommend them. There's also an immersive linear algebra free online textbook. Chapter 2 of Ian Goodfellow's Deep Learning book is all about linear algebra, and that's kind of coming from a machine learning perspective. Yes, I think that's it. So then the USF policies are in here, but again you should have seen these in most of your courses about academic integrity, accommodations for disabilities, and so on. All right, you guys ready for the first lesson? And again, this is going to be less codeful than future lessons, but it introduces some really interesting concepts, I think. So kind of why are we setting this? So the key question of this course is how can we do matrix computations with acceptable speed and acceptable accuracy? And this is from a news algorithm, or not a news algorithm, a journal article that came up with the top 10 algorithms of science and engineering during the 20th century. And three of the things on the list we'll cover in this course, which is exciting. And one is the idea of matrix decompositions as an approach to linear algebra itself, because it's such a powerful idea. So a lot of this course is about breaking matrices into other matrices that are going to be easier to work with. And so they're going to be four things to keep in mind broadly when choosing or designing an algorithm. We'll go into each of these in a little bit more depth. Memory use, speed, accuracy, and scalability. And then kind of on the motivation of doing this, so you know a lot of the things we'll talk about, you know, could be done in Scikit-Learn or SciPy. It's really great to know kind of how those libraries work in case that... in case you want to do a variation that's not accounted for, knowing the trade-offs that you're making when you choose between different options. Also a lot of these fields are moving very quickly and you might find new results that haven't yet been implemented in one of these libraries, particularly the areas of deep learning, recommendation systems, approximate algorithms, and graph analytics. There's a lot of research happening in all of those right now. And so yeah, knowing how to debug algorithms can really kind of accelerate your work. Or sorry, knowing how they work helps you debug them, but knowing how to debug them also helps accelerate what you're doing. So in this part we'll probably be review. I just want to say there are two main types of matrix computations, which are taking products and then decomposition. So putting them together and then pulling them apart. So for an example, here this is a matrix. I'm going to make this a little bit smaller just so it can fit on one screen. This is a matrix of... sorry. Yeah, well it's a matrix of probabilities, but this is a Markov chain with different states of health. And so there's kind of an asymptomatic HIV stage, symptomatic full-blown AIDS and death, and these are kind of the different states of the Markov chain. And then this matrix of probabilities is saying what are your chances if you're in one state of moving to each of the other states. And so you'll notice each row sums to one since they're probabilities. And that's kind of the row gives you the state you're starting in, the column gives you the destination state you're moving to. So I mentioned earlier I had an internship while I was in grad school that was a whole kind of research group that looked at problems like these. They would also take into account kind of the cost of health care and different types of treatment and use that to kind of weigh recommendations. So here I want you to take if this is kind of your starting vector of what, you know, you have a group of people 85% are in the asymptomatic group, 10% are symptomatic, and so on. And if this matrix kind of is giving you the probabilities of what their health will be like in a year, can you tell me what those probabilities are? So take a take a few moments to code that, just to warm up. Oh yes, although I don't want to, so if you go to the answer tab it should show up as a cell that says exercise. I'm not opening mine because I have the answer written there. It shows you what the correct answer should be. Oh oh yes yeah it shows you the output as well. Yes? Oh yes, okay yeah. So the question was from someone not registered for the class, so let me pull up the GitHub. So on GitHub it's users fast AI and then numerical dash linear dash algebra is the repository. And I've also put the syllabus in the readme of the repository kind of with links so you can view the notebooks and we'll be adding to it. Raise your hand if you're done. Raise your hand if you want more time. Let's look at the answer. So you can use NumPy to put these in as arrays. So this is a two-dimensional array for the matrix, the vectors, but actually I guess I put that as two-dimensional as well. So I did A dot T which is transpose. The at sign is matrix multiplication in Python 3, I believe you need. So for Python 2 you could be using NP dot matmul for matrix multiplication, although I highly recommend switching to Python 3. So does anyone want to say why am I doing a transpose? Yeah, yeah, and so it would have been fine if you had multiplied on the left. Also the other way I think about it is... Just by the way, the reason we're using the microphone is so we can hear it on the recording. The other way to think about this is your matrix, kind of the dimensions are basically, let me point, like sources by destinations and so I kind of think of like when you do the matrix multiplication you're wanting to multiply it by the sources and so I know if you want the sources over here you would need the sources to be the columns to kind of line up as you want, you know, sources by sources. Any questions? Oh well no, that would work as well. That's actually equivalent. The kind of taking two transposes is, I can write this, like this is equal to like that whole thing transposed. Yeah, so now for our matrix matrix products. This is a problem that I've taken from kind of this fact sheet that has several different linear algebra problems and so here this is, and a lot of them, I think a lot of times when you're doing things by hand they look like overly simplified examples, but it's important to remember that the power of matrices is that you can do these on really large data sets and large matrices as well, but here you've got three people who want to buy some different groceries and they have different prices for two different shops. Each person only wants to go to one shop which is the better shop for each person to go to. So take a moment to do that and again if you go to the answer tab there'll be a little bit a little bit of space and it should show you what the ideal answer is. Raise your hand if you're finished. Raise your hand if you want some more time. Okay, go ahead and look at the answer for this. Yeah, so this is kind of pretty straightforward of entering the matrices as NumPy arrays and I did A at B or you could do np.matmall if you're in Python 2. Any questions? And this, if you look at, I think it's even in the part I copied, it's kind of nice how they copied out this example of, you know, the amount spent by person 1 is this row and we're multiplying it by this column to get what they would spend in shop 1. Oh, so next up is image data. I really like this GIF that illustrates how images can be represented by a matrix of numbers and so here this is black and white and the values are between 0 and 255 to show that this is the handwritten digit 8 and it could be represented by a matrix of, not exactly sure if this is like 20 by 20 or so on. And typically a lot of times what happens is that matrix might be unrolled then into a single row, but now you've got, you know, 400 numbers representing what that picture looked like. And for color images you just have three matrices, one for red, green, and blue. Any questions about that? Okay, so we're gonna look at convolutions briefly. So this is not a deep learning course, but I think deep learning is a really good illustration of how linear algebra is being heavily used right now. So convolutions are the heart of convolutional neural networks, CNNs, and so basically pretty much any results you hear that have AI or deep learning that are related to images are using CNNs. And then even, this is just from a few weeks ago, Facebook's AI team published results for speech translation where they use CNNs instead of RNNs, which are kind of typically what people use for language, and they were nine times faster. So convolutions are very useful. So using convolutions or convolutional neural networks, computers are more accurate than people at classifying images. I should zoom in on these. So some of these I wouldn't get. This is an ultra marathon, not a half marathon. So the computer got the top choice was ultra marathon, their second guess was half marathon, third guess was running, and fourth guess was marathon. So a lot of times these are very fine-grained categories or distinctions. I like this one. This is a heptathlon, not a decathlon, hurdles, or pentathlon. But this is what computers are capable of. And then here's an example of an algorithm to kind of find bounding boxes for different objects inside a picture, and then identify what the object is. Oh my goodness, okay. Wow, so that's even more impressive. I think this was done on videos is what Jeremy just said. Yeah, so this one they've identified two different chairs in the picture, you know, including this one which is kind of you're only seeing part of it and it's in the dark, you know, in a person and a dog. And this is pretty intricate. There are a lot of objects overlapping each other and algorithms recognizing them. And so we will not be getting into the full details of this, but I wanted to talk a little bit about how convolutions work since they're useful building block for deep learning and an application of linear algebra. So this is some images from a blog post. Yeah, written by a student in the deep learning class that was here at the Data Institute. And the idea behind a convolution is that it applies a filter. So here we've got a filter that's just four numbers, alpha, beta, gamma, and delta, and it's being applied to a picture perhaps that's just three by three, so pretty small. And you kind of put it in each location, so we put it in the top left corner and then we'll multiply alpha by A, add that to beta times B plus D times gamma plus E times delta, and get a single number out P as the result. And then you slide that filter across the picture and do it at each possible space. So here it is in the top right, we get out one result, bottom left, bottom right. So this is just with a single filter on a small picture. And so that's kind of one way to think about how a convolution works. Another is, and I find these pictures less helpful, but a lot of people like to draw neural networks from this point, so this is a neural network. Here the alpha, beta, gamma, and delta are the connections, and so those would be the weights on the connections. So whenever you see a red line, you know, that's saying there's a connection between A and P, and the weight of that connection is alpha. And so the same operation is happening that we saw before to get P. P's, you know, got four connections going into it. A times alpha, B times beta, D times, is that right? Yeah, D times gamma, and then, what else, E times delta. So that's another perspective. And I really, I really like this approach of thinking about topics from different perspectives, because I think that kind of help, can help you get a deeper understanding. And then this is, this is neat. Here Matthew's kind of unrolled the the filter and put it into this larger sparse matrix and shown, hey, this is actually a matrix multiplication. So now our kind of A, B, C, D, E from our picture is just a single vector, and we've got this sparse matrix, and that just sparse means it has a lot of zeros. And those actually show up a lot, kind of matrices that have lots of zeros in a specific structure, like this one does with the diagonals. And you can do a matrix multiplication and get the same result. Any questions? Okay, so now we're going to look at how we could use this for edge detection in this notebook. And this, don't worry, don't worry too much about the setup, but these are kind of the files you need to, or libraries you need to import. Yes? Yes, oh thank you. This is convolution intro, and this was originally part of the deep learning course. So we'll be looking at MNIST data, which is, you know, this really popular data set of lots of handwritten digits. This is very useful for banks being able to automatically identify, you know, when you insert your check into the ATM, what the numbers on it are, post offices automatically sort our mail by zip code using image recognition on the digits. And then I should say scikit-learn has a lot of built-in data sets, which are a really useful feature, and we'll be using several of them in this course. Yeah, so we kind of import, and here we're, so for the larger data sets that scikit-learn includes, it doesn't include the data set, it includes a data loading utility that you can run to get the actual data. So we run that. You can kind of check what the keys are of what you get back, because you're kind of getting back this dictionary-like object. We're interested in the data and the target, and target is going to be kind of the label of saying this is what the digit is. The data itself, and then something else that's always great to do whenever you're kind of starting anything, is just check your dimensions to see if they are what you expect. And you can often also kind of find stuff about the meaning based on the dimensions. So here this is 70,000 by 784. So even if you didn't know, you could guess, hey maybe this is 70,000 different samples or different digits. And this is a 28 by 28 if it was put back together. So each row is just a single digit that's kind of been unrolled. And so we're going to reshape them to be 28 by 28 using NumPy's reshape. So now we have, and often so kind of higher dimensional matrices are referred to as tensors. So you could say this is a tensor that's 70,000 by 28 by 28. So for the labels, we're converting them to integers. And then we're gonna, I think it's actually best to probably kind of skip to looking at these places. So here we've plotted images zero. So that's the kind of first entry of images. And you could confirm that's 28 by 28. That's our our picture. So we plot it. It's a zero. We check the label and that is also, or says zero. Was that a hand over here? Okay so that's a great question. Oh okay yeah the question was why are we dividing by 255 in I guess input 53 which I should probably run again. And this you you wouldn't have to and it would still plot properly. This comes up later when when we're using correlate I believe. But yeah if you plot it if you, so we're trying to turn these into numbers between zero and one. It would still work if you had them between zero and 255. So kind of just a way of normalizing. Or I'm sorry I should say the plots would still work. It would still be when you plot it you'd be like this is clearly the same image. Some of the computations we're gonna use later we needed it to be normalized for. Here we also have a plots helper function and so these were the methods that were kind of defined up here. Although I would say don't worry too much about the details of them unless you're particularly interested. We're using and it lets us put in a whole array of images and plots them like this which is really handy for being able to look at our data. And this is also something I would recommend. I think sometimes it can feel kind of finicky writing the helper methods to be able to look at your data, but it's pretty much always worth it because as you're doing computations you want to check that things are what you think they are and be able to see what your results are. We can also zoom in on our images. So if you want to see just a plot of part of one here we're just getting the rows from 0 to 14, columns from 8 to 22. So this is kind of the middle top of the 0 is what this this thing is from this picture. Okay so for edge detection we're gonna have a matrix with the name that kind of gives a lot away called top that's negative ones along the top row, ones along the middle row, and then zeros along the bottom. And this is why that what top looks like. And so something to keep in mind and actually oh here this is an interesting perspective and this could have been in higher up. Using NumPy we can look at kind of just a part of the the matrix and see this is so it's not plot plotted but this is what the matrix looks like. And here the zeros are black and these numbers between 0 and 1 are giving the intensity of the white part for the handwritten 0. So we're still still kind of looking at this just from different perspectives. And so we're using a method called correlate and this came from it's scikit-learn's image. Oh there it is. Oh sorry, scipy's nd image filters provides a correlate. Correlate method. And then something you can do that's a nice feature of Jupyter notebook is if you're inside the parentheses for a method if you hit shift tab a few times it pulls up the method signature and documentation which is nice to see. And so this gives an array correlated with a given kernel and so here we pass in images 0 and top and if we plot that this is what we get. And so you'll kind of notice that they're white which is the highest value along the tops of the 0 and black kind of the lowest values along the bottoms of the edges. So this is picked off the edges. I'm going to talk about kind of what's going on there with this negative 1, 1, and 0. A way to think about that that's going to be greatest when the negative ones are multiplying by zeros and getting canceled out. If we were trying to think about how could we maximize you know top multiplied by something else. And this is I should be clear this is element wise multiplication we're doing so this is not a matrix product but we're element wise kind of you know putting the filter on top of something and then multiplying each element on what it's kind of on top of. And so having zeros in a full row and then having like the highest value since this is normalized there should be ones and another row that would give the biggest value for this. And so that's why it's picking out tops because it's whenever you go from something small to something large this this correlation will have the highest values. Your questions? I yes yeah okay so Jeremy asked the question about convolution versus core or suggested that I talked about convolution versus correlation. The key difference is just with convolutions they're actually flipped and so this is kind of a mathematical accounting thing right like there's not a yeah that it's yeah so it's really it's the same and kind of in the math you take into account like oh okay this has actually been rotated when you're doing a convolution but it's the same idea as a correlation and I think correlations are easier to think about otherwise you're just kind of flipping everything but getting the same result. This is actually element wise. Oh yes yeah so this is this is different from a correlation matrix that you hear about of in statistics yeah so kind of overuse of the word correlation yeah this is a different use. Yeah but yeah but think about that kind of in a separate bucket from the statistics idea of correlation between between different variables. Yeah so that was really kind of the key idea of how a matrix can be used for edge detection. Here we'll see if we rotate so remember top was that three by three matrix we can rotate it by 90 degrees. Oh okay so now it's identical because we've rotated it so it still does the same thing. I would I would not worry too much about this distinction the key thing here is just the idea of you can pick up edges by sliding a filter and then this is kind of nice we rotate number of times kind of to get these different ones and this will give us edge detection for bottom top left and right can also do diagonals and so if we apply that kind of all these different filters to the the picture of the zero here you see we've picked off the top this one's picked off the left-hand side since that's where the white marks are picking off the bottom right-hand side this is picking off kind of the diagonals towards the bottom right corner white here it's kind of on the diagonals I mean you can almost think of it as like a light shining from the top right corner in this line here from the top left and then I guess this one is bottom left although the edges are not as defined okay any questions on this thanks all right so that's it for putting matrices together I mean we'll be using matrix products every day but I kind of in the intro applications and now I'm just gonna very briefly say some of the matrix decompositions we'll be seeing we'll be covering all of these in a lot of depth in the future lessons so one is topic modeling and we'll see it with NMF and SVD and so a group of documents can be represented by a term document matrix here these are works of Shakespeare along the top is the particular play on the left is different words that appear in those plays and so you can see Anthony and Cleopatra the word Anthony appears 157 times in Julius Caesar the word Anthony appears 73 times and this is a way that you can represent a group of documents as a matrix and this is notice that nothing about syntax or order or structures being included this is treating them as a bag of words basically but it can let you figure out different topics and in matrices what that looks like so this is for NMF so the the words are the rows the documents are the columns and you can decompose that into a matrix of topics so that would be topics by words and then topics importance indicators kind of by topics or I mean really that's the kind of documents by how how important each topic is for that document and I think it's always um always helpful to kind of write out what your dimensions are when thinking about it but here topics is kind of going to be your short dimension that you're finding we'll see background removal which will use robust PCA which uses SVD and SVD uses QR so there's kind of some nesting going on and that's to kind of remove so we have this surveillance video and we can kind of pick out what's the background and what are the people which could be useful the page rank algorithm is all based off of eigendecompositions and finding an eigenvector so we'll look at that and we'll look at that at on a data set of Wikipedia pages and then that page I linked to before of the matrix factorization jungle has a number of other decompositions well and actually this is a like perfect timing so it's it's noon I was thinking we could take maybe a seven or eight minute break and then come back yeah get some water go to the bathroom and then we'll dive into kind of yeah these four concepts that I think are pretty fundamental to numerical linear algebra great alright so it's um it's 1207 we're gonna start back and actually um Jeremy said that everyone is used or that the MC on recommends Python 2 so he's just going to briefly talk about and having both 2 and 3 installed and so you can switch between them so for those who are interested in trying Python 3 there's only two things you need to really know the first is that print statements now have parentheses around them the second is that when you divide an integer by an integer you get a float rather than integer which makes a lot more sense but if you're used to the Python 2 behavior you're playing that surprising there's a really fantastic thing called anaconda which some of you may have come across it's a Python distribution that when you install it it'll offer by default to install it in your home directory rather than replacing your system Python so you can install anaconda 3 and that'll give you the latest Python 3.6 that supports all the cool linear algebra stuff virtual showing and it won't replace your current Python in any way so then you've got a choice with anaconda you can actually run multiple versions of Python inside anaconda so like we can help you do that on slack if you guys want to do that or you can just switch between the two by changing your path to add or remove your home directory Python from the path so so that's definitely an option there like don't I don't suggest you replace your system Python with Python 3 that's going to cause you a lot of confusion but instead install anaconda and then other another nice thing about anaconda is that all of the well for example pytorch but which we'll be using later for using the GPU by far the easiest way to install it is with with anaconda in fact that's the officially sanctioned method so there's a number of reasons maybe to try out anaconda but definitely don't replace your system Python yeah so feel free to ask on slack or ask either of us if you have questions about that and then also I want to say Python 3 is not required for this course so if you want to keep using Python 2 that's fine as well but it is a neat option and anaconda and Jupiter both make it pretty easy something that's nice about Jupiter is when you start a new notebook it'll ask you like which kernel you want to use and so if you have both installed you can choose whichever one you want Rachel's code often won't run as is in Python 2 if you are using Python 2 but we can also show you a couple of lines you can add to the top of every page which makes a Python 3 file largely compatible with Python 2 so we should probably start adding that to our yeah and then also as Jeremy said many of the things that don't work are very minor and it's yeah adding parentheses around your print statements or I guess yeah some casting if we were dividing integers by integers to get floats okay so um yeah in this part I'm going to talk about kind of four four huge areas of concern in numerical linear algebra or when doing matrix matrix computations on a computer in general so the first is the first is floating-point arithmetic and so to understand accuracy we need to look at how computers store numbers because it's and this is something that really I hadn't thought about thought about until I got to grad school is when you're doing math it's continuous and it's infinite you know you kind of have this infinite precision as possible but computers are inherently finite and inherently discrete so it's really kind of important to think about how computers deal with numbers in math so for an exercise I want you to look at this method F that I have defined so F takes a value if the value is less than or equal to a half it returns two times that value if X is greater than a half it returns two times the value minus one and imagine that we feed one tenth into that and so that would be one tenth is less than a half so it's gonna return two tenths and now feed that two tenths back into F and I want you to keep doing that and just write out on paper kind of what you would get for the first ten iterations of kind of starting with one tenth doing F of that and then do F of your answer and this I definitely want you to kind of write out before you before you run the code raise your hand if you want more time all right someone tell me what you got for kind of working this by hand right yeah so it's a cycle great thank you okay yeah so this is a cycle so now we're gonna try running it for 80 iterations to see what happens so it starts off point one point two point four point eight point six point two point four point eight point six but what's what's happening is this goes on and then we actually end up getting one just over and over again so the method on the computer has converged to one being the answer and I think I think this is pretty cool like it's a fairly simple example and it's something that you can work by hand and work on the computer and you're clearly getting two different things and so we'll talk about this in a moment and something to keep in mind is that when you do get kind of these computer kind of numeric errors it's often happening with repetition when an error is kind of getting multiplied because you'll notice that this you know this wasn't exactly point six but it was pretty close right it was point six I don't know how many ten zeros or something and then a one so that's a pretty small error but those got bigger and bigger kind of how far it was off yeah and so that kind of the two limitations of how computers represent numbers are numbers can't be arbitrarily large or small like there has to be some limit and there have to be gaps between them they can't be continuous and so the way that computers store numbers and this is called floating-point arithmetic and I want to specify floating-point arithmetic is just one piece of accuracy so we're kind of talking about the broader concept of accuracy on a computer and this is one component to consider but floating-point numbers have three parts there's a sign this is just a single bit positive or negative what's called the mantissa or often the significant and that kind of has the digits if you're familiar with scientific notation when you have the like 1.73 that's you know that's the mantissa and then in scientific notation the rate X is 10 which is the base you know you have an exponent so you've actually kind of seen this before of having significant and exponent and in computers the rate X is 2 but this is the idea and that you I mean the computer has to make space to sort the store these things the sign the number of digits and are the significant kind of the value or precision of those digits and then the exponent and so I triple E is a set of standards that came out and I haven't written down I think it was maybe like mid 80s about how computers should store numbers and it's really great to have something that's consistent no matter what type of computer you're using because that could be a big issue and in the early days of computing there people were doing different things so this is just and we will talk about this a little bit pythons if you've primarily been using Python Python doesn't require you to say what your types are and kind of hide that from you many languages and particularly older languages you had to say what type something was and that's what the computer knew how much memory to set aside and so for what we think of decimal numbers they're actually you know typically a float or a double those are both kind of same type of numbers but double saying you want more space to store it and so here I've just said and Python is handling all this stuff behind the scenes it just kind of hides that from you yeah so here are what the requirements for doubles are numbers can be as large as this is something 10 to the 308th which is that's pretty big or as small as 10 to the negative 308th and then I think I think this is really interesting the way that they're represented so we'll think about the interval from 1 to 2 you can represent 1 and then 1 plus 2 to the negative 52 1 plus 2 times 2 to the negative 52 1 plus 3 times 2 to the negative 52 and so on up to 2 and then the interval from groups the interval from 2 to 4 is going oh and this is an error is going kind of you can represent 2 and then 2 plus 2 to the negative 51 2 plus 2 times 2 to the negative 51 2 plus 3 times 2 to the negative 51 and so the you'll notice the numbers are not equidistant apart basically the bigger that the magnitude of the numbers get the kind of the more they're spaced out which I think is kind of weird and interesting so this is a nice kind of a graphic showing that that close to kind of for small numbers they're closer together Jeremy pointing out that of the two things that we're using in this class to you to represent numbers being NumPy and PyTorch isn't so with NumPy if you feed it they won't it will create a double precision float anything is a float array if anything if they're all instal create a long integer array and then when we use PyTorch will also be explicitly saying what type it is so we are using type libraries pretty much exclusively in this course thank you that's a great point and then also even another library we'll use is number which lets you add types to Python in general Oh just I thon okay sorry I thon is the one that lets you add types to the Python um so that is something that comes you will often want to add and that yeah NumPy kind of has built in when you're doing scientific computing and also for improving performance it typically lets you kind of go faster to handle it yourself and so machine epsilon that's kind of defined to be half the distance between one and the next larger number so for double precision machine epsilon is 2 to the negative 53 you kind of see that up here that the thing kind of the next number after one is to the negative 52 more so half of that is 2 to the negative 53 and this is a kind of a term that you'll hear people people talk about and then converting from base 2 to base 10 that's equivalent to about 10 to the negative 16th and we'll see this I'm going to show up an example later any questions why does machine epsilon matter so machine epsilon often you'll talk about your air as a in terms of machine epsilon because that's something that's kind of inescapable that you know the computer can't represent something smaller than that and so you'll just kind of talk about I mean if you have an algorithm that makes that worst or is in terms of you know the square root of that then that's kind of worse than the computer could be doing and depending on what you're trying to do it varies what's possible but this is kind of a good unit to talk about how you're how you're doing and then two important properties of floating-point arithmetic one is that the difference between a real number and its closest floating-point approximation is always smaller than machine epsilon in relative terms so here FL of X is the floating point representation and then the X is kind of the true number you're wanting to represent and it's saying the floating-point representation is going to be equal to X times 1 plus some epsilon and that epsilon is less than or equal to machine epsilon so that's kind of nice it gives you a bound on your accuracy and then for the kind of key floating-point operations which are addition subtraction multiplication and division so we'll let star represent that operation and then circle star is the floating-point equivalent of that kind of how it's implemented your result is going to be no more than a multiple of 1 plus epsilon off and then this next part I included cuz I just found it was really interesting to read about I found there's a book called handbook of floating-point arithmetic and chapter 1 is available for free but it lists a lot of other types of storage schemes that were tried with numbers at various points and so I I don't know what all of these are but I thought it was an interesting list that people tried kind of possibly infinite strings of rational numbers floating slash number systems so there's been a lot of different approaches that have tried and actually let me skip ahead there's a really nice quote from the book that really you're having to make compromises between speed accuracy dynamic range ease of use implementation and memory they're kind of like all these different considerations and that floating-point arithmetic arithmetic seemed to be a good compromise for kind of all of this and a lot of people kind of converge to accepting this here is a interesting history of floating-point arithmetic Donald Knuth sites the Babylonians is being the first to have a floating-point arithmetic system and theirs was base 60 and that was 8,000 BC and in 1630 the slide rule was invented and there you're manipulating only significance and that's base 10 and radix and base are kind of the same thing 1941 this is interesting was kind of the first real modern implementation Conrad Zeus made the z3 computer and Conrad Zeus lived and worked in Nazi Germany and so he was really cut off from the rest of the scientific community and so he built some very interesting computers that kind of nobody else knew about for quite some time and although many of them were destroyed in bombings but he was the first to kind of kind of implement this in a modern computer and then yet 1985 and William Cahan who was I believe at Berkeley played a huge role in kind of pushing for this standardization of wanting different computer manufacturers to be doing the same thing and then just a real quick so I think with computers you know kind of zeros and ones radix having base two seems to make a lot of sense apparently the Russians were using radix three for a while and there are some benefits to that and this is in the 50s everyone's yeah and it was neat because it says that this does kind of minimize in some ways like the number of symbols times digits you have to use also rounding gets nicer okay any questions about floating point okay so we're still under these up we're still under the sub point of accuracy right now but the next thing to think about act with accuracy is conditioning and stability and so since we can't represent numbers exactly on a computer it becomes really important to know how having a small change in your input affects the output because sometimes that's going to happen inevitably with how you're representing your numbers and so try but then had a quote saying a stable algorithm gives nearly the right answer to nearly the right question and so that kind of nearly the right question is referring to you can't represent your numbers exactly and then you want your algorithm to be doing nearly the right thing conditioning and stability and I think many people kind of use them as synonyms technically conditioning is referring to the problem itself how it behaves under perturbations which are small changes to input stability is about the behavior of an algorithm and so we'll be talking about it in the context of a few different algorithms and so then a kind of simple example is we look at the matrices A and B so a is one a thousand zero one B being one thousand point zero zero one and one these are very similar matrices right there's only a point zero zero one difference in one entry between A and B if you calculate me so you would actually kind of hope that these would have similar eigenvalues and they don't and that's not because of how we're calculating them that's kind of an issue of the problem of finding algorithms highlighting so here for a the eigenvalues are one and one it has a kind of multiplicity two for the eigenvalue one and then for B they're two and zero so those are very different and J is for the imaginary term a lot we are just we're just gonna focus on real numbers in this course but a lot of these problems in general can give complex values and so real valued matrices can have complex valued eigenvalues or eigenvectors and then I also just wanted to highlight because I think this command is so useful NP dot set print options suppress equals true otherwise what happens is and you've probably seen this you get like zero written out with like 15 digits and scientific notation and just zero zero zero zero so suppress equals true turns that off and makes make sure zeros just show up is zero any questions about the eigenvalue example and so that is something kind of to keep in mind kind of things you'll see that relate to the math because this is something even if you weren't using a computer and you solved for the eigenvalues by hand you would still get these different answers for what are fairly similar inputs and then just looking ahead this will come up again when we talk about classic versus modified Gram-Schmidt which are methods for the QR factorization also with Gram-Schmidt versus Householder and conditioning a system of equations and then another area to kind of think about with accuracy is approximations it's actually pretty rare that we need to do highly accurate matrix computations at scale particularly if you are doing machine learning often being slightly less accurate is the form of regularization that can help you prevent overfitting and if you are willing to accept some decrease in accuracy you can often increase your speed by orders of magnitude which could let you calculate an answer several times and kind of regain some accuracy with that approach and then I guess the issue that I didn't write down here but to think about is also the the quality of your data and kind of if you're aware that your data may not be super precise it's then bizarre to spend a lot of time trying to get the kind of most accurate answer possible when you know that your data wasn't even collected in the most precise way and I think this happens really a lot in the kind of tech world and then a kind of popular example so this idea of kind of inserting randomization or approximation into algorithms yeah it's very powerful and a really kind of popular example is a bloom filter and that's something that allows you to search for set mom membership with 1% false positives if you were using that uses less than 10 bits per element the kind of general idea is with a bloom filter if it tells you no it's definitely no like you know that's correct if it tells you yes it's probably right there could be some error there and so this is a tweet joke about it would you like to learn more about bloom filters no or probably because you can never get a definite yes with them but you can get a definite no and then a kind of a place that they're used is looking for kind of if you want a web browser wants to block pages with viruses it could use a bloom filter and if it says no then it you know lets you view the web page no problem if it says maybe then it can look it up you know and that takes a little bit longer but it's only having to do it for a small percentage of the pages any questions okay and then this is also kind of just for fun but kind of expensive expensive errors the European Space Agency spent ten years and seven billion dollars on the Arian-5 rocket but it was trying to fit a 64-bit number into a 16-bit space at one place and so it exploded. Let me just skip ahead to that. This is maybe in the 90s. 94 yeah. There was also I couldn't say as this is going off I didn't include it that the U.S. had a Patriot missile defense system in the Middle East and this was supposed to get the U.S. space agency back in the space race game. So this stuff can have big implications. But yeah as I say a U.S. kind of missile defense system its clock like gradually got more inaccurate that actually they made I think 28 people were like killed by a missile that it failed to recognize because it was like you have to look it up I think the clock what might have was significantly off because it hadn't been reset whereas this yeah the kind of air accumulated and then another very expensive air is Intel released a chip in 94 that in just in certain cases only had like five digits of accuracy and so they ended up having to kind of do a recall on that and it cost them close to half a billion dollars. So just to highlight that this stuff does have real-world implications wow yeah so that was just the cost to Intel not to people who yes yeah so that's a that's it for accuracy and all these concepts will kind of return to as the course goes on this is just introducing them. So memory use so we've talked about how numbers are stored now looking at how matrices are stored and so a key way to save memory and also computation is not to store all of your matrix and you could just store the nonzero elements and then you know anything but you're not storing must be zero. This is called sparse storage well suited to sparse matrices. Here's an example and these actually show up a lot in problems where you kind of have some sort of structure and maybe things on the diagonal or tri-diagonal are nonzero but that you have zeros elsewhere. This picture is from something called a finite element problem and they show up in engineering we won't cover them here but it's also a multi-grid problem whenever you're kind of having to model like airflow around a plane engine or something or nose of a plane you get these matrices with sometimes very pretty patterns of where the nonzero elements are and so in this picture this is you know like a hundred by a hundred matrices and black squares are nonzero values white squares are zero values. And so we'll come back to this because scipy in the future lesson we'll kind of talk about scipy gives you three different ways to store a sparse matrix and so you you know once you've decided like okay I'm not gonna have a cell for everything you do have to kind of talk about like okay what are you gonna store to keep track of so we'll return to that. And then the opposite of sparse is dense which is probably kind of what you're most used to with both matrices and storage. And then kind of as a rule of thumb some people say that you can consider a matrix sparse if the number of nonzero elements scales with either the number of rows or the number of columns whereas a dense matrix the number of nonzero elements is scaling with the product of the number of rows and the number of columns. Any questions about that? Okay so speed and speed is another one that has several kind of sub sub points underneath it and things that affect the speed of your algorithm are the computational complexity, vectorization, scaling to multiple cores and nodes, and locality. We're not gonna I'm really gonna get into computational complexity and big O notation in here just to check who's familiar with big O notation. Okay I've linked to a few resources kind of interview cake has a nice overview and I went through that kind of the start of code Academy I think has a really nice kind of buildup of it has you doing starts with simple problems that kind of get more complex but I think those are kind of useful tools if you do want to review it or learn about it. And typically yeah and this is something that in software engineering definitely comes up in every interview. Data science I would see is more mixed but will come up but like yeah like in coding boot camps people kind of spend the first 80% of the course actually learning to build web develop you know build web apps and stuff and then at the end it's like just study the theory of computational complexity because that's what you'll see on interviews. And that kind of idea behind it is that it's giving you just this approximation that's kind of like an order of magnitude you're not interested in kind of your constant terms or even your coefficients of how how slow things would be. And so if you had an m by n matrix you might have you might describe an algorithm as being n squared times n and if you were having to do if that's kind of how many operations you had to do for your algorithm. Vectorization, so modern CPUs and GPUs can apply the same operation to multiple elements at the same time. This is called SIMD, single instruction multiple data. You will not be explicitly writing SIMD code and this is typically done in assembly, but libraries like NumPy which we will use a lot have been vectorized to do that. And those rely on low-level linear algebra libraries such as BLAST and LOPAC which I want to say a little bit about because you'll probably hear about them and they're they're like everywhere. So BLAST started out as a Fortran library in 1979 and it's a specification for low-level matrix and vector arithmetic operations. So kind of very kind of the more basic like you're doing matrix multiplication or matrix vector product. Some examples of it include AMD, Atlas, MKL and OpenBLAST. So you may hear about these. Then LOPAC is uses BLAST and so it's kind of like a layer above it. And LOPAC is for matrix factorizations which is what we'll be seeing in this course. So LU, Telesky, QR, SVD. Yeah and LOPAC arose out of kind of previously there were two separate libraries Icepack and Linpack. Icepack was for eigenvalue routines. Linpack was for linear equations and neither of those were really taking advantage of cache. So they were developed in the 70s and 80s and I think LOPAC came out in the early 90s to kind of take advantage of cache in modern systems. And you'll see like if you're reading the SciPy source code at many points you'll see it calling LOPAC routines. And so there are points where if you want it to go in depth you can kind of look at that at the LOPAC documentation to kind of see, okay, this is what is happening when SciPy calls this LOPAC routine. Well and then the next concept is locality. And so a lot of the kind of slowness from computers nowadays comes from when they're having to move data around from one location to another. And slower ways to access data such as getting something from the internet can be up to a billion times slower than faster ways such as the register which is basically the fastest memory. And it's important to remember that basically the faster memory is the less you have of it and so your fastest types of memory are much more limited in space. And so once you have data in fast storage it would be great to, I don't know if you're gonna have to do three computations with that data, to do all of them while it's in fast storage as opposed to like putting it back in slow storage, retrieving it, doing your second computation, putting it back in slow storage, doing something else and then retrieving it for your third computation because it's that, you know, having to retrieve it that's slow. And so you really want to minimize those and so kind of ways that you can group together, you know, times that you're gonna use a particular piece of data are really helpful. And so kind of issues in that category are known as locality. This is, so Jeff Dean of Google gave a presentation on numbers every programmer should know and these versions are from, and a lot of people still, so it's been years, a lot of people still share the slideshow. There's an updated version. I would say, actually let me open this because it's kind of neat, the updated version has like a slider so you can even look at like what the numbers were in different years. The key thing to kind of look at is that an L1 cache reference, one nanosecond, that's kind of the fastest, fastest you can do. Main memory reference, and this is also kind of RAM, would be a hundred nanoseconds. And here, I don't know if you can read this from there, they're switching colors so they're kind of saying a hundred nanoseconds, you know, a hundred black boxes is one blue box. So that's kind of what's going on with the colors in this picture. So main memory reference, okay that's a hundred times slower. And then if you get to disk seek, that's really slow. So that's three million nanoseconds. So it's kind of important to keep these in mind. More the idea of kind of the orders of magnitude that you're seeing as opposed to memorizing specific numbers. Any questions? And I definitely encourage you to check out a lot of these links. So now I'm going to show part of this video, and so this video is about a language called Halide, which we will not be using, but it's just a really good illustration of some of the things you would have to think about in kind of thinking about what order to do things in. And so in the video, don't worry about, just briefly at the beginning, he shows kind of a bunch of code. There'll be these kind of visualizations though with green boxes, and green means that you're reading something, and red means that you're writing kind of what order things are happening in. And the problem he's looking at is just the blur of a photo. So kind of taking a lot of the developers on Halide work at Adobe, so they do a lot of kind of photo processing, but the idea of just you need to read, you know, a few pixels to be able to give the XY blur. So you're kind of taking this photo and then we want to look at a few pixels around it to get what the blurred version would be. Oh yes, this is a convolution. So kind of similar to what we saw before, that idea of kind of sliding a filter. Hi, I'm Andrew Adams and this video is about Halide, a new language and then a vertical blur, which reads and averages three points from intermediate results, which we store in a temporary image. This code takes about 10 milliseconds per megapixel on the quad-core x86 that I benchmarked it on. But an optimized implementation, this machine is more than 10 times faster. The code is hideously complex. All we're trying to do is average together 3 by 3 pixels, but an 11x speedup is too much to ignore. So why is this code fast? We've transformed the pipeline to optimize for both parallelism and locality. The parallelism comes from distributing work across threads, and that's what that Pragma OMP parallel 4 at the top does, and also from computing in 8-wide SIMD chunks on each core's SSE unit. Exposing parallelism though is only half the story. Just as important and often much harder to think about or express is locality. For example, making sure the pixels produced by one stage are still in cache when the next stage reads them. And without locality optimization, even a really well parallelized pipeline will probably be limited by the available memory bandwidth. So here the optimized code improves locality by computing each stage in tiles, interleaving the computation of tiles across stages. So we compute just a single tile of blur in x and then a single tile of blur in y, and then we go back to compute the next tile of blur in x. So this, hopefully, keeps all that intermediate data in small buffers that never leave cache. But it complicates the code because it's interleaved the computation of each stage. So the execution of the pipeline looks like this. The input image is at the top, flowing down through the blur x and blur y stages below. The earlier stages are evaluated over larger buffers because we're computing filters that have a footprint, so we need more inputs than there are outputs. Each point in blur y, the output stage, depends on three pixels in blur x, which in turn depend on nine pixels total in the input. So the unoptimized version that we looked at first computes every pixel in the first stage, writing them out to memory before computing the next stage which has to slowly read them back in. The optimized version interleaves the stages instead. To compute a chunk of blur y, we first need the corresponding chunk of blur x, which loads a chunk of the input. The blur x stage filters that input, and then blur y immediately consumes it to compute a chunk of the output. So next we throw away that intermediate data, that chunk of blur x, load the next chunk of the input, compute the next chunk of blur x, followed immediately by that next chunk of blur y. So we've moved the computation of each chunk of pixels in a consumer stage closer in time to the computation of its inputs. This improves producer-consumer locality by keeping all the intermediate data nearby in local caches, but it's made optimization a global problem of carefully interleaving the computation and storage down an entire imaging pipeline. You can't address locality just by optimizing stages in isolation or by just tweaking operations in your inner loops. Also we're making a trade-off here. We're saying that for each chunk of blur y we should independently compute, consume, and then throw away the required chunk of blur x. This means that neighboring chunks, which depend on overlapping pixels from higher up in the pipeline, do redundant work where they overlap. Now for this pipeline it made sense to redundantly compute some values in exchange for the increase in locality that we get by never letting the intermediate values move out of cache into main memory, but this is not always the right choice. Let's try to get a full handle on the space of choices we could have made. In general in an imaging pipeline there are two questions you must answer for each stage. The first is in what order should that stage compute its values? Let's look at some choices. The most common way to traverse a region is in scanline order. This means we traverse a region of a function sequentially across y and within that sequentially across x. This walks down scanlines just like the loops you would typically write in C. We can transpose the x and y dimensions which gives a column major traversal which walks down each column in turn. Or we could go back to scanline order but traverse the x dimension in vectors of width 4. We could distribute the scanlines across parallel threads. Finally we can split the x and y dimension into tiles which opens up further recursive choices for the order of the outer and inner components of each dimension. By traversing the outer components outside the inner components we get a simple tile traversal. That's the first question. The second question is more subtle. When should each stage compute its inputs? Let's look at some options. Here we have a visualization of the blur pipeline. On the left is the input, on the right is the output and in the middle is the blur in x stage. Green means we're reading, red means we're writing and blue means we've allocated a temporary buffer. So right now we're reading from the input and using it to write to the blur in x stage. We read three values from the input one two three to compute a single value of the blur in x stage. We haven't even started writing to the output yet. So the choice we've made here is that we're going to compute all of the blur in x stage before computing any of the blur in y stage. If we phrase this as a decision made by blur in y that decision is compute all of my inputs ahead of time before I start computing any of my values. So what's the pitfall with this approach? Why is this slow? The answer is of course locality. By the time the blur in y stage goes to read some of the intermediate data it's probably been evicted from cache. So that load will be slow and will be limited by the system memory bandwidth. So let's look at a different option. Here we compute three values of blur in x by reading nine values from the input and we immediately use that to compute one value of the output. So here we get maximum locality. We're using data as soon as it's available without giving it any time to be evicted from a cache. What's the pitfall here? Well if you look carefully at what the blur in x stage is doing you'll realize that we're doing a lot of wasted work. Each point in blur in x is redundantly recomputed three times. Okay well maybe we can figure out how to get around that. Here's another choice. At first it's going to look similar but notice that we've allocated enough memory to keep around all of the intermediate stage and we're not throwing away values as we go. That means when we get to the second scan line we can start reusing values that we computed earlier. So great we have locality and we're not doing any redundant work. What's the pitfall here? We've introduced a serial dependence in the scan lines of the output. We're relying on the fact that we've computed scan line n minus 1 before we can start computing scan line n. This means that we can't paralyze across scan lines with this strategy. With the previous two strategies we could. So this approach has poor parallelism. I really like kind of that visual approach if he's kind of showing these different ways of doing the same computation and that each one has different positives and negatives and in fact none of them seems ideal because there's kind of a trade-off no matter what. I just want to mention that later on we are actually going to build an algorithm in all of those different ways in Python and see how they are different. Keep that video in mind because when we get there it'll be useful to think about those pictures. Yeah, thank you. And then something that he said in the video is just locality is really hard because you kind of have trade-offs it feels like no matter what you do. Sometimes redundant computation can save you memory bandwidth. So kind of computing things multiple times means that you know you don't have to kind of be pulling them in and out of memory as much or you can sacrifice parallelism to get better reuse but then you can't parallelize. And he kind of says and the people building this are experts who have been working in Adobe for a long time like they really often just have to try a bunch of different stuff and can be hard to predict what's gonna end up being fastest. And then kind of another. The difference in speed that Rachel's talking about is many orders of magnitude not a few percent. So this isn't like a minor thing these are things where like in practice you'll run something overnight and it hasn't finished you'll make one of these small changes and it runs in three seconds. It can be that big of a difference. Yes, thank you. Yeah, if it was a minor difference in speed it wouldn't be worth the bother. And something even I think in the part I showed you he mentioned kind of getting an 11x speed up by writing this more complicated version and it's I mean I couldn't even see what the code said but it was a full screen of code just to do this blurn X blurn Y operation. And then another issue that comes up is temporaries and that's when you're doing a calculation and kind of temporary variables end up getting stored. And so this can be a lot a lot slower than if you're able to keep all the data in cache. So this is if the temporary variables are stored in RAM. NumPy creates temporaries for kind of every operation it does. So if you were doing A equals B times C squared plus the natural log of D, what NumPy would have to do is calculate C squared, store that, multiply that by B, store that result, take the natural log of D, store that somewhere, and then use those two variables it's stored you know to add them together and give you the answer you want. And so kind of along the way NumPy is having to deal with this creating temporary variables and putting them somewhere. And then I'm about to get to the scalability section but I just want to note that scalability definitely impacts the the speed of what you're doing and whether you're kind of fully taking advantage of the resources that you have. So for scalability kind of there's the one approach is to be able to scale an algorithm across multiple cores within a single computer or scaling across multiple computers in a network which we will not be covering. But yeah we will talk about kind of parallelizing which is scaling across multiple multiple cores in a computer. Yeah and I think this is great timing. I can take questions. Oh yes. Just a tip, you'll see that Rachel's used a lot of hierarchical headings and it's really easy to navigate those and understand her thought process by collapsing sections. You need to install a Jupyter extension or collapsible headings for that to work. So you might want to look at installing that extension. Yes yeah that's a great extension. Any questions? Okay well I'll see you on Thursday. Thanks.

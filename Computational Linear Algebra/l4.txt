 Okay it's 1105. I'm gonna get started. So we're finishing up topic modeling and first just can someone remind us what the SVD is? Brad? Oh wait, hold on for the microphone. Exactly, yes. So singular value decomposition you get a matrix of orthonormal columns and then in the center there's a diagonal matrix and as Brad said it's got the singular values in descending order and then third matrix U is orthonormal rows. And so we were talking the end of last time about how we could use a randomized SVD to get the truncated form a lot faster. And so kind of the truncated so full SVD you're getting you know you're able to fully reconstruct your original data matrix which might be useful but if you're trying to do data compression or you just want to do something faster you probably don't need all of that and so it'd be quicker to not calculate the whole thing. So I wanted to remind you that full SVD is slow so this is the calculation back from the very beginning of this notebook and again to remind you vectors this is actually I'll ask you what is a vectors this is kind of the matrix we've been looking at for the past three days in notebook two. Does anyone remember? What data set we're looking at? Do I see your lips moving? Is that a... Thanks Tim. So this is the news groups data set so it's kind of posts that people have written on these message boards on the internet in different categories and our rows are the different words and the columns are the particular post. So when we did the SVD on it using SciPy's implementation this is for a full one it took 28 seconds which is pretty slow. You can see that here and I think I mentioned this before but percent time is a magic or they're called magics in Jupyter notebooks but it's really useful to get the time of something you're running. So then we check the shape and here we were getting U was 2,000 by 2,000 we had 2,000 singular values and then V was 2,000 by 26,000. However if we do the randomized SVD so suppose we were only interested in the top five topics we run that and and this is from Scikit-learn that only took 154 milliseconds so this is an order of magnitude faster. So that's kind of the talk a little bit more about this but that's the idea we're closing closing out today of doing this randomized version and it's a lot quicker. So I looked it up this was a question last time the runtime complexity for SVD is big O of the minimum of M squared N and MN squared. We'll be talking in a later week about how you would actually calculate the SVD. For now we're just going to use SciPy's implementation. We had this question of how can we speed things up without coming up with a better implementation for finding the SVD and our idea was to just use a smaller matrix with smaller N and so and so this right now we're kind of explaining how the randomized SVD works. So instead of calculating the SVD on the full matrix A which was M by N we could just use B equals AQ and have that be M by R where R is a lot smaller than N. So we're actually still using the same SciPy implementation of SVD we're just multiplying our matrix to make it smaller by a random matrix and that's a lot quicker and gives us just the information we need. Here's the calculation again and we can see that the topics are really good. So again as a refresher. Oh Brad? Oh that's a that's a great question. So we want B to have the same column space as A is the hope. So yes that would be a low rank approximation. I'm not sure that sounds like it like I actually yeah that is that is the case that you do get it yeah best low rank approximation that's yeah that's not what we're doing here that's a bit circular almost because in order to do that you have to get the full SVD to know that you're taking the the best low rank approximation. Here we want to get a low rank approximation that's kind of good enough and then just do the SVD on that. And so well okay so a difference so it's an approximation of the column space yeah so I guess this is another key distinction is A and B have different dimensions because we've made and so if A is like our giant matrix B is tall and thin so you know we've kind of cut off all these columns so yeah this is an excellent point so B's not actually approximating A but the the hope is that the columns of A span a similar space to the columns of B so we're kind of using fewer columns to hopefully span the same space. Yeah thank you. And this so and I should also say I've kind of added so this is I tried a different perspective for talking about the randomized SVD from the end of class last time so there's some new material in this notebook that I am updated on github yesterday but if you have the version from Tuesday some of this is new but I just kind of really wanted to emphasize the speed and the fact that we haven't found a new way of finding the SVD yet we're just kind of taking a smaller matrix to speed things up. And then this is from the Facebook research blog post that I keep referencing that had the nice colorful picture and they ran some a few cases that I wanted to highlight for benchmarks but they looked at large dense matrices that were up to a hundred thousand by two hundred thousand in size. Large sparse matrices up to ten to the seventh by ten to the seventh in size and those had ten to the ninth nonzero elements and then they also looked at Apache's sparse distributed SVD implementation and they just wanted rank ten approximations so for rank ten calculating out a matrix that is ten million by ten million is really going to be excessive and so you can see and so our pack is kind of state of the art like it's a matrix factorization package that is widely used so this is not saying that kind of our pack is bad it's just kind of like how slow it is to fully do these things and they found that it was one second to do the randomized version versus a hundred seconds for ten to the sixth by ten to the fifth matrices and then it was five seconds versus sixty three minutes for ten to this oh for the this is like a denser version so yeah I just wanted to show those numbers are a really big improvement because you yeah five seconds versus sixty three minutes is huge and that's also I think um it's a really compelling detail when people include things like that in blog post I should say in preparing I've been preparing the next lesson I had found a blog post that um it had this like threaded implementation to like run something in parallel and it was just like oh this should be faster but they didn't have any like actual times and so I used their code and ran it and it was actually slower with the parallel processing and so that's something it's very valuable to definitely run it and tell people that times yeah so that's that's impressive and then we're not gonna get into too much detail in this but I'm kind of the idea that should help justify why this why this is okay is the Johnson Linden straws lemma and that is that states that a small set of points in a high dimensional space can be embedded in a space of a much lower dimension in such a way that distances between the points are nearly preserved and that idea of distances between points being preserved is really about preserving the structure kind of if you have a data set that's in a ton of dimensions you should be able to put it into fewer dimensions and kind of keep the structure and that's what we're doing here with a and B in a sense of Tim yeah yeah projection is a good way to think about it yeah protecting it oh another question yes so with the with the column space going from from a to B so a is the the large one actually let me just draw that so kind of a is this size B is the same height but thinner so here you can think of the data points is the rows and so the idea is kind of you've gone from if this has and columns and this has our columns and then they're both m tall you have m data points and that those m data points are still have the same structure between them or the same distance between them whether you're looking at a where it's n-dimensional or B where it's just r-dimensional yes yeah any other questions and we're not gonna we're definitely not gonna prove the Johnson Linden Strass lemma in here or even if some of the details I think last time and it's okay if you don't have all the steps from last time of kind of this process of how we did the randomized SVD I mostly want you to kind of get this general intuition of why it might be okay and they were taking this random projection all right yeah so going back so we looked at this code last time that I want to go through it again briefly so to kind of implement the randomized SVD first we created a randomized range finder and here this is this is kind of getting our mapping kind of well getting the random the random matrix that we were going to multiply our original matrix by and to do that we randomly generate a matrix and then you could ignore this for now we take a QR factorization on a and what that does it actually check does anyone remember what the QR factorization gives you so you get two matrices back exactly yeah so you get back orthogonal matrix in an upper triangular and that orthogonal matrix is the one we'll use for our random or sorry we're taking so we're taking randomly generated matrix Q multiply that by a and then take the QR on that and then we call that from randomized SVD and does anyone remember so number of components is the kind of number of singular values we want or the ultimate number of topics for our topic modeling what is the number of over samples anyone okay this is the this is the buffer that we kind of added on so we're saying you know if I want five topics it's actually safer we don't want to just compute like if we make B just have five columns that's actually cutting it a bit close that we're really gonna get the five best singular values of a from that and so what we'll do is we'll add some more on and be like okay let's actually call calculate 15 columns and 15 singular values and then we'll take the top five out of that so we'll still return five if that's our desired amount but we're just kind of adding this safety buffer because we do have some randomness and we don't know that we're gonna get the the best five singular values yeah and so that's them that's mostly it so you'll notice we're calling the same sci-fi lineage SVD method in here it's just now we're calling it on B which is a lot narrower than a oh and then I should roll this up so then an exercise that I wanted you to try was to write a loop to calculate the air of your decomposition as you vary the number of topics and so and let's take a few minutes to do this but what you're doing here is just using the randomized SVD you don't have to change its implementation at all but kind of decompose your matrix recompose it and then see what its air is from the original matrix and this is a this would be useful if you're doing data compression or something kind of to see you know how much of the original are you capturing and something with and this one did you kind of loop through through a few numbers to see how it's changing remember we originally had 25,000 columns so we would expect that five singular values is not going to capture everything so the air will be relatively high hey let's go ahead and talk about the answer to this one and actually can you remind me did I leave the plot in the notebooks that you had on github okay so what do you what do you notice about the plot come back to the code in a moment right okay yeah yeah the air drops off I thought surprisingly quickly for this that yeah you have a lot of air initially but well really I mean around 50 is like kind of the steepest but definitely past like a hundred it's really a lot lower and kind of not changing as rapidly so you can see like kind of the first the first many singular first few singular values giving you a lot of a lot of power kind of capturing a lot but as you go on the additional singular values aren't capturing as much it's kind of what this is showing but so what I what I did to to get this was have a loop where I was taking the randomized SVD of I as I is increasing and I did this in steps of 20 and then I got the select returns a US and V I created the reconstructed matrix using you times NP dot diag turns s which is just an array into a diagonal matrix by filling in zeros and an NP dot diag also if you give it a matrix and not an array then it'll return an array of what was on the diagonal but here I'm using this from array to a matrix functionality of it so I'm just doing U times the matrix form times V and then I'm saying that the air is the norm of the vectors minus reconstructed Jeremy I mentioned a trick that I like Oh yeah there any questions about this alright are there any further questions on topic modeling before we start on background removal and robust PCA Vincent oh oh sure yeah Kelsey sorry oh down here yes well so that's actually let me go to the equations I think show it better and these and I apologize one source had everything kind of transposed from the other sources so this may not be consistent but the idea here is that oh horrible highlighting do this so you know we've got our skinnier B and we take the SVD on that and then what we're interested in is the SVD of A and so we're kind of plugging back in remember from last time we said a is approximately Q times Q transpose a where Q ends up being you know this random matrix we've used and the Q transpose a is how we got our narrow matrix B so we're just kind of plugging in okay this is B let's kind of plug that into here for Q transpose a and that's how we get Q times basically the SVD of B any other questions oh okay Oh Tim yes You said you were going to explain later the for loop there, like what does that mean? So I was putting that off, but as a kind of as an intuitive idea, what that's doing is by taking additional powers of A, you're getting something that's more, kind of even more in the range of A. Kind of by taking extra powers of A, hopefully that's helping you kind of really get the important components of the range of A, because remember here we're trying to find something that has the same, you know, we want B to have the same range as A, and so additional powers of A help us do that. But if we were just to take powers of A without, you know, here we're using this LU decomposition, the reason we're doing that is just taking powers of A, we would have the problem of either the matrix will kind of be like exploding as its values got larger and larger, going towards infinity, or they would be shrinking to zero, and either way that's a problem. And so this LU factorization is having the effect of normalizing it. Yes, yeah. Jeremy? I just wanted to mention, I feel like Johnson and Stroustle is like a super power to have. Since I've known it like 10, 15 years ago, you can use everything, like any time you're doing a data science model of any kind and you have more columns than you can deal with, you know, like logistic regression, too many columns, overfitting, too slow, run it by a random matrix, less columns, and keep going, it's like, it's amazing, you can use it everywhere. I was just going to bring up the Wikipedia page because I like that they list a ton of different fields where they're saying this lemma has uses in compressed sensing, manifold learning, dimensionality reduction, and graph embedding, but I just kind of like liked that like list of places where this is useful. Yeah, like manifold learning might contain some sort of machine learning and stuff, too. Yeah, thank you. Any other final SVD and NMF questions? And we'll be returning to SVD, we're both going to use it in other applications and then we'll learn more about how it's calculated. All right, let's start on background removal. This is notebook three. I'm really excited about this one, background removal with robust PCA. And so I'm showing here, this is our goal that we'll be working on in the notebook is we have a surveillance video, so you see here there's some figures walking across the screen and we want to be able to pick out what's the background and what are the people. So we've taken the picture on the left and broken it into this middle picture and the picture on the right. That's the goal that we're going for. So I'm using something called the real video of three data set, here is a link to it if you want to download it so that you can do this as well. And I also found some other sources of video data sets, it was something I kind of had to look for for a while, because I would be curious if you do try this on a different video data set to see how your results are. So I had never worked with video data in Python before this, and I found the MoviePy library, which seemed really neat and actually kind of made me want to do more work with videos, because here we kind of just use it briefly to convert this into a matrix. So yeah, I've got my imports, and then I wanted to show you this, you can actually watch the video or at least part of it, it was too long to watch the whole thing. So there you can see people walking, and this is in color, but I converted it to black and white to kind of keep things simple or simpler. All right, and that's it, that's our data. And this video is 113 seconds long total. So I have some helper methods, and we're not going to super go into those, but here really it's mostly, I'm just using, so the video library has this get frame that I'll use to kind of pick out specific frames, and then we're stacking those up to form a matrix. And I think that's actually kind of easier to see from, I have a few pictures. So this is a single frame, one point in time, and I actually, I guess I should first note, so I scale this, and I tried doing it with kind of the full resolution, and it was really slow, but I do have some pictures from the high resolution, although I don't recommend that. But here you can enter the scale as a percentage, and so I'm just using 25 percent scale to make it faster. And so an image from one moment in time will be 60 pixels by 80 pixels, and we're going to unroll that into a single column. So kind of each moment in time is going to be this 4,800 long column, and then we're stacking those all together from these different points in time. And so we end up with a matrix that's 11,300 by 4,800, representing the video. And I wanted to show a picture of, this is what the whole video looks like. So does anyone have ideas, what do you think those wavy black lines are? Wait, do you want to toss the... It's probably the people moving. Yes, those are the people moving in the frame. And so then what are the kind of solid horizontal lines? The background. The background, yeah. So remember, each column here is a single moment in time, and so some things like that sign are never moving and probably just show up as kind of these horizontal white lines, but the people do move, and so those pixels are different. So the columns are a moment in time, or the rows? The columns are a moment in time. Okay. Yeah, and this is, I think, backwards from probably what I wrote in the older version of depending on when you last pulled from GitHub, but yeah, here the columns are a moment in time. Okay, is this at all related to optical flow? What do you mean by optical flow? Yeah, I don't really know what it is. So it's some sort of a methodology of tracking movements through, like if you have two adjacent frames, how do you warp one to match the next frame? Oh, okay. Yeah, I'm not familiar with that area, so I'm not sure if there are, because that brings up a good point that here it's assumed that the camera is fixed, and so things are kind of being referenced by, I don't know, everything that's at X coordinate 20, Y coordinate 30 is showing up at the exact same, when you unroll it, that's kind of 0.600, and so it's always going to be showing up in the 600th row. This is kind of fixed. Jeremy? We mentioned the other day that we look at a low-rank matrix, we tend to see lines on it, so I guess this is showing, this is a low-rank matrix already? This is... I mean, it could be turned into a low-rank matrix. Yeah, I mean, yeah, so the horizontal lines indicate that it's low-rank. And actually, first I should ask, who can tell us what rank is, when we're talking about matrices? Anyone I haven't heard from, or Kelsey? So rank is the number of independent columns. Yes. Yeah, so rank is the number of independent columns, and that turns out to be equal to the number of independent rows. Another way to think about it is, it's the dimension of the column space, or the dimension of the row space, which again is equal to the number of linear independent columns and rows. Yeah, so let me just kind of go back up here. So I definitely encourage, it's really important to be able to look at your data, and kind of find ways to visualize it. But here, we've created our data matrix, put that all into M, this is just a NumPy 2D array. And then here, we're kind of just saying, let's take all values in a particular, sorry, take all rows, and then a particular column, so that's the point in time that we're looking at. And we do need to reshape it, to get it kind of back into a picture. So we've kind of unrolled it into, let me see if I kept, I might have gotten rid of it. At one point, I had a picture of just like a single row, which is pretty boring, because it's just, doesn't look like anything by itself, you know. It's black, white, gray, black, white, gray, then we kind of have to reshape it back into our square matrix. So NumPy dot reshape is pretty handy. But so far, just kind of on what's in M, or this setup. OK, and then we've just talked about SVD, so kind of remember what kind of matrices we're getting back. So we're gonna try first, just with a, with using randomized SVD, to see what we get. So we put in M, and then I actually, I tried this with a few different, so the second parameter is just telling you the number of components you went back, or the number of singular values. And I actually felt like I got the best results with two here, but I got back U, S, and V. And then I said my low-rank matrix is U times NP dot diagonal S times V, and this is what the low-rank version looked like here. And just first let me ask, why is this low-rank when I do U times S times V? Remember, U, S, V is the truncated SVD that we've asked for, and now we're multiplying those three matrices together. Lowering, it's lowering because we only have two columns, because there's only two orthogonal vectors. Exactly, yes. So even though, and let me go to the picture, even though U's got a ton of non-zero values, what S looks like is just, I don't know, some value S1, 0, 0, S2, and actually I guess that means that this, sorry, this should only have two columns then to line up, but the, or no, yeah, two columns. Okay, so U is like this, and so U's very tall and has a lot of, you know, non-zero values for that, but S has just kind of got these two components, and then we go to V is going to be really long, and again, it's got a lot of non-zero values, you know, in its long narrowness, but it's only two rows. And so when we multiply kind of U, S, and V together, they're only going to be two linearly independent columns in the result, because we kind of can't get more complexity than that since we've only got two dimensions in these inputs of, you know, what we're multiplying together. Brad? And so that's because when you're multiplying those basis vectors in S, each one of those other vectors is going to be essentially scaling that vector since the other value is 0, right? So all, like when you multiply S with U, it's all going to be adding the two basis vectors, 5, 0, and 0, I'm not sure what the number is, but it's just going to be those two. Yeah, so that's a good way of thinking about it. So we talked last time about how you can think of matrix multiplication as taking a linear combination of the columns. So here, when we do U, S, what we're getting is, and actually let me, I'm going to redraw this, hold on a moment. Okay. So U is two columns, U1 and U2. And so then when we do U times S, we're getting, let me get something that's tall, really that's just going to be S1 times U1 is the first column, and the second column is S2 times U2. Which is why you can replace the matrix product with an element-wise multiplication and get the same results. Yes. And then we multiply that by our columns, sorry, rows, V1, V2. I guess at that point we're kind of having to think about dot products, but you can see that like really we're kind of just taking these two long vectors and that's all we have to work with here. You can also, let me write out the dimension of these, might be nice to see. Yeah, so you can see back, U is 76,800 by two, S is just two, and V is two by 11,300. And so that's kind of showing up when we plot this picture, and actually I could plot the whole picture again, I think that would be good to see, okay, this is a little bit slow. So we'll look at the picture of the one, basically it's got to have practically the same picture for everything. I guess, I mean, I guess you could have two different pictures that it could show, but it's really not able to capture a whole video's worth of data. Other questions about this, Kelsey? Yeah, no, I encourage you to try this with different ones. I just tried it with a few and thought that two looked the best, but yeah, try it with others. I think though, some of the idea is that, unless you're doing a huge rank, I don't know, so if you were doing rank 1000, it's like maybe you are trying to capture all the different places people can be, or capture the information about people. But beyond that, I don't know, with a rank of 10, there's not really 10 pieces of information you can capture. I mean, you kind of have the background, and then the next piece of information is all this information about people. Sorry, I don't know why this hasn't shown up yet. This is quite slow. Oh, let me do that. Although, I must if the other ones are showing up. Yeah, okay. Oh, and Sam's got a question in the back. In the sklearn randomized SVD, is it creating additional columns when, like is it, we talked about generating many columns, and then like maybe if we wanted five, we do 15 random columns. Yes. It is, yes. Okay, so you're just using the default then, which is probably more, but is that something else that we should try and play around with? And I think default actually is 10 or 15, it's, yeah, and I think, and this is coming from a paper, probably from the Halco paper. So this is, yeah, so that is what sklearn is doing. I don't think that's something you would want to tune yourself, unless you had like specific reason to think that that would be helpful, but I think the paper has developed this theory of why this should work, or why this does work. So in trying to intuit why we get back just the background in doing this, would you say that it's because given that the vectors that make up the background in the matrix over time sort of are most important across the entire matrix, when you're looking at the most important singular values, go from the largest, as opposed to the vectors that correspond to people moving, since they're very sparse in there, when you limit it to a few singular values, it's going to ignore those. Right, yeah, like the most, because the most important component, so there's, I guess there's two things going on. One is that really in any frame, like most of the frame is background, but it's also like looking across time, the background is what shows up in like every single point in time, like you always have the background there, and I mean sometimes, you know, pieces of are obscured by a person, but for the most part, the background, I guess a way to think about it even is if you're looking at this whole picture, like most of this picture is the horizontal lines that form the background, so those are going to be the largest singular value, because they're kind of like the most important component. By the way, you mentioned to Sam about the source code, a cool tip I don't think we've mentioned is if you type in your notebook, question mark, question mark, and then the name of the thing, it'll show you the source code, so like maybe you could still, oh, actually, I'm not working. Yeah, my kernel died. I need to rerun a few things. Yeah, question mark, you know, when out.randomized.cd, you'll see the source code, and it's really simple, so it's fun to look at. Yeah, no, that's a great point. Thank you. I'm just going to rerun this, because I do want to have the matrix to be able to use. Oh, I didn't run my helper methods. Oh, I was going to say, going back to this, kind of talking about rank, and creating the data matrix is a little bit slow, so I'll have to wait a moment on that. I would actually expect rank one to be pretty good, and I think I did it and found that rank two was for some reason better, but really, like, the background should be able to perfectly be captured by a rank one matrix, because remember, we've unrolled, you know, 2D space, so that a single picture can show up just as one column, and so really, the column just of the background that has rank one. Here it is. So this is our low rank reconstruction. So we did the SVD, and then we reconstructed our matrix, and so if you think back to, kind of from the math standpoint, what's going on is we're trying to reconstruct this matrix, so you could think of this as a data compression problem, like you didn't want to have to store all of this, and so with just rank two, so just these, you know, two columns in U, two singular values, two rows in V, we get this much of our original matrix, which is a lot of it, kind of like everything except the wavy black lines. So then, kind of more exciting part, I think, is the people that we want to see, although it's, you know, it's easier to pick out the background, because that's what we have information about, so what we'll do is just subtract M, our original matrix, minus the low rank approximation, which is just the background, and so that hopefully will give us the people. And so you can see here, this is what's left, and here I'm just looking at a single point in time, 140, and actually let me copy that, and so you can do this at different points in time. The other one was, yeah, but now I've written over my variables, so yeah, so this is, the low-res version doesn't show quite as much, and actually I don't, here, yeah, so this is the low-res version, so you can't see as clearly, but I was kind of trying the high-res. I recommend staying with low-res just for the speed, though, but you can see kind of what the people are doing at different points. And so it's not, it's not perfect, like we do have kind of like these light marks around the people, and then, whoops, here in the high-res version you can see that the sign that was in the picture, or at least I guess it's kind of like white space around the sign was getting captured, but it's pretty good. So yeah, that's what we get from randomized SVD, which is a pretty simple, simple approach for this. And so then we're going to be doing, we're going through a more complicated algorithm, and this is the result that we'll get with the more complicated algorithm, which doesn't have the issue with the sign, so I think it is a bit better. I will be honest, I was disappointed that it wasn't more better, because it's, it is a lot more complicated, but it's nice because it's also kind of an example of robust, robust PCA, and we'll be able to talk about a lot of issues that arise, and it still uses randomized SVD as part of it, so basically we kind of have a more complicated algorithm that's iteratively using randomized SVD for one of its steps. Oh, okay, and I saw it's about 12 o'clock, so this is probably a good time to stop for our break. We can meet back at 1207, and then when we resume we'll be talking about PCA and robust PCA. Okay, I'm going to start back up since it's 1208, and one more thing, I announced this last time, but I just wanted to say again that I'm going to be out of town on Friday, which is normally when I have office hours, but let me know if you want to meet today or next Monday or Tuesday, and then while we were on break I just tried running this again with a rank one approximation because I was curious, and so here this is, this is what a rank one matrix looks like. It's just a single column repeated again and again, and so it would give you the exact same background for any point, and here I'm subtracting the kind of original matrix minus the background and I get the people, and so that's pretty amazing I think for a rank one approximation. It's not the average, no, because, so it's, if you did the full, oh and I should repeat the question was, is rank one matrix like the average? It's not the average, it's still doing an SVD, but it's finding the, basically the matrix that's going to give you the closest, actually I'm unsure if that would be the average. You want the matrix that's closest to your original matrix but only has one rank? And so that's true, yes I think, and it might depend on your error metric, but you would want, so you kind of your original data matrix that has the wavy lines, you would want to be minimizing that minus, you know, this matrix, and so I think to have error just a few points will probably end up being better than having like a small error everywhere, you know, so if you took the average then every point that a person ever walked by is going to be like off a little bit, so you would have a tiny, kind of tiny error everywhere if you did that. That's a good question. And then also I'm stepping back, I just wanted to say, so with data like this is, I don't put it on GitHub, but you can download the data yourself from, let me go back to the link, up here, so this is the link of where I got this from, and so this is video three in the real videos, although I definitely, if you do run this on different videos and get interesting results, please send them to me because I would be curious to see them. And then the other thing I probably should have done more of is whenever you, like, here where I calculate the matrix and it's really slow to calculate, you don't want to have to do that every time you run this, so you can use np.save to save the NumPy array, and this was for the high-res version, but you would probably want to do it as a low-res version, so I'm just giving a file name, passing in my matrix M, I can save it, and then next time I run this, instead of creating data matrix from video, I could just do np.load. That's a nice trick. Cool. Any other questions about the rank one approximation? Oh, yes, I'm past the microphone. I didn't see it, but did anything happen with the signpost? Anything happen with what? The signpost that would kind of be online? Oh, the signpost? I think more what's happening is you're getting some blurs of the people right behind the signpost, and so it's like having those blurs of the people are what let you, kind of are giving you that artifact of the signpost, although it's hard to see in the low-res version. Yeah, you kind of have to use the high-res version to see that, but yeah, so here's the, which I just did for the rank two approximation up here, but so it's I think probably not so much that the signpost is showing up is that you have these like light blurs of people walking by behind it. Okay, so principal component analysis. Let me go to full screen. So typically when you're dealing with a high dimensional data set, you want to use the fact that the data usually has a low intrinsic dimensionality, so even though it's in a lot of dimensions, not all those dimensions have information, so you can think of that kind of as lying on a lower dimensional manifold, and so this is even kind of, you know, with the randomized SVD, the idea of, you know, A had all these columns, but really the information of the column space could be captured by something with a lot fewer columns, and so principal component analysis lets us eliminate dimensions, and so classical PCA is seeking the best rank K estimate L, so I'm just calling it L because it's low rank of a matrix M, and so K is a parameter that you're specifying going into it, you're saying, you know, I want to see what the best, and we just previously looked at rank two and rank one approximations, but you could choose any value for K, and then you're going to use an algorithm that's going to minimize the difference between M minus L for any possible L that has the rank you're interested in, and so traditional PCA can handle noise in small magnitudes, but it's very brittle to having, even if you just have one observation that's super wrong, that can really throw it off, which unfortunately in a lot of real-world data sets you can have some observations that are just, yeah, very wrong, and so robust PCA is a way around this. Robust PCA factors a matrix into the sum of two matrices, a low-rank matrix L and a sparse matrix S, and so I shall ask you, what does it mean for a matrix to be sparse? Sam? It means that there are a lot of zero values. Exactly, yes, so there are a lot of zero values, and so going back to our example of the people, there are a lot of zero values, and here I'm actually zoomed in, so if you think of all this kind of gray background, those are all zero values, and they're actually even more when you kind of zoom out to the full size, and so we want to find a factorization, and this factorization is a little bit distinctive in that it's a sum. Most of the factorizations, in fact I think all the other factorizations we'll learn in this course are multiplication, but here we're factoring something into a sum, and it's the low-rank matrix plus a sparse matrix. Oh yeah, it's not factoring, it's a decomposition. We are decomposing it into two, and most decompositions are factorizations because they're multiplying, and so yeah, we'll see this in a moment, but in the problem of if your data was corrupted, hopefully your corrupted data is not, most of your data is not corrupted, and so you can think of the corrupted data as being the sparse matrix. So now I have some pictures that I took. This is from a really nice blog post, let me open it, called robust tensor PCA with Tensorly, and I haven't tried Tensorly, which is a specific library, but these are really great illustrations, and so this is face recognition, and it was done on a data set where you had multiple pictures of one person's face, but from different angles with different light, or where the light is coming from different angles, and so here there would have been a lot of pictures of this person's face, but kind of lit up from different angles, and so here's the original. They've added some noise in, or you can say grossly corrupted entries, and I think this is pretty amazing. They get from this original, this is the low rank component, and this is the sparse component. So the sparse component is picking out the noise, and the low rank component is picking out the face. I think particularly this third one is amazing, because the one on the left right here, like I can't even tell that this is a picture of a face, but they have picked out the face. So this is another application of this. So remember kind of what we're looking at, the low rank one will be the background, and the sparse component will be the people. Here the low rank is the face, and the sparse component is this really awful noise. And then here he takes it up even another level, and grays out entire sections in this picture of the face, and is still able to recover the faces. Yeah, yeah, so it's very, yeah, very impressive. So other applications of this, latent semantic indexing, you could use robust PCA. The low rank matrix would be the common words that show up in all documents. So this is and, you know, probably kind of in all your documents. And then your sparse matrix could capture a few keywords kind of from each document that make it different than others. And then ranking and collaborative filtering. So this is an issue that Netflix has in terms of some of the data being really bad. And I don't think they give examples of this, but I feel like this could even be things of, I don't know if a toddler accidentally enters some ratings for movies that are just totally have nothing to do with that person's actual preferences, but they use robust PCA. Do you want to say more about, this is Jeremy? There's a great example, I'm sure a lot of you know about the Netflix prize, it's a $1 million machine learning prize, and there's a lot of great write-ups with that. And the winners had to come up with some, interestingly, like before the Netflix prize, people have kind of forgotten about some of these robust decomposition techniques, but the winners really brought them back into vogue and they've had a lot of attention since then. I can read the papers from that because they're very accessible. I can read the papers from the Netflix prize about Napoleon Dynamite. Are you all familiar with this movie? So apparently Napoleon Dynamite was a hugely polarizing movie, and it also didn't relate to people's other preferences, and so including Napoleon Dynamite in the Netflix problem makes it way, way more difficult because it's almost like random how people will feel about the movie. So I thought that was kind of like a fun example of something that could really throw things off. And I know I'm very biased as an ex-CAGL person, but I was going to say a lot of people complain that the Netflix prize didn't lead directly to results, but the truth is actually the results of the Netflix prize made a huge impact on how people think about all kinds of areas of linear algebra and collaborative filtering and it's super valuable. So actually, yeah, studying competition-winning results is a great way to further your machine learning and data science skills. Thanks. Yeah, so just, I'm actually, should ask, have you seen the L, raise your hand if you've seen the L1 norm-inducing sparsity before, and we'll kind of review the concept of it. And this is a good thing to be familiar with. I've had this show up in data scientist interviews that I've done, but the idea is that, so with the L1 norm, which is just, just taking kind of the absolute values of your entries, so what the unit ball looks like is a diamond, whereas with the L2 norm, the unit ball is a circle. And so the idea here is kind of where you're looking for where this is going to intersect kind of with this other value that you're, or other curve that you're optimizing. With the L1 norm, that is way most likely to kind of happen at corners. Yeah, let me briefly, I guess, say the L1 norm should, let me write it on here, so L1, you're taking a summation of absolute values for each, if you have something in multiple dimensions, sorry, this should be an xi, so each of your x values for the L2 norm, you're summing the squares and then taking the square root of the whole thing. And these are, these are generalization, so actually, this you're taking the whole thing to the power of one, which is, which is nothing, or you know, has no effect, but this is generalization of this idea of the LP norm, but L1 and L2 are kind of the most common, oh, for, sorry, for regularization, and this is the penalty term that you're adding when you're doing some sort of optimization problem, and so the idea with any, any of these is that you don't want your weights to get too large, and so you're putting a penalty on the size of your weights, but the type of penalty you put will kind of affect what your answers are. And so with the L1, it ends up kind of saying, I want to keep this sparse, and so you're kind of getting more of a penalty for, you know, making new things non-zero, whereas the L2 norm, since you're trying to kind of keep this whole square root of sum of squares to a small value, you will have probably a lot of things with small values, whereas L1, it's better to maybe have one thing slightly bigger and the other weight zero. Yeah, so these are how those, how those penalties work. And so these are kind of some common pictures that people will show that if you, you know, have the problem you're trying to optimize, and then also this penalty kind of thinking about where they intersect for L1 norm, it's going to be a corner, which, you know, here the x-axis is zero and y is one, whereas here it's going to be kind of somewhere on the circle. Brad, can you throw the microphone, Jeremy? So one question I've always had with these, these pictures is that we're saying that with the L1 penalty, we can induce sparsity, but we can't with L2, even though when you look at the graph, those points at zero are there. Is that because... So what points at zero? At the north pole of the circle and at each pole, right? Like here? Yeah, or in the graphs above. You can technically see that you could hit those corners in the square there, but is the reason why we don't, because the circle is a continuum and the probability of hitting any one of those points is zero? I mean, I think it's more when you think about the structure of kind of this, kind of, so, and I'll look at this one. So here, these are kind of like the contours of what, in a lot of cases, this would be kind of like the air of your model. When you look at these, like it's very unlikely that this would be like perfectly aligned on the, I mean, it's possible you could have something that was like perfectly here and that like lying on the X axis, but that would just be, to me that implies like you had this very artificial data set or something kind of bizarre going on. So you're right, you could happen to hit those, but yeah, the chance is so minimally small that you're, and it also to me implies that there was something very like artificially constructed about the problem you were minimizing. Right. Tim? Yeah, I feel like the only way that like ellipse or circle would hit, like you'd have actually have it hit zero exactly is if the actual minimal, like the best solution was light on one of the axes, right? Yeah. So that would mean that one of the coordinates was zero in the first place. Right. So that would be the only way you would get that condition. Right. Yeah. Not only that it was like one of those had to be zero, but that implies there's like no noise in your problem as well, because it's like even if you had a little bit of noise distorting it, that's going to, yeah, take it off the axis. Right. Yeah. Thank you. Other questions? And so this, and we're actually not going to get kind of too much into the details of this, but I wanted you to see kind of how the robust PCA can be written as an optimization problem. And there it's, you're minimizing the nuclear norm of L plus some parameter lambda times the L1 norm of S. So it's, and subject to you wanting, you want L plus S to equal M. So M is your matrix that you're decomposing, and this is basically kind of just saying how do we describe a sparse and low rank matrix using math. And the way to describe a sparse matrix is to say we're minimizing the L1 norm. And the way to describe a low rank matrix is minimizing something called the nuclear norm, which is the L1 norm of the singular values. And so you can think of this as kind of results in sparse singular values. If a lot of singular values are zero, that means you have a low rank matrix. So this is just kind of how this is formally written out. And I should say we're not, definitely not going to get deep into optimization in this course, but if this is a topic that interests you, optimization is a huge, huge field. And I link to, in particular, Steven Boyd, who's a professor at Stanford, has an online class kind of videos through Open edX. And also, I just found this last week, he has like a tutorial or a short course he gave using Jupyter notebooks, and so I have links for that. Yes, yeah, yeah, the link is in here. And so part of what I want to show with this, so we'll be looking at an algorithm called primary component pursuit, which is one way of doing robust PCA. And again, robust PCA is this problem statement of decomposing a matrix into a sum of a low rank matrix and a sparse matrix. And so this is kind of an example of implementing an algorithm from a paper, and it's okay, we are not going to get into all the math details of this, but I kind of wanted to more show you some of the process of doing this. So the original paper is this one called robust PCA. So it's by a collection of researchers from Stanford, UIUC, and Microsoft Research. And then there's a kind of second paper that gives more details that's also used, and I have a link to that lower down. I also, so I'm looking at kind of encoding this, I looked at those, I looked at two existing implementations that I found online, and there are links here. So kind of a key thing to know is you don't need to know the math or understand the proofs to implement an algorithm from a paper, and it can also be very distracting to try to read all the paper, and some of this depends on your purposes and what you're trying to get out of it. But I think if your goal is to implement it or write the code, I think it's good to try to get there quickly, and you don't need to kind of go through all the theorems. So for example, let me go back to the Candace Lee Monwright paper. Okay, here it is. And so this is 39 pages. I think the introduction is really nicely written on this, and explaining kind of what robust PCA is. It also has some nice, and you can see I've based my notes on some of this, it gives several examples of applications. Number one is video surveillance, so this is great to see. But then if you scroll through, you'll see a lot of this is kind of theorems about the kind of the correctness of your results, which are gonna be less relevant. And so you actually have to go all the way down to, so we've got, it's telling you the architecture of the proof before you even get to the proof. So section two is the architecture of the proof, section three is the proofs, well part of the proofs, and I think section four is also, oh no, section four is numerical experiments, which is good. And actually this is kind of nice that they do show some, using it on the surveillance videos here. And they also used it on faces. Although I think the results, here they're removing shadows from the faces, so it's a lot kind of subtler to look for. I think you can particularly see it, like on this one, there's a pretty pronounced shadow here coming from the nose, and you can see that that's been removed coming over here. Although you've also kind of lost the the gleams in the eyes. But in terms of actually getting to the algorithm of this paper, it doesn't come till page 29. You'll find the algorithm. And so, and I'll go back to the notebook, I just kind of wanted to show you that I think it can be intimidating, kind of the amount of proofs that come before the algorithm, but if the algorithm is what you're interested in, please skip to the algorithm. Okay, here we are. And so here on page 29, we have the principal component pursuit by alternating directions. And the basic idea is that they will be taking, so and they define this down here, well so they define this operator cursive D, which is the, I think they call it the singular value thresholding, actually I should just show their definitions. Oh, here it is, okay. So you need to know the definitions of S, which they call curly S, the shrinkage operator and cursive D, the singular value thresholding operator. And so the shrinkage operator is basically, here it is, they take the singular values and subtract a value tau off of them. So you have this parameter tau, you take your singular values, subtract that off. If the values were less than tau, you just round them to zero. So you don't want to be flipping the sign on your values, but you just want to make everything that's more than tau away from zero a little bit smaller, and then if it was within tau of zero, just round it to zero. So you kind of have this, yeah, this process of making the singular value smaller. And then for curly D, let me find it here, that's the singular value thresholding operator. And so basically that's taking the SVD and then making your singular values smaller, and then recomposing your matrix. So you're kind of decreasing, you're both decreasing the magnitude a little bit, and you're taking some of your singular values to zero is a huge part of this, is that anything that was within tau of zero, you're just rounding to zero. So you're kind of making your singular values more sparse, which you'll remember is kind of a goal. So it's really very similar to just doing a truncated SVD and then model them back up together again. That's a great point. So it's, Jeremy just said, it's similar to doing a truncated SVD in that you can think of if the columns that you were chopping off all had tau or less for their singular values, that's what you've done here. You've kind of topped off everything that was less than tau by setting it to zero. So this is really a lot like a truncated SVD. Thank you. Which makes sense, because we just learned that that creates a low rank matrix. Yeah. And so, and going back, going back to this algorithm, kind of the idea is the low rank matrix you're creating is, yeah, the singular value thresholding where you've just chopped off a bunch of singular values because they were tau or less. The sparse matrix, here they're just kind of looking at the error that's still left from your original matrix minus your low rank. Because remember, you want the sparse plus the low rank matrices to equal your original matrix. And so you'll notice there's kind of this alternating that's happening between, okay, for my low rank matrix, that's approximating the original matrix minus the sparse one. The sparse matrix is approximating the original matrix minus the low rank one. And then this mu times yk is kind of keeping track of, I guess it's keeping track of what you still have left to approximate. And so here I really just want you to kind of get the broad strokes out of this. Like I think this idea of kind of the alternating back and forth between, okay, let's try to improve our estimate for the low rank matrix, okay, let's improve our estimate for the sparse matrix and kind of going back and forth iteratively is a really nice pattern. And then this idea of, yeah, like at the singular value thresholding being really similar to just truncating a bunch of our singular values. And then there's another paper that builds on that one, okay, I don't know what happened. Let me go back down. So there's another paper that builds on that one and kind of gives additional detail of how to, well, has, this is great, really takes advantage of actually calculating a truncated SVD. So, you know, even though we're kind of doing this truncation by throwing away a bunch of our singular values, we don't want to have to do the full SVD first. So the second paper kind of gives us estimates of like, okay, you can just calculate this many singular values each time, although we'll still do the thresholding where we truncate. Okay, that was weird. I just tried to click on a link and I don't know why it took me back to the top. Okay, here it is. Okay, well, so I'll just show the clips from it. So here in this second paper, so RPCA stands for Robust PCA, and they're using this optimization technique, alternating Lagrange multipliers, and they give some, so here are section four and this second paper has some really helpful implementation details. So in particular they give you a formula of how many singular values to calculate each time. And so basically it's just, if it's small enough, you're just kind of incrementing, otherwise you're taking basically like 20% of your dimensionality, so kind of doing the minimum of this SVP, so you don't want to be calculating more than 20% of kind of whatever the size of your matrix is. And it also, in section four, and this is always, this is great when papers have this, it gives values for the parameters to start with. And so let's get down to the code. Here are the links I mentioned to Steven Boyd's Open edX videos and his Jupyter notebooks. So I want to show, I guess like in particular, and it's definitely, it's good to keep, try to give, you know, have little methods and try to keep things I guess somewhat modular. I think one of the implementations I found online that was kind of all just in one giant method and really, really difficult to read, but having, you know, here calling it like a shrinkage method for the shrinkage operator that they've described, and remember this is the one that just subtracts tau. If that, if it was less than tau away from zero, you're setting it to zero, and so that's what this is doing. And then our SVD reconstruct is subtracting off, actually I guess it kind of contains that of subtracting off from the singular values and then kind of putting, or sorry, taking an SVD, subtracting off from the singular values, and then multiplying it back together to reconstruct. Yes, yeah. So, so here you could do this with, so you'll notice here we're just calling underscore SVD. Here underscore SVD we've set to be, to use FBPCA, which is a Facebook library for randomized SVD, let me pull it up, and I should have saved timings for this, but basically we just kind of tried this with several different SVD implementations to kind of try to find what was quickest, and this was fast so we went with it, but you could, you could still use scikit-learn's randomized SVD here if you wanted. And this is also I think an interesting point in that there are, kind of like the speed of your implementation with these things matters, and that there are different options, but some of them I at one point started working with this library called PyProPack, and I made a pull request to update it to Python 3, and the author of the library was like, oh, I no longer am supporting this, and it was like all based off of this library ProPack, which came out of someone's like PhD thesis 10 or 15 years ago as like this Fortran library, so you do kind of, sometimes kind of do get into these implementation specifics even in figuring out like which libraries to use. So Rachel is now the official maintainer of PyProPack, thanks to you, especially. Yes, so he did merge my, yeah, he was like if you want this library you can take over maintaining it, but although he actually didn't switch it off to me, but yeah, so FBPCA was fast, and it's, I'll just say personally I do find it encouraging when you see that there's like a big company supporting a library, because that does make me feel like it'll hopefully be maintained, but that's what we're using here, and it's a little bit confusing because they've called the method PCA, but it's a it's an SVD, and I find that a fair amount that there are people that almost use PCA and SVDs somewhat interchangeably. They are different, they're slightly different, yeah I'm not gonna get into it right now unless you wanted to talk about it. I mean they're basically the same almost, I think it's worth mentioning that PCA basically uses SVD. Yes, PCA uses SVD, PCA is typically you're multiplying the matrix by its transpose first. No, they are, sorry, I should mention they both do that, the only difference as far as I'm aware is that PCA subtracts the mean. Oh that's right, yeah PCA subtracts the mean, okay thank you. That is something to be careful of with subtracting the mean, if you start off with a sparse matrix subtracting the mean makes it no longer sparse, so you don't want to do that. Okay so we have, actually let me see if I want to say anything else about, about this. So this is the primary component pursuit method itself, yeah so here trans just transposes the matrix because it, this is quicker, you want your, I guess you want this matrix to be tall and skinny, so if you're given the reverse you need to transpose the matrix, do this, it'll be quicker, and then go back. Yeah so I'll just say like here we've got this loop that we're going through, we're getting a new estimate of our sparse matrix using the shrinkage operator, then we're getting our low rank matrix by reconstructing the SVD, we're kind of each time updating how many singular values we want to compute, seeing what our error is, and adding that on to Y where we kind of keep this, keep this total. Yeah and that's it. So next I kind of, or want to show the, the results that we get from this. So here it's kind of taking as inputs the number, the maximum number of iterations you want to do as well as a hyper parameter, and that, that hyper parameter I actually added because I was trying this on different videos and found that like it was needing different parameter values to converge, but you can see the error each time of how, how it's doing. And these are the results. So we've gotten back our low rank matrix and our sparse matrix and we can plot them. So the original, the sparse, and the low rank, and you will see there's still some kind of like blurs of people showing up that haven't been fully removed. I wanted to note that extracting a little bit of the foreground is easier than fully identifying the background, so you'll notice kind of like the middle pictures I think look better than the pictures on the far right, and that's because, I don't know, as long as you're close it'll look like people even if you don't have every piece of the person to the right intensity, whereas with the background, you know, if any of the persons remaining you kind of do have this little smudge or ghost, ghost image. Any questions? Okay, on that note, I may stop here before we start since LU factorization is kind of a big meaty topic that we'll get to next, and you'll remember LU factorization we saw was used in the randomized SVD that we wrote in that kind of a middle for loop, and it's also used in Facebook, the Facebook PCA at randomized SVD that we were using has it. So we're going to kind of dig in next time to how LU factorization works. Yeah, we'll just end I think five minutes early. Okay, thank you very much.

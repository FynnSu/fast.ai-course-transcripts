 All right, I'm going to go ahead and get started. So yeah, just to announce again, I won't be here next week, so no class on Tuesday, and then Thursday there will be the test at the kind of the final exam during the normal class time, and David Umansky will be here to proctor that. On Tuesday, the final draft of the blog post is due, as well as homework three, and homework three is just a single question. Yes, and I won't have my regular office hours tomorrow afternoon, but I could meet with you either this afternoon or tomorrow morning if you need to. So just ask me after class, and then I just wanted to thank everyone for being such an engaged group of students. I've really enjoyed teaching this class. This has been fun, so thank you. Yeah, so we'll get into it. We've run out of time a little bit, so we're not going to fully cover lesson seven, but I did want to go a little bit further in it. Just to remember kind of what we did last time, we were looking at... Thanks for pointing that out. Thank you. This will make class a lot easier to follow. Yeah, so last time we talked about eigendecompositions, and there's kind of this close relationship between the eigendecomposition and the SVD. So even though we're looking specifically at how to do eigendecompositions, similar techniques are used for calculating the SVD. And so we were looking at this DDPedia data set, which is a data set of all the Wikipedia links, and we were finding the kind of the principal eigenvector of that. And what's significant about that eigenvector? What information is it giving us? Wait, hold on. I think the most significant is that we can easily find the any power of the matrix just by multiplying the eigenvalues and finding this diagonal of the matrix. So that's something that's really powerful about having the full eigendecomposition, is that you can quickly take powers of matrices. Yes. And then what's kind of the meaning? So we're actually just finding... So you need kind of all the eigenvectors and eigenvalues to do that. For the DDPedia data set, we're just finding a single eigenvector. Okay, what's the meaning of that eigenvector? Matthew and Valentine, can you throw the microphone to Matthew McKelland? Good ducking. You're talking about the solution vector? Yes. Isn't that the probability, the overall probability that it'll end there if you did a random walk? Yes, yeah. So it is kind of telling you the probability that you're going to be on each page if you're randomly walking. So what does that mean for ones that have higher probabilities? Maybe it's more likely to end there, I guess. Yeah, yeah. And so this is kind of the idea behind PageRank, which was Google's original algorithm, but it's giving you kind of the most important pages because people are more likely to end up there. Thank you. So yeah, that's kind of the idea that even though it's a single eigenvector, it's actually pretty important because it's giving you... Some people call that the stationary distribution when you have a Markov chain of kind of where things will end up. And for web pages, that's kind of their relative importance. Matthew? Is there some meaning to the actual value of the matrix? Have you keep multiplying them? Yeah. So we talked about... We had kind of our original matrix, which was the graph adjacency matrix, and those were zeros and ones. And there, when you take values... So once you start taking powers, they can kind of be any integer values, and that's giving you the number of ways you could get from page A to page B in k steps. So if you took the third power, that's kind of saying how many ways are there to get from A to B, taking three hops. So you could go... Yeah. So you could like, with normalize, you could look up one and you'd go... Yeah. And so then it... Exactly. Yeah. So then it becomes a probability when you normalize. Yeah. Good question. Yeah. And so the method we looked at last time, the power method... Go down to it. I would just highlight again, since we've talked about sparse matrices a lot, here we kind of very naturally had our data in the coordinate sparse form, since we just know kind of this page links to this page, and we don't explicitly have... This is all the pages that it doesn't link to, and there's not really a point in storing all that. So we kind of use coordinate form to create a sparse matrix, and then we switch to compressed sparse row format. And why did we switch? Sam? Because it takes half as many reads from memory. Exactly. So for matrix multiplication, it's the same number of floating point operations, but half as many reads from memory to be in sparse row format as opposed to coordinate, sparse coordinate format. Yeah. And the kind of general idea behind the power method is that we're multiplying A times... I called the scores because it's kind of the score of how important each page is, but you could also think of that as the probability of a person being on a particular page, and we're just doing that a bunch of times, but normalizing to make sure that things don't go to zero or to infinity. And then something I wanted to point out is, if you remember back in lesson three when we did the surveillance video background removal, we used Facebook's fast randomized PCA SVD library, FBPCA, and in the source code I found a comment where they're using the power method at one point, which I thought was exciting. So showing up in the real world. Any questions on that and what we did last time? Okay. So we'll go on to the QR algorithm. So last time we were just finding a single eigenvector, and the QR algorithm lets us find potentially all the eigenvectors. This is a paper called The Second Eigenvalue of the Google Matrix, which I thought was neat, and it kind of lists some benefits of what you can get from even just the second eigenvalue, and that's that it relates to the convergence rate as well as the stability given kind of as the structure of the web's changing, detection of spammers, and for the design of algorithms to speed up paint drink. You can check that out for more information. This is, yeah, again kind of more practical uses of the second eigenvalue. So I just want to avoid confusion. So right now we're going to talk about the QR algorithm. Later in class we'll talk about the QR decomposition, and the QR decomposition is something we've been using from time to time, and the decomposition is what takes A and gives you back Q and R, where Q has orthonormal columns in the reduced form, or for the full form it's an orthonormal matrix, and R is upper triangular. And then the QR algorithm, which is different, uses the QR decomposition, so just to keep those separate in your mind. It's a little bit of linear algebra review. Two matrices, A and B, are similar if there's a non-singular matrix X, such that V equals X inverse A X. Does anyone remember what non-singular means? Yes, it has full rank, and that's equivalent to saying it has an inverse. Exactly. So this relationship, so we watched a three blue one brown video last time. What's the kind of analogy they used about similar matrices? Kelsey? Sam, can you pass the microphone? They were using like a language map? Yes, yeah. What's happening between A and B? That they're like, they're different coordinate systems for expressing them the same. Exactly, yeah. So A and B are kind of different coordinate systems, and we can think of kind of applying this X as a way to translate between them. So we're wanting to kind of translate from A to B. And they also, they had a nice, I think at one point they described this whenever you multiply by X inverse in front and X on the right hand side, that that's a kind of a mathematical form of empathy, but transforming A to kind of understand B better. So there's a theorem that A and X inverse A X have the same eigenvalues. So this is going to turn out to be useful, but A and, so here A and B have the same eigenvalues if they're related this way. And then, yeah, I apologize that there's a new vocabulary, but a sure factorization is a factorization basically exactly like this, only if the matrix in the middle is upper triangular. So here you would still say A and A and T are similar and T's upper triangular. Actually first I'll ask, does anyone know what the eigenvalues of T or of an upper triangular matrix are? You want to grab the microphone? Exactly. Yeah, so the eigenvalues of a triangular matrix or the diagonal. So what would the eigenvalues of A be in this case? You can say it, yeah. Yes, so this is why the sure factorization is going to end up being useful is that we're, you know, finding a matrix that's similar to, you know, there's nothing special about A, like it's not necessarily triangular, but it's similar to a triangular matrix and then it's really easy to get the eigenvalues of a triangular matrix because you just grab the diagonal. Any questions? Okay. And so we're kind of going to go over the the basic version of the QR algorithm, which is really simple. Then you have a loop and you get the QR decomposition of A and then you kind of set your new A to be equal to R times Q. And under certain conditions this algorithm is going to converge and give you the sure form of A, which means that it's going to return something triangular. So now I've kind of written this, so here I've got some pseudocode. I've written it again with subscripts, and so I'm saying QK, RK is equal to A from your previous iteration, so that's just the QR factorization. And then we're setting our next K equal to R times Q, which is different than the matrix we just factored because we're taking the factors and switching their order. So one way to think about this is, so AK equals QK times RK, because that's the definition of a QR factorization. Then we could multiply by Q inverse on each side and get Q K inverse times AK equals RK. And why is that when I multiply by Q inverse? Louder. Just how did I get from this line to this line? Wait, grab the microphone. It cancels Q on the right side? Yeah, exactly, because Q is orthonormal. Yeah, thank you Sam. So Q is orthonormal, so it cancels out when we multiply by Q inverse. So then we can kind of take this equation, and what happens when we multiply R times Q? We can say, oh really that's like saying Q inverse times A times Q. So I've just plugged this in for RK down here, and I get Q inverse AK QK. And so this, and we're not going to do kind of all the proofs for, there's some other relationships, but the idea here is that by getting the QR factorization and then switching, we're kind of just repeatedly applying these similarity transformations to A. And so after a bunch of iterations we're ending up with QK inverse, oh there should be some dots here, let me fix that. Okay, QK inverse dot dot dot on down to Q2 inverse Q1 inverse times A times Q1 times Q2 dot dot dot up to QK. So what we're getting back is something that's definitely similar to our original matrix we started with, and then Trevathan has a proof showing that if we were, so I was using these subscript K's to talk about the values we were getting each time we iterated, so just to denote that we had a different QR and A for each iteration. Here I'm taking a power again, so A raised to the Kth power is going to be equal to Q1 times Q2 dot dot dot QK times RK, RK minus 1 dot dot dot R1. And so you can look in Trevathan on page 216 to 217 for the proof of that if you're interested. But that kind of, the main idea here is that the QR algorithm is coming up with orthonormal bases for successive powers of AK. And so let's see what this looks like in code. So this is the pure QR algorithm. I'm making a copy of A, but you often would do this in place, and when you use something in place you're, you know, writing over your original matrix, which saves memory, although if you wanted to use your original matrix again later that would be a problem. And so I'm just going through a bunch of iterate, well, so I initialize Q to be the identity, and then I am getting the QR factorization just using NumPy's implementation. Get back Q and R, set AK equal to R times Q, so I flip the order, and then I'm keeping this kind of a running total of what happens each time you multiply Q by each other, and then at the end I'm returning AK and QQ. And so if we do that on, this is just a randomly generated matrix A. Let me do this with fewer iterations, this is a lot. And so I'm just printing it out every hundred iterations so we can see how AK is updating. So what do you see happening? So this is our original, actually this is, this is after ten iterations, but it's still kind of a full matrix. Let me put the original A here so you can compare. So even after ten iterations we can see the bottom left-hand corner. This should be larger, perhaps. Are you able to read this all right? The bottom left-hand corner is starting to get closer to zero, right? We've gone from these values of like 0.4, 0.5, down to 0.06. Whoops. And then as it goes, now we're getting some zeros along this kind of left-hand triangular, getting more, more. And so what is this matrix heading towards? What type of matrix? Exactly, triangular. So we can see if we're getting kind of more and more zeros in this bottom left part. And so here I've just done this for a thousand iterations. We don't quite have a triangular matrix, but we've gotten quite close, right? So we still have this 0.47 here and here, but the other values are zeros. And if it was perfectly triangular, what would we expect about the eigenvalues? Exactly. Sam, can you pass the microphone to Linda? Thank you. The eigenvalues, yes, yeah. So the eigenvalues of this AK or T that's triangular would be equal to the eigenvalues of the original. And so here we can compare that. What are the eigenvalues of A? Notice the first one, 2.78, that's the first value here. So we did find that eigenvalue. We also have negative 0.11832, that's this value. So we've, let me see if we have any more. Okay, so here it looks like we've just gotten two of the eigenvalues. But the idea is that when and if this converged, you could have all of them. And we can also check that the Q we get back is worth the normal. So downsides to this algorithm are that it's really slow, and it's also not guaranteed to converge. But I think it is neat that we get, yeah, that we can get some of the eigenvalues with this because it's a fairly simple approach. This idea of just taking the QR factorization and then switching them and doing R times Q and taking the Q factor, QR factorization of that, switching those R times Q and getting the QR factorization of that. Any questions about this, about the pure QR algorithm? Okay, so practical QR algorithm is adding shifts. So the idea is basically just instead of factoring AK as QKRK, we'll get the QR factorization of AK minus some scalar times the identity into Q and R. And then we'll do R times Q. We have to kind of add back on the scalar times the identity to kind of cancel that out. And that will be our new AK. So it's quite similar. It's just we've basically subtracted something from the diagonal, then the QR factorization, multiply R times Q and add back on what we had subtracted from the diagonal and keep going. And this is called adding shifts. The SK is kind of the shift that you're like moving, moving it over a little bit. And this speeds up convergence and also can help it converge in cases where it wouldn't otherwise. And I really, I wanted you to see this because it shows up in a lot of different numerical linear algebra methods as a way to speed things up. And this will be, this is the homework 3 problem, is to take the QR algorithm from above and add shifts to it. So that'll just be kind of modifying this algorithm up here to no longer be, no longer doing the QR decomposition on AK, but using AK minus the scalar times the identity. And the recommendation for, so ideally you want SK to be an eigenvalue that you've already calculated. And you can kind of just use the diagonal of where your current, or use a value from the diagonal of where you're currently at for that. Any questions about this? Okay, so in that case we're gonna move on to, oh actually I should just comment a few things. So this is a lot better than the unshifted version since that was not guaranteed to converge and was really slow. But this is still order of n to the fourth power, which is bad for a runtime. For symmetric matrices it would be O of n cubed. So we've got a method for finding all the eigenvalues, but it's really slow. And so what's done in practice is that if you start with something called a Hessenberg matrix, which has zeros below the first sub diagonal, it's a lot faster. And so in practice what you'll do, so this picture is from Trevathan, is you'll have, so your QR algorithm is gonna end up being phase two, but you'll have a phase one that introduces a bunch of zeros and gets you to something that's almost triangular. So you'll see this matrix H, it's like a triangular matrix except it has an extra, this is called a sub diagonal, but right below the diagonal there's some nonzero values. And so you'll kind of use one algorithm to get things to this Hessenberg form, and then you could use the QR algorithm to get from Hessenberg to your triangular matrix. Matthew and Linda, can you pass the microphone? So you could, it's just really slow. So this is a way to feed it up, speed it up, is that this phase one is going to be much quicker. Yeah, but it would work if you had the time. So it's like, well we're using a new algorithm here, so then yeah, kind of once you're at this Hessenberg, then it's faster to just get to the, yeah. Yeah, good question. Okay, and so yeah, we're not gonna, we're not gonna talk about phase one. If you're interested, I've written about it in the notebook though, and so you can go through this later on your own. Yeah, I just wanted to kind of at least give you a little bit of a map of how finding the eigenvalues works. And this is, like I would say, a fairly complicated area of numerical linear algebra just because you're kind of having to like switch between algorithms, and there are like different versions of what you can use for phase one or phase two. Okay, so now we're gonna start notebook eight. This is, this is exciting, final notebook. And then this is also, we're gonna learn how to implement the QR factorization, which is something we've been using kind of for most of the course. So you'll remember we used it just now for computing eigenvalues. It was also a way to compute the least squares regression. It showed up in the primary component pursuit, how we did robust PCA. It showed up in our randomized rangefinder. So it's a really important factorization. In fact, Trevathan says one algorithm in numerical linear algebra is more important than all the others, QR factorization. So real, kind of real fundamental building block. So here I'm just illustrating, so we're using NumPy's QR factorization. We take in matrix A, we get back Q and R, and we can confirm Q is, Q is orthonormal and R is triangular. And so today we're gonna actually talk about three different methods for finding the QR factorization. And there'll be kind of some fun examples at the end of how they have different stability properties from each other. But first I wanted to review the idea of projections in linear algebra. So the idea of a projection is that, actually here I should go ahead and go to this link. This is, there's something called the Immersive Linear Algebra textbook. And it's still, I find it to be like fairly mathy in its explanations of things, but all the graphics are interactive, which is kind of nice. So I was just going to show their, their example for projection. If it loads. Okay. So the basic idea is here we have a vector U that we're projecting onto V. This is their diagram for 3.4. And then W is the projection. Move you around and see how W is changing. What it is, is you can think of it as you're kind of decomposing U into this component W that lies along V, as well as something that is perpendicular or orthonormal to V. So you're kind of taking out all part, like any part of U that can be represented by V is being taken out. That's W. And then you're just left with this like orthonormal part. And this dotted line would be U minus W. Are there questions? Just about the kind of like idea of you're getting, getting the part of U that lies along V and then what's left over has to be perpendicular to V. Okay. So you can also do this with a plane. And I took this from a blog post that I thought was pretty good that you might like called the linear algebra view of least squares regression. So I just wanted to show you that that's out there, which we've kind of covered somewhat when we covered least squares regression. So here we've got a plane A. And really this plane is like the column space of the matrix A. And a vector B. And now we're trying to find, you know, what's the vector that lies on the plane that kind of captures, you know, every part of B that's parallel to the plane. So we get that. And then we've got this B minus P and that's perpendicular to the plane. So we can get the formula for projection coming from the idea that, okay, this dotted line must be perpendicular to the projection. And so here, and I apologize for the different variable names, B minus XA dot with A is going to be zero. So in this case XA is the projection of B onto a line A. And X hat is kind of the scalar that you're multiplying by. And those are dotted with each other because that's what's true of perpendicular is just kind of like another name for orthonormal. Or sorry, orthogonal. They're not necessarily going to have length one, but yeah, they're orthogonal to each other. And so then to find what the scalar is, you can do A dot B divided by A dot A. Questions on projection or orthogonality? And Trevathan, I think, has a whole section on projectors in the kind of in the first part of the book since it is a concept that shows up a lot in numerical linear algebra. Okay, then we'll get to, so it's going to turn out projection is really useful for Gram-Schmidt. So classical Gram-Schmidt, the idea is, so remember our goal is we're trying to come up with a QR factorization, and in Gram-Schmidt we're going to come up with one column of Q at a time. And the idea is kind of iterate through, and each time we want to calculate a single projection. So projection PJAJ where you're kind of projecting onto the the space orthogonal to the span of your your previous Qs. So what this looks like code-wise is, so we're going to just pick off the jth column of the, and then, well the first time through, I guess we still, so we need to project, we need to project A onto our already existing Qs. In the initial case we don't have any, so you can kind of just like take the the first column of Q and have it normalized, or sorry first column of A, normalize it, and that's going to be your first column of Q. And then to get the second column of Q, you want to kind of take the second column of A, project it onto the first column of Q, subtract that off, and then that'll be your second column of Q once you normalize. And you go through, and so each time you're kind of taking a column of A and you want to represent it, but you need to kind of subtract off everything that's already been accounted for in your Q. And so as you do this you're creating, you know, these orthonormal columns of Q that are going to represent your columns of A. And so we can we can test this out. So we want to confirm that this algorithm, so to make sure that you have like a valid algorithm that's actually finding your QR factorization, you want to check that Q times R gives you your original matrix back. That's an important part of being a factorization. We want to check that Q is actually orthonormal, which is what we expected. And I probably should have checked that R is actually triangular, and it is, oh in this case maybe A is not, let me see, A might not be square. But yeah, that you're getting something triangular back. So that's that's Gram-Schmidt. Questions about classic Gram-Schmidt? Kelsey, and can you pass the microphone? Okay, yeah. Let me see. This will work to let me... okay. So I'm gonna see if I can put maybe put them side by side so you can kind of, whoops, see the algorithm and some pseudocode. Let me just see if Trevathan writes this out. Actually, okay, so one thing that Trevathan has that might be helpful is what you're coming up with. So here A1 is going to be the first column of A. And basically that's just going to be a multiple of Q1. So we're trying to build an orthonormal basis, starting off with A, and actually let me go ahead and leave myself space. Put this over to the side for a moment. So you can think of, we're kind of starting with, we've got these columns A1, A2, up through AN. And we want to end up with this being, we're kind of trying to find, you know, what can we choose for these columns Q1 up to Q, QN, such that we've got this triangular matrix R. Those are all zeros. And so a way to think about that is that you're getting a set of equations, actually maybe I'll put this full screen just for a little bit and then I'll go back. You're getting the set of equations, A1 is going to be just a multiple of Q1, and we're going to store that in the this rectangular matrix R. So how would, so we have A, how would we choose what R11 and Q1 should be? Any ideas? Matthew? You can pass the microphone down. Well so we don't, we don't have the Q's or the R's yet, we're trying to find them. But basically, yeah. So that's, that equation would hold, but what other property do we want Q to have in addition? Yes. Well so, you're very close, but yeah, does Tim want to chime in? So yeah, Matthew's suggestion was kind of choose Q1 to be equal to A1, which is close. Tim? You just said R11 to be the norm of A1. Exactly. Yeah, so you want to, you would set R11 to be the norm of A1, and Q1 is just A1 divided by that. Yeah, so the idea is you need to normalize it because Q1 has to have length one, is kind of the additional property of an orthonormal set of vectors. And so take A1, normalize it, that's Q1, and then R11 should be the norm so that you get the right magnitude back. And kind of thinking about that in terms of vectors, it's Q1 is pointing in the exact same direction as A1, but it's just been scaled appropriately to have length one. And let me even maybe write that. Like if this, this is A1, remember A1 is a column so it's a vector, then Q1 is going in the exact same direction, except it just has length one. Oops, not a very clear one. And then the length of all of A1 will set to be R11, or rather we set R11 to be the length of A1. Okay, so what's the, what's the next equation I could write? Yes, Matthew? A2 equals R12 times A1, or Q1 plus R22 times Q2. Yes. Yeah, that's great. So A2 is going to equal R12 times Q1 plus R22 times Q2. And so that's just coming from, we're kind of taking the second column of R, let me scroll over it, second column of R, multiplying that by the matrix Q, which just picks off Q1 and Q2. Remember everything below R22 is zero, so we have zero coefficient for Q3, Q4 up to Qn. And so taking this linear combination of the columns of Q, we get R12 times Q1 plus R22 times Q2 equals A2. And so at this point, we know what A2 is, or sorry, no, we do not know, oh yes, we know A2, because that's our original matrix A, so we've got A2. We know what Q1 is, because we just solved for that in the previous step, and so now we're trying to find Q2, R22, and R12. And we have this constraint that we're going to want Q1 and Q2 to be orthogonal to each other, and Q2 is going to have to have norm 1. And so that gives us enough information that we can, what we can do is project A2 onto Q1, and we'll get kind of R12 is the scalar we need, given that, kind of to get back to the full kind of component of A2. And then we can find Q2 is just kind of what's left over, the perpendicular part. Questions about that? Matthew? Yeah, so at that point what you're doing is you're gonna have A2 minus R12 Q1, and so in a picture this A2 minus R12 Q1 would be the dotted line, and actually let me draw that picture. So if this is A2, then maybe this is Q1. Remember Q1's only got length one, so different colors, and this is, these are perpendicular to each other. So the idea is we've projected A2 onto Q1, and we get that R12 is the scalar, you know, since we might have to scale Q1 to be the appropriate length. And then now we're interested in this kind of perpendicular piece that's left over, so this part in red. It's okay if I erase this first diagram over here. Everyone? Not get too crowded. Some space. So then in red what we have is this whole thing. Whoops, that's not red. This thing is A2 minus R12 Q1, and we're going to want that to be equal to R22 Q2. So how do we get from, so we've got A2 minus R12 Q1, how do we get this from this red vector? How do we decide what R22 is and what Q2 is? Exactly, yes. Yeah, so we normalize them because again, let me draw it, so maybe this is what length one is, so that'll be Q2, and then we have to use R22 to get the right length. So Q2 is giving us the direction, but just with length one, and R22 gives us the correct magnitude. Are there more questions about this? Yeah, this is a great question to kind of draw this out in more detail. Linda? Can you pass the microphone? Can you go through Q3 so we can see what's next? Sure, yeah. Yeah, let's go through Q3. And this will get a little bit trickier with drawing just because it's going to be in three dimensions. Okay, so now we've got A3 equals R13 times Q1 plus R23 times Q2 plus, on the next line, R33 times Q3. And so that comes from taking the third column of R, and that's giving us a linear combination of the QI, and now we're just picking off the first three QI, then everything else zeros out. I'll also just put a line to kind of separate this part. Can I erase the drawing from finding Q2? Okay. Let me just go down to have a little bit more space. So now we're gonna have to imagine that we're in three dimensions. I'm gonna draw a background. So we're in three dimensions, and we've already found... I'm actually gonna make length one a little bit longer just so this doesn't get too tight, but this is Q1 and this is Q2. So we have two vectors, and these are orthogonal to each other, and that can be... you know, remember we're in three dimensions, so that can look different depending on kind of what angle we're viewing this from. But we've got this Q1 and Q2, and we're interested in taking our vector A3, and we want to find a good Q3 for it. So what should we do first? Exactly. So we're gonna project it onto Q1. Sorry, my one looks like a two. Yeah, so we project it onto Q1. I probably should have drawn this. I'm gonna redraw A3 just so that it is clearer what its projection would look like. Okay, so maybe A3 is actually off in this direction. We project it onto Q1, and then we're interested in that, and this case is also gonna kind of have to be extended out to be the appropriate distance. So we subtract that off, and then what do we want to do next? Matthew, and can you pass the microphone, Linda? Do we have to take care of the Q2 projection? Exactly, yeah. So we'll project onto Q2 to kind of get that part, and so in this case... yeah, and it's really hard to visualize these in three dimensions, but the idea is that, the way I've drawn it, there might be like very little of A3 that's represented on Q2, but we've got some component here of A3 was represented from Q2, and part of it was represented with Q1, and then we're interested in what's left. So we're kind of looking at... I'll do these in black. So we kind of have like the Q1 part of A3 and the Q2 part of A3, but then that probably doesn't capture all of A3, and so whatever's left is gonna be the Q3 part of A3. So we would do kind of A3 minus, you know, this Q1 times a scalar that gets this direction, minus Q2 times a scalar that gets this direction, and then what we're left with is Q3 once we normalize it. Yeah, I guess, I don't know, maybe in this case that would be like... maybe there's something like going a little bit this way that hasn't been captured so far, and that could be... and that could even be... like that would be R33 times Q3, and then we normalize it to get, okay, this is Q3. Questions about this? All right, and we are kind of well due for a break, so it's 1204, so let's meet back here at 1210 to continue. Thanks everyone. All right, let's go ahead and start back up. So I wanted to kind of go back to the code now that we've seen these pictures of what's going on, and so what's happening with this inner for loop... so the outer for loop is where we're trying to find like a particular Q, Qi or Q2, and then in this inner for loop is where we're doing the projections onto the previous Qs. So for the case of Q3, we're trying to find this, you know, for kind of outer loop J equals 3, and then we go in here and we project the third... A3 onto Q1 and subtract that off, and we project it onto Q2 and subtract that off, and then that's going to leave us with what we can use for our Q3. No more questions about this? Okay. Yeah, so this is what we saw. This is perhaps like the most straightforward way of coming up with a QR factorization. Unfortunately, it turns out it's unstable, which we'll see an example of later on. So we're going to use the... Our next look at the modified Gram-Schmidt. And modified Gram-Schmidt basically... So in classic Gram-Schmidt, for each J, you're coming up with this single, kind of single projection onto your QJ to get the kind of the final like this is Q3, you know, we've projected onto it. Where, you know, like and in that case, you know, Q3 is the projection onto this space that's orthogonal to Q1 and Q2. For modified Gram-Schmidt, you're doing... Sorry, hold on a moment. Actually, I think what I showed you is really modified Gram-Schmidt of doing the projection separately. Oh, we're gonna... No. Yeah, really, in some ways this is good because modified Gram-Schmidt was actually used that were... Like for each J, we're kind of calculating onto... Yeah, getting something that's the part that's perpendicular to Q1 and to Q2. Let me... I'll email you about this. I'll think about this some more. Okay, no, no, because like... Okay, so with this one, I was starting with the column and then kind of subtracting off how it projected onto the other ones. With modified Gram-Schmidt, I... Really, I'm kind of starting by normalizing my vector and saying that that's Qi. Yeah, let me continue and I will email you an update about modified Gram-Schmidt. But keep that picture of kind of like... The general idea is that Gram-Schmidt is all about projections and that it's about constructing both classical and modified Gram-Schmidt are constructing your matrix Q one column at a time and that kind of each outer iteration at the end of it you want to have an additional column of Q. And so this is kind of a point about Gram-Schmidt. If you were to stop part of the way through, you would have, you know, like a partial set of Qs that are orthonormal to each other. And so Trevathan refers to this as triangular orthogonalization and that you're kind of just coming up with this one Q. But you could actually think of that as being a lot of smaller, not smaller, but less informative triangular matrices because basically kind of doing these projections could be represented by this triangular operation. But you're just getting one Q and kind of the focus is on building Q. So Householder is a different approach where basically you're focused on zeroing out the bottom triangle in A and so kind of the whole focus is putting in basically each iteration for Householder you're gonna take another column and make everything below the diagonal zeros. And you keep doing that and it turns out that that's equivalent to multiplying by a orthonormal matrix each time. So kind of like, yeah, I guess the opposite thing is center. With Gram-Schmidt the central idea is, yeah, getting these Qs and you're focused on what do we need to get the next Q. With Householder it's how can I zero out some more entries below the diagonal. So with Householder, Householder I think is a lot a lot harder to think about conceptually. I even think I might, yeah, we're not gonna get into the details of the algorithm. Let's check that what we get. So there's this, there's also this concept called Householder reflectors. And so it's basically, this is where I put my stylus. Each time with a Householder you're multiplying by a block matrix where part of the matrix is the identity, so block matrix is a way of kind of putting together smaller matrices into a bigger matrix. And part of that is the identity matrix and then part of it is this special matrix called a Householder reflector. And this is kind of designed and what's gonna happen is that I is getting bigger and bigger as you iterate. Let me check back. But you're going in one direction of I either getting bigger or smaller, F getting smaller or bigger, and this whole thing is orthonormal. And the idea is that multiplying this times, it's on this side, this times A is going to zero out an additional column. So you'll get kind of more zeros. So you're kind of constructing, constructing these matrices. Um, yeah. So we calculate Q and R using these block matrices F. And here I'm kind of just showing, okay, so it must be that I, yeah, I starts out small. So this is a one by one identity matrix right here. And F is three by three. Actually, then we can see this would be nice to look at. Now, we've got a two by two identity matrix in this corner, and F is two by two. Here I is three by three, and then F ends up having to be one since it's got to be orthonormal. It's either going to be one or negative one. You're kind of constructing these. And the key is you get back Q and R, you can check. Well, so actually, Householder is described in Trevathan doesn't explicitly calculate Q. It just calculates R. It turns out for most problems you don't need, you don't actually need Q. And there are kind of these implicit methods that can use the Householder reflectors to get, to get what would Q times, Q transpose times B be or Q transpose, solve QX equals B. So either like multiply or solve that equation. I mean, this is something that kind of shows up in numerical linear algebra periodically is that you don't always like need the explicit matrix. You just need to know what is that matrix times a vector or what is the solution of that, can I solve a system of equations AX equals B when I know A and B. Yeah, so I think I'm going to go on to, yeah, sorry that I'm not getting into the details on these. And Householder is covered in Trevathan, but it's a less intuitive algorithm than Gram-Schmidt where you're taking these projections. And I want to see, like I think part of the fun part is kind of seeing how these are different from each other numerically. And I wanted to definitely make sure we get to this with plenty of time. So this is coming from lecture nine of Trevathan. It has a few examples of how they vary. And so one of these is comparing the classic versus modified Gram-Schmidt. And so here we're going to kind of intentionally construct a matrix that has singular values spaced by factors of two between two to the negative one and two to the negative n plus one. So that means the magnitude of the singular values is really varying logarithmically. And so kind of to make this matrix, we're taking NP dot power of two then to this range from negative one to negative n plus one. And so we're using the idea of a kind of SVD to construct a matrix that has the singular values we want. So we'll just get kind of two orthonormal matrices for U and V and then S, we're putting what we want along the diagonal and multiplying U times S times V and that'll presumably have this, will have the singular value decomposition with the values of S that we've created. I should stop. Are there questions about this way of constructing a matrix to kind of make sure it has the singular values that we want? This is the reverse of the SVD is we're coming up with two orthonormal matrices and a diagonal matrices and multiplying them together. Okay, so we create a, we do a classical Gram-Schmidt and modified Gram-Schmidt on it. And then actually I should have checked that. And these might end up, I'll see if this works, they might be off by a sign. So you can have variations with kind of where your negative signs are. Let's get the, oh actually no, the point is these are going to not be the same. So ideally we want these to be similar to each other, but it turns out that they're not. So we can graph S, which is what we know the true singular values to be, and those are the red dots. And then we're graphing the diagonal of R and the diagonal, the diagonal of RM, which is the modified Gram-Schmidt's R, and the classic Gram-Schmidt was the blue. And so first I'm going to ask you why, why should the diagonals, why would we expect those to be similar to the singular values or to be the singular values? The diagonal of R. Oh wait, grab the microphone. So the, yes, well there's another piece that I think you're probably thinking, and why are you talking about eigenvalues? Can you talk in the microphone? Right, so the eigenvalues are related to the singular values, and then how is that related to the diagonal of R? Linda, do you want to say it? It's a triangular? Yeah, since it's a triangular matrix, the diagonal, yeah, it gives you the eigenvalues, which are the singular values in this case. I have a question. Is the singular value also eigenvalues? So how do they relate it? How do they relate it? So singular values, yeah, so singular values are like a generalization of eigenvalues, and the idea, let me go back to the notepad, is that singular values we can think of as A times V equals sigma times U, and then eigenvalues can be thought of as Ax equals lambda x. And so notice that this first equation, typically like when we're talking about the SVD, we're writing out the full matrices. This is just thinking about particular vectors and they're, you know, like one, kind of one vector at a time, or one pair of vectors being U, and one singular value from the diagonal. So this is, you know, coming from A equals U sigma V, and we would have a number of these for different singular values. Kelsey, can you pass the microphone? Linda? Is the singular value kind of like the stretching that you're doing? It is, yeah. The stretching that you're doing. Like if A is rotating and like elongating, or? Yes, yes, exactly. Yeah, that's a great thing to note. So it's kind of, with singular values, then you'll see these pictures, maybe you're taking a circle, and perhaps A transforms it to an ellipse. So if matrix A changes the unit circle into this ellipse, you're getting the vectors U and V are basically like a change of basis. And so you can think of kind of, this is really relates back to the three blue, one brown video that we're kind of interested in, like what's the, I don't know, like more natural basis for this ellipse, and that would be like, oh that's a different color. You know, like really like these are the axes of that. And so those vectors, kind of the directions are coming from U or V, and the magnitude is coming from sigma, as Kelsey said. And here really that's like, you know, what was just a unit axis. So hopefully that looks familiar in thinking about change of basis. And so singular value decomposition is really a change of basis. And then kind of going back to, or actually first I should ask, are there questions about that? This idea of SVD being a bit like a change of basis. Sam, can you pass the microphone back? So would you be, would you have a magnitude of one in the basis provided by A? Like in this case you're stretching it and elongating it. So the elongation would be with respect to the original axes. So I think, so the elongation is captured by the sigma so that you... So I guess my question is, would you have a magnitude of one in this case? So you, you do have a magnitude of one, yes. And so looking at kind of the eigenvalues, eigenvectors, so here this is a very similar equation and the difference is just that you don't have separate u and v, you just have x. And so this, this could be written, why was I able to change the order of sigma and x when I went to the matrix form? Yeah, because sigma is just a scalar. So you're in, you're just putting those on the diagonal of your, your big sigma, or sorry, big lambda. And so then, whoops, this is this, which should look familiar of kind of this is what it means for A and lambda to be similar. We have this change of basis and that's what the eigen decomposition is doing. Do all, do all matrices have a full eigen decomposition? Thumbs up for yes, thumbs down for no. Okay, good, I see mostly thumbs down, yeah. So not all, not all matrices have a full eigen decomposition. A full eigen decomposition is where you can get a matrix X that's orthonormal, or a matrix X of full rank. And then, but all, all matrices have a singular value decomposition, so it's kind of more general, you know, it doesn't have this additional constraint on it. So kind of going back to this, this comparison of, you know, we're trying to get the true singular values, we know what they're supposed to be, we look at the diagonal, and we're definitely, we're right up here, like it seems, you know, both classic and modified did well. Does anyone want to guess what one of these places where it's leveling off is? This is a concept or term that we talked about in lesson one. Any guesses? Kelsey, I'm gonna grab the microphone from Sam. Yes, machine epsilon. So this is the, and in particular that's for a modified Gram-Schmidt. It can't capture something that small. Tim? Why does it look like it levels around to the negative 57? Yeah, I am. I'm not fully sure about that. That was like close enough that I was like the seams right on, but yeah, I don't know why it's, yeah, not, not perfect. Good question. Yeah, so I've written down here, so there's a way from numpy.finfo to find out what numpy saying machine epsilon is for float 64. And so, and remember we've, you know, we're graphing this by powers of two. We get that epsilon is around negative 52. This is close to that. And then the square root of epsilon is around to the negative 26. Remember when for values less than one, square root makes things larger, not smaller. It's kind of a reverse for greater than one. And so this is, I thought, like a neat illustration of kind of when, when machine epsilon is posing a problem and also kind of the difference between depending on machine epsilon for your accuracy and depending on the square root of machine epsilon, which is what classic Gram-Schmidt does. This is, although it's a contrived example, it's nice because we know exactly kind of what the answer should be and we see when each one stops following, following the line. Questions about this example or machine epsilon? And so classic, classic Gram-Schmidt is not, not used in practice for this reason. Okay, so now we're gonna look at another example. And if we were, if we were to do Householder on this graph, it's basically the same accuracy as modified Gram-Schmidt, so it levels off at the same place. So I had it on here at one point, but it's harder to read. But in terms of accuracy, well, for this example, for accuracy, Householder and modified Gram-Schmidt are giving you the same thing. Okay, so the next example we're gonna look at is a matrix, and this is, this is also coming from Lecture 9 of Trebethin. So we have a matrix A that's 0.7, 0.70711 and then 0.70001, 0.70711. And you can probably, just from looking at this matrix, already guess maybe, guess where this, this may be going. So this matrix is close to not having full rank because the second row was almost a repeat of the first row, you know, there's just this difference of one to the negative fifth between them. So we'll use our modified Gram-Schmidt to get Q and R. We can use Householder to get Q and R. And I did this, I guess, both with R version and NumPy's. And then check that all the QR factorizations work. And so here notice that in all cases, we're not actually getting back the original A because this bottom left entry has been, has ended at 0.7, although that is close to what the original A was. But now we can check how close is Q to being perfectly orthonormal. And so here there's no difference between what we're getting from Gram-Schmidt and Householder in terms of reconstructing our A. So looking to get a measure on kind of how close Q is to being orthonormal, we'll take Q, Q times Q transpose minus the identity. If it was perfectly orthonormal, what would Q times Q transpose minus the identity be? I heard someone whisper it, but yeah, zero, because it was perfectly orthonormal, Q times Q transpose would be the identity. Here we're looking at the norm of that. And you'll notice with Gram-Schmidt, it's something to the negative 11th, 10 to the negative 11th. And with Householder, it's 10 to the negative 16th. And so that's kind of an example that Householder got us a Q that was closer to being perfectly orthonormal than Gram-Schmidt did. And I believe Numpy's implementation of QR is using Householder. Linda and Tim has the microphone. So if we go back to the graph, I was wondering what makes it different between the classic and modified this. That's causing that? So the difference lies in the algorithms. And we didn't really talk about what's going on with modified Gram-Schmidt. But modified Gram-Schmidt basically looks like you've just kind of made this algebraic change. Like you're doing the same thing, but you're kind of changing the order of how you do things. But it's something that the operations are compounding, or like the error is compounding differently between them. Even though mathematically, if you're like proving it on paper and pencil, it's like, oh, these things need to be equivalent. Yeah, good question. Yeah, are there questions? I think that might be all I want to say about, well, I'll briefly put classified Gram-Schmidt and modified Gram-Schmidt next to each other just to compare. Oh, you know what it is? Sorry. With modified Gram-Schmidt, yeah, so what I showed you with the picture when I drew the vectors, that was classic Gram-Schmidt, where you're kind of doing these projections once. Modified Gram-Schmidt, it's like you're modifying each value of, like you're getting this whole matrix of projections that you're modifying each time. So we have V, which is what we're kind of copying A into. And so like here with classic Gram-Schmidt, you just need to have a single vector V each time that you're just kind of like, you know, projecting it, and then you subtract off something else, project, subtract off something else. Whereas here, you actually have to have a whole matrix V and like in this, well, yeah, and it affects what your norms are. So basically it's like you're calculating more projections kind of in this more complicated way, like each projection is really made up of several small projections. Matthew, and can you pass the microphone? Oh, this Matthew. Oh, actually I wasn't planning on it, but yeah, we can talk about it. Yeah, and Kelsey also has her hand up, it seems, about this. Let me go back to the QR factorization. And I don't have anything off the top of my head of, yeah, like intuition for that in particular. Okay, and I can, oh, Sam has something and then I will say a little bit about the final exam after that. The diagonal that's produced or close to diagonal will, if we have all real eigenvalues, then it will be, it will merge to a total diagonal, but if there are complex eigenvalues, then there will be extra values in the lower half of the matrix. And looking at the matrix, how are we supposed to know which of the diagonal are the eigenvalues? Is it ones that have no other values, like in the column or in the row then? Yeah, so I'm actually not sure if this is a theorem, like in just all the cases I did, if it like didn't have other stuff beneath it, it was turning out to be an eigenvalue, but I don't know if that's always the case or not. That's a good question. Okay, so yeah, about the, about the final, so that's gonna be all on paper, so no computers for that. And it's, yeah, I've tried to take questions, kind of, many of them are things, yeah, I mean all of them are definitely like related to what we've done in class and in the homework and in the notebooks, and so a mix. So like there are, there are some examples where I like give you code and ask you to modify it, and this will be written, but then there are also, yeah, like conceptual questions. Um, well, what do you mean? Anything about the algorithms? Yes, there are questions, yeah, on the algorithms that we've learned, and also on the specific examples that we've covered in the class. No, yeah, and I tried to stay away from stuff that I thought was like just like annoying memorization, like I wanted it to be more be like do you understand the concepts, and like key points that I thought you should know of maybe kind of like what an algorithm gives you or what an algorithm is used for, but yeah, you definitely do not have to regurgitate the algorithms. Yeah, I've tried to make it not something that's like tricky or overly, like I tried to make it fair, I guess, and like kind of about the like concepts and important parts. Any other questions about it? Okay, now we're on this can end a few minutes early today, and then I will definitely be available through email, and so yeah, feel free to email me if you have questions in the next week. Okay, that's all. Thank you. Thanks.

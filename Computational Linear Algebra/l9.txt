 I'm gonna go ahead and get started. I wanted to make a few announcements. One is that I will not be here next week, so there will be no class on Tuesday, and then Thursday's the exam, and David will be here. David Juminski will be proctoring the exam, but I will have email, so you can still email me questions about anything. Tuesday is when the final draft of the blog post is due, and also I've put up homework three, and it's just a single problem, but that's due Tuesday also. And then I'm not going to have my normal office hours this Friday, but I could meet with you earlier in the day Friday, so if you want to meet on Friday or on Thursday afternoon, just let me know. All right. Yeah, so I was going to follow up with, we had some loose ends from last time for lesson six that I've added, kind of added into the bottom of the notebook. Just to remind you, lesson six was about different ways of calculating the linear regression least squares. And so this is coming from a question Tim asked about kind of full versus reduced factorizations. And I've kind of been skimming over this, but I really are hand waving and not wanting to get into the detail, but it's actually good to look at, and it helped me realize I had an error in the SVD code, which is why it was so slow, and so I'll show you that in a moment. But the idea is there's both a full SVD and a reduced SVD, and they're both referred to as SVD, so it's not always clear. And the idea is that with the full SVD, U is a square matrix. And so for the kind of final, I guess, m minus n columns of U, you're just filling out what's needed for an orthonormal basis, but these are getting multiplied by all zeros. So you've kind of also added several rows all of zero to the base of sigma. So it doesn't, like you want these columns to be such that you're creating an orthonormal basis in U, but beyond that they actually don't have like a connection with your original data set. So that's kind of what's going on with U, and the reduced form is you're just finding kind of the columns that you need to represent A. And so notice both of these, you're getting your original matrix A back, since here you've kind of got these zeroing out. The full SVD would be useful if you needed U to be a kind of square orthonormal matrix. Are there questions about this? Linda? I don't think I get, so if the full SVD is not a square matrix, then how is it a singular, like on the diagonal it shows the importance factor? And if it's not square, how does it show for the last part of it? Okay, yeah, that's a great question. So the diagonal of sigma is what's giving you the importance, and so there's no importance corresponding to these vectors right here. And that's because those vectors aren't even a part of A. So their importance is like kind of like zero, you know, they show up zero amount in A. So if they're not a part of A, where do they come from? And so that comes from just trying to create an orthonormal basis. And so there that's kind of like around the idea of like, oh, if you want you to be square and orthonormal, you kind of need to come up with these additional columns, even though they're not a part of A. That's a good question. Any other questions about this? Tim and Linda, can you pass the microphone back? So in class B, when you did the reduced SVD, the V star didn't have to be square either. It could have been rectangular. So is that because you're dropping some of the non-important singular values out of sigma? Oh, so for the, yeah, and so I should also note that the reduced SVD is different than the truncated SVD. So yeah, for truncated SVD, what you would do, let me see if I can write on the screen. OK, I didn't sync it. The idea with reduced SVD, let's use the mouse, is you're kind of, sorry, truncated SVD is you're cutting off this kind of right hand side here. Or no, you would be cutting off the bottom. Well, yeah, you want to cut off rows of V. Yeah. Yeah. And so then you have to cut off the bottom few rows of V. And so that's when you end up with the rectangular V. And that would also cut off some more columns of U, right? Yes, exactly. And you cut off more columns of U. But in reduced SVD, V star remains square. Yes. Yes. Good question. Are there questions about this? OK, so doing this, I thought to check on SVD, and it turns out the default is to do the full SVD. And so I'll let you guess. Which one of these do you think is faster to calculate? The full or reduced? People are laughing. But yeah, so the reduced is faster to calculate. And so above in my least squares linear regression, I was calculating the full. So let me go back to SVD. So what you need to do is use the full matrices equals false parameter. And so this is here in the least squares SVD. And then that takes out and I should have known this. I was like slicing off the first end. So I should have known like I'm calculating too many. The fact that I'm having to throw these away. So then I reran all the timing. And SVD is a lot more reasonable now. So let me show you. So here, this is this first table. Is this a good size? For the maybe one bigger. No, that's too big to fit. The SVD is still, for most of these, a bit a little bit slower than the others. Here, though, you'll notice SVD is slightly faster than QR in the case that's a thousand by 20. We've got 0.019 compared to 0.018 for SVD. And again, just to kind of summarize, this is a bunch of different methods for finding the least squares linear regression being applied to the same set of matrices that were randomly generated. And we went through in a loop and randomly generated matrices of different sizes, solved the problem. It's five different ways and saw how the time and the error compared. Let's see if anything else stands out as particularly noteworthy. Yeah, so this puts SVD in a much, much more reasonable speed range. Are there questions or kind of general questions on this comparison? We're about to go into some specific examples where we get worse error rates for some of the algorithms. So overall, what seems to be the fastest? This might depend on the size. Can you pass the microphone forward? Did you say it, Linda? Yeah, Telesky is the fastest for most of them. Correct. Any other observations about this? OK, let's go back. Oh, and then while we're talking about full versus reduced, I wanted to say that QR also has full and reduced versions. And so it's a very similar idea. If you've got a rectangular A and the reduced version, you're just getting a rectangular Q and a square R, which is upper, upper triangular. And then for the full version, you're adding additional columns on to Q to make an orthonormal basis. And then you're adding rows of zeros to R. So those are going to cancel out. So these columns of Q are not actually part of A. But they're nice if you wanted a square matrix that was orthonormal. OK, so I wanted to address, so kind of comparing these algorithms, if we just look at speed, Telesky seems really good in a lot, and then also taking the matrix inverse seemed to be pretty fast. And I think we were getting that it had the same error as everything else. Let me confirm. Yeah, we were getting the same error as everything else. But in practice, you never want to do this. And so I wanted to talk more about why that is. And that's because matrix inversion is unstable. And so here we're going to look at a specific example called a Hilbert matrix. And let me actually start by doing a smaller one just so you can see what it looks like. So the Hilbert matrix is basically kind of these fractions. You can see you've got one, a half, a third, a fourth, a fifth is how the matrix is constructed. And it's known to have a poor condition number. So here I'm just using NumPy's lin-alg.inverse method. So I've created this Hilbert matrix that's 14 by 14. Run that again. I invert it. And then ideally, A times A inverse minus the identity. What would you expect that to give you? Say it louder. Zero. Yes. So you'd expect it to give you zero. I take the norm of that and I'm getting five, which is a bad sign. And so let's see. And we can even look at A times A inverse here. And you can see this is not very close to zero. Many of these decimals have numbers kind of in the tenths place. This even has a negative. Oh, wow. This even has a two. Yeah. So this is quite far from zero. So I think this is kind of a nice illustration and this is not even a huge matrix and it's 14 by 14. And we're using NumPy's inverse and we are not getting back to the identity with that. And we can also check. So I mentioned last time there's something called the condition number of a matrix. And actually, does anyone remember what the formula for the condition number is? OK, I'll pull that up. It's the norm of A times the norm of A inverse. And larger condition numbers are bad. So you want the condition number to be smaller. So that's kind of the formal definition in general. But for a matrix, here it is. Norm of A times norm of A inverse. And it's something that it shows up all the time in kind of different theorems around conditioning. And so it's useful to have it as a quantity. And Linda, can you pass the microphone to Tim? Is that defined only for square matrices? Is that only defined for square matrices? Yeah, I'll look that up and get back to you because there is something called the pseudo inverse, but I don't know if that's... I think you're typically talking about square matrices with it. But yeah, that's a good question. Yeah, and so if we look at the condition number of A, it's 10 to the 17th magnitude, which is a really bad sign that it's so large. And so that's the primary reason that you don't want to calculate a matrix inverse. And there are a few other issues, I thought I wrote them down, that can come up. One is if you have a sparse matrix in particular, when you calculate the inverse, it stops being sparse. And so memory wise, that could be really bad, particularly if you had a matrix that was large enough that you can't store the non sparse version. Calculating its inverse is kind of making this really huge matrix. But I think the primary thing is the instability. So this is a little bit of a cheat. Let me confirm. So I'm still on this Hilbert matrix. And so I run all of the options. Well, you'll notice I run all the options except Cholesky. And so we didn't focus on this last time. So this is a difficult question, but does anyone have a guess of why, maybe why we can't use Cholesky here? Okay, I just. Oh, Matthew. And can you pass the microphone? Because it's unstable like you said. No, so that's matrix inversion is unstable. It doesn't require the matrix inversion. No. So, yeah, Cholesky entails coming up with the, you're coming up with a triangular matrix, which times its transpose equals A transpose A. This I just briefly mentioned. The Cholesky factorization is only guaranteed to exist if the matrix is positive definite. And so this matrix actually is not positive definite. So that's a down, significant downside to Cholesky is that even though it's really fast, you can't use it in all cases. And so we can't use it here. So I had to take it out. I mean, you can try running it and you get an error basically that there's a negative singular value. If you attempt to do it. So here we've got this huge error for. I don't know why I have this twice. Yeah, we've got this big error for taking the inverse, which is not surprising. Speed wise, so QR factorization, SVD and SciPy's implementation are all fairly similar, but SVD was actually fastest. Although I have to confess, I ran this a few times and I don't think that was consistently the case, but I wanted to save it because I was having trouble finding a case where SVD was the best. But here, really, I mean, they're so close that there's not like a clear winner. But yeah, this illustrates kind of you definitely don't want to be taking the inverse and you can't take Cholesky. Oh, and can you throw the microphone, Matthew? So any reason that after inversion that the matrix is going to just get more dense? I guess it's like, I mean, there's nothing that guarantees that you would have zeros in other places. You kind of think like the process for taking inversion is it's like doing Gaussian elimination. Only instead of having like a single column, you have like the identity matrix is one way to calculate the inverse. And so there you're kind of basically like adding and dividing things to every location as you go through and zero out your original matrix. But yes, you're kind of like putting values in where you had zeros when you do that Gaussian elimination. Yes, I want to talk a little bit about runtime, and this is not exactly big O because I've included constants here and, you know, with big O notation you are... I mean, I don't even know if it's OK for me to call this runtime, but this is kind of normally you drop the constants. But I just wanted to show them because here most things are n cubed, and so it kind of matters what the constant is. So Cholesky is one third n cubed. And so that's a reason that it's kind of faster than these others. So QR is kind of a formula involving powers of third order terms. And then something else to highlight is that matrix multiplication is n cubed and solving a triangular system is n squared. And so if you'll remember, this kind of goes through with the Cholesky factorization, what you end up with is... actually maybe I'll go back up to how I had it written. OK, here's the Cholesky factorization. So here our process was we take A transpose A, X equals A transpose B. So we are having to do a matrix multiplication to get this A transpose A. Then we do the Cholesky factorization and get R transpose R, where R is upper triangular. And then we're solving R transpose W equals the right hand side. And the reason that's nice is that's a triangular system because R is triangular. And how can you solve a triangular system? Anyone I haven't heard from today? OK, yeah. Can you throw the microphone to Vincent? Yes, back substitution. Exactly. So bottom row, you only have a single entry. Actually, this is R transpose, so really it would be the top row in this case. So you can just divide through and get that value, plug that into the next equation, which only has two variables, plug it in, solve for the other one, and so on. And so that's n squared. So that's a quicker way of doing this. Whereas with the kind of naive approach, even when you get, let me find it. Even once you get this inverse, you still have to do a matrix multiplication. So you're doing A transpose A, calculating the inverse, which takes n cubed. And then you're multiplying that by A transpose, which is another matrix multiplication. So that's slow. So really, kind of what I wanted to highlight here is that matrix multiplication is n cubed and solving a triangular system is n squared. Triangular system is n squared. So yeah, and I found these from slides. This was actually from the convex optimization course I linked to a few notebooks ago. They had like a numerical linear algebra background lesson. And so this is just showing why Cholesky's fast. So here I'm looking at another very specific type of matrix called the Vandermonde matrix. And this is coming from an example in Trevathan. And so Trevathan tells us that kind of if we normalize by this, he's calculated the kind of true solution on a very high precision computer and has that it should be one. And here, so we're creating this matrix by taking powers of t. So we have kind of evenly spaced points between zero and one. And then we're taking different powers of them. And then doing e to the sine of four times that. And so we can check that using QR, we're getting 10 to the negative seventh, which isn't perfect, but is reasonable. This is another condition number of to the power of 17, which is bad news. So we run it for the four. And again, Cholesky doesn't work on this one because it's not positive definite. And then here we'll see again, there's a big error for the naive approach where we take the inverse. And then time wise, QR is a bit a bit faster. Well, really, the naive approach is the fastest, but just very wrong. But then QR factorization is the fastest of the ones that are correct. Any questions? OK. And then one more example, and this is a little bit redundant. This is with a low rank matrix. So really, I've just kind of created a set of rows and then I've stacked them on top of each other. So there's a huge amount of redundancy in this matrix. And running that, we get a large amount of error for the naive solution. And then we do have more error than we've had previously for the others. And SVD is noticeably worse than QR in this case, although SVD is quicker. Yeah, so I guess kind of in summary, Cholesky is typically fastest when it works, but Cholesky can only be used on symmetric positive definite matrices. And then also Cholesky is, well, I guess we weren't really able to do it, but it's unstable for matrices with high condition numbers or low rank. So linear regression via QR has been recommended by numerical analysts as the standard method for years. Trevathan describes it as good for daily use. And Trevathan says that there are problems where SVD is more stable, although I was not able to recreate that. But my hope with this lesson was to kind of get to see what's going on underneath the hood with linear regression and least squares, and also to see that there are different approaches for it and depending on your problem, there might be some solution or some approaches better than others. Questions? OK, in that case, we're going to start on notebook 7. Page rank with eigendecompositions. So new start, new topic. I wanted to introduce two tools that are just useful in general, and we're going to use them today, but you can use them for a lot of problems. One is a library called psutil, which lets you check your memory usage. And here we're using a larger data set than we've used before, so this can be nice to have. So you'd have to pip install this and then import it. Here we're kind of getting the process ID, the process memory info. And then I've written a helper method, memory usage, that just returns the... So RSS memory stands for resident set size memory, and so we're kind of just returning that memory divided by the total memory to get the memory usage. So this could be a nice thing to monitor if you're having trouble with running out of memory. And then tqdm is a kind of nice library that gives you progress bars, and so this can be good if you are running a for loop with things that are slow. It gives you like a more visual appearance of how you're going, and so this is just a simple example. I'm using sleep to make this a slow for loop. I'm running it and I kind of don't see anything until it's finished. To use tqdm, you just wrap that around whatever you're iterating over. So here we've wrapped range 10 with tqdm. Now running the exact same loop, you see we get this progress bar that's updating as you go. So to get started, can someone remind us what the SVD is? Matthew and Vincent, can you throw the mic? Yes, singular value decomposition. And what does it give you? Yes. Right, yeah, and that's actually kind of getting to the next question of what are some applications of SVD. So yeah, topic modeling was one of them. Yeah, so you're getting U, which is... So in the full version, U and V are both orthonormal square matrices. In the reduced version, U has orthonormal columns. Yeah, so Matthew got us started. What are some other applications of SVD? Roger? PCA? Yes. Yeah, so PCA is SVD. And what are we doing to get the low rank approximation? Exactly, yeah. So this is also called truncated SVD, where we kind of drop off part of them and just take the first few. Exactly. Other applications? There's one we spent an entire week on. Yeah, background removal. So we got pretty good results just using SVD, and then we used the robust PCA, our primary component pursuit, and that used SVD as a step and kind of iteratively did multiple SVDs. And then just now we saw SVD as a way to calculate the least squares linear regression. Yeah, so hopefully you're convinced that SVD is useful and shows up lots of places. I'm kind of a little bit off topic, but I'm doing a workshop this afternoon on word embeddings, such as Word2Vec, which is like a library of word embeddings. And so I was looking at the problem of bias in Word2Vec. So word embeddings give you these analogies, which can be really great, like Spain is to Madrid, as Italy is to Rome, but they also have been found to have kind of biased analogies, such as father is to doctor, as mother is to nurse, or man is to computer programmer, as woman is to homemaker. And so there's been work done around how can you kind of remove this bias. And so I was reading a paper on it and they used SVD as part of their de-biasing process. So I thought that was pretty neat. I was like, oh, this is a relevant application of SVD. So just wanted to highlight that. So a few different ways to think about SVD are data compression. So this comes up with the PCA or when you're dropping many of your kind of lower singular values, the less informative parts, you're getting something that fits in a smaller space. Another related way to think about it is SVD trades a large number of features for a smaller set of better features. And I think that's kind of what we saw with topic modeling. You could have a feature for each word, or we could get the smaller set of features that are kind of groups of words that have related topics. And then I think this is kind of neat, but it's all matrices are actually diagonal if you change the bases on the domain and range. And so this might be a good time. I was going to show the three blue, one brown change of basis video. Let me do that now. Can everyone hear or does that need to be louder? From its tail to its tip involves moving three units to the right and two units up. If I have a vector sitting here in 2D space, we have a standard way to describe it with coordinates. In this case, the vector has coordinates 3, 2, which means going from its tail to its tip involves moving three units to the right and two units up. Now, the more linear algebra-oriented way to describe coordinates is to think of each of these numbers as a scalar, a thing that stretches or squishes vectors. You think of that first coordinate as scaling i hat, the vector with length 1 pointing to the right, while the second coordinate scales j hat, the vector with length 1 pointing straight up. The tip-to-tail sum of those two scaled vectors is what the coordinates are meant to describe. You can think of these two special vectors as encapsulating all of the implicit assumptions of our coordinate system. The fact that the first number indicates rightward motion, that the second one indicates upward motion, exactly how far a unit of distance is, all of that is tied up in the choice of i hat and j hat as the vectors which are scalar coordinates are meant to actually scale. Any way to translate between vectors and set of numbers is called a coordinate system, and the two special vectors, i hat and j hat, are called the basis vectors of our standard coordinate system. What I'd like to talk about here is the idea of using a different set of basis vectors. For example, let's say you have a friend, Jennifer, who uses a different set of basis vectors, which I'll call b1 and b2. Her first basis vector, b1, points up and to the right a little bit, and her second vector, b2, points left and up. Now take another look at that vector that I showed earlier, the one that you and I would describe using the coordinates 3,2, using our basis vectors i hat and j hat. Jennifer would actually describe this vector with the coordinates 5 thirds and 1 third. What this means is that the particular way to get to that vector using her two basis vectors is to scale b1 by 5 thirds, scale b2 by 1 third, then add them both together. In a little bit, I'll show you how you could have figured out those two numbers, 5 thirds and 1 third. In general, whenever Jennifer uses coordinates to describe a vector, she thinks of her first coordinate as scaling b1, the second coordinate as scaling b2, and she adds the results. What she gets will typically be completely different from the vector that you and I would think of as having those coordinates. To be a little more precise about the setup here, her first basis vector, b1, is something that we would describe with the coordinates 2,1, and her second basis vector, b2, is something that we would describe as negative 1,1. But it's important to realize from her perspective in her system, those vectors have coordinates 1,0 and 0,1. They are what define the meaning of the coordinates 1,0 and 0,1 in her world. So, in effect, we're speaking different languages. We're all looking at the same vectors in space, but Jennifer uses different words and numbers to describe them. Let me say a quick word about how I'm representing things here. When I animate 2D space, I typically use this square grid. But that grid is just a construct, a way to visualize our coordinate system, and so it depends on our choice of basis. Space itself has no intrinsic grid. Jennifer might draw her own grid, which would be an equally made up construct meant as nothing more than a visual tool to help follow the meaning of her coordinates. Her origin, though, would actually line up with ours, since everybody agrees on what the coordinates 0,0 should mean. It's the thing that you get when you scale any vector by 0. But the direction of her axes and the spacing of her grid lines will be different, depending on her choice of basis vectors. So, after all this is set up, a pretty natural question to ask is how we translate between coordinate systems. If, for example, Jennifer describes a vector with coordinates negative 1,2, what would that be in our coordinate system? How do you translate from her language to ours? Well, what our coordinates are saying is that this vector is negative 1 times b1 plus 2 times b2. And from our perspective, b1 has coordinates 2,1, and b2 has coordinates negative 1,1. So we can actually compute negative 1 times b1 plus 2 times b2 as they're represented in our coordinate system. And working this out, you get a vector with coordinates negative 4,1. So that's how we would describe the vector that she thinks of as negative 1,2. This process here of scaling each of her basis vectors by the corresponding coordinates of some vector, then adding them together, might feel somewhat familiar. It's matrix-vector multiplication, with a matrix whose columns represent Jennifer's basis vectors in our language. In fact, once you understand matrix-vector multiplication as applying a certain linear transformation, say by watching what I view to be the most important video in this series, chapter 3, there's a pretty intuitive way to think about what's going on here. A matrix whose columns represent Jennifer's basis vectors can be thought of as a transformation that moves our basis vectors, i-hat and j-hat, the things we think of when we say 1,0 and 0,1, to Jennifer's basis vectors, the things she thinks of when she says 1,0 and 0,1. To show how this works, let's walk through what it would mean to take the vector that we think of as having coordinates negative 1,2 and applying that transformation. Before the linear transformation, we're thinking of this vector as a certain linear combination of our basis vectors, negative 1 times i-hat plus 2 times j-hat. And the key feature of a linear transformation is that the resulting vector will be that same linear combination but of the new basis vectors, negative 1 times the place where i-hat lands plus 2 times the place where j-hat lands. So what this matrix does is transform our misconception of what Jennifer means into the actual vector that she's referring to. I remember that when I was first learning this, it always felt kind of backwards to me. Geometrically, this matrix transforms our grid into Jennifer's grid, but numerically, it's translating a vector described in her language to our language. What made it finally click for me was thinking about how it takes our misconception of what Jennifer means, the vector we get using the same coordinates but in our system, then transforms it into the vector that she really meant. What about going the other way around? In the example I used earlier this video, when I had the vector with coordinates 3,2 in our system, how did I compute that it would have coordinates 5 thirds and 1 third in Jennifer's system? You start with that change of basis matrix that translates Jennifer's language into ours, then you take its inverse. Remember, the inverse of a transformation is a new transformation that corresponds to playing that first one backwards. In practice, especially when you're working in more than two dimensions, you'd use a computer to compute the matrix that actually represents this inverse. In this case, the inverse of the change of basis matrix that has Jennifer's basis as its columns ends up working out to have columns 1 third, negative 1 third, and 1 third, 2 thirds. So for example, to see what the vector 3,2 looks like in Jennifer's system, we multiply this inverse change of basis matrix by the vector 3,2, which works out to be 5 thirds, 1 third. So that, in a nutshell, is how to translate the description of individual vectors back and forth between coordinate systems. The matrix whose columns represent Jennifer's basis vectors but written in our coordinates translates vectors from her language into our language. And the inverse matrix does the opposite. But vectors aren't the only thing that we describe using coordinates. For this next part, it's important that you're all comfortable representing transformations with matrices and that you know how matrix multiplication corresponds to composing successive transformations. Definitely pause and take a look at chapters 3 and 4 if any of that feels uneasy. Consider some linear transformation, like a 90-degree counterclockwise rotation. When you and I represent this with a matrix, we follow where the basis vectors i-hat and j-hat each go. i-hat ends up at the spot with coordinates 0,1, and j-hat ends up at the spot with coordinates negative 1,0. So those coordinates become the columns of our matrix. But this representation is heavily tied up in our choice of basis vectors, from the fact that we're following i-hat and j-hat in the first place to the fact that we're recording their landing spots in our own coordinate system. How would Jennifer describe this same 90-degree rotation of space? You might be tempted to just translate the columns of our rotation matrix into Jennifer's language, but that's not quite right. Those columns represent where our basis vectors i-hat and j-hat go, but the matrix that Jennifer wants should represent where her basis vectors land, and it needs to describe those landing spots in her language. Here's a common way to think of how this is done. Start with any vector written in Jennifer's language. Rather than trying to follow what happens to it in terms of her language, first we're going to translate it into our language using the change of basis matrix, the one whose columns represent her basis vectors in our language. This gives us the same vector, but now written in our language. Then, apply the transformation matrix to what you get by multiplying it on the left. This tells us where that vector lands, but still in our language. So as a last step, apply the inverse change of basis matrix, multiplied on the left as usual, to get the transformed vector, but now in Jennifer's language. Since we can do this with any vector written in her language, first applying the change of basis, then the transformation, then the inverse change of basis, that composition of three matrices gives us the transformation matrix in Jennifer's language. It takes in a vector of her language and spits out the transformed version of that vector in her language. For this specific example, when Jennifer's basis vectors look like 2, 1 and negative 1, 1 in our language, and when the transformation is a 90 degree rotation, the product of these three matrices, if you work through it, has columns one-third, five-thirds, and negative two-thirds, negative one-third. So if Jennifer multiplies that matrix by the coordinates of a vector in her system, it will return the 90 degree rotated version of that vector expressed in her coordinate system. In general, whenever you see an expression like A inverse times M times A, it suggests a mathematical sort of empathy. That middle matrix represents a transformation of some kind as you see it, and the outer two matrices represent the empathy, the shift in perspective. And the full matrix product represents that same transformation, but as someone else sees it. For those of you wondering why we care about alternate coordinate systems, the next video on eigenvectors and eigenvalues will give a really important example of this. See you then. Alright, so I really like that analogy of change of basis being like translating between languages. And the idea of kind of like empathy with multiplying by change of basis and inverse to change the basis back. And so we'll be using those ideas a bit. We're not going to watch the eigen decomposition video in here, but you might want to watch that at home if you're interested. I think this is a good time to take a break. It's 1155, so let's meet back in eight minutes, like at 1203. And if you have questions about the video, maybe write them down and then we can talk about them when we meet back. Thanks. So yeah, we just saw the video on change of basis. Yeah, I really like the analogy about translating, translating between languages as alike that it showed. Again, we've talked about how you can think of matrix multiplication is taking a linear combination of the columns. And that's what was going on with kind of the original basis. We're going to start with the Jennifer's basis and then we're taking coefficients to translate into that. Were there any questions about the video? Okay. So highlight. So this is coming back. We had this statement. All matrices are diagonal if you use change of bases. And so that's kind of what's going on with this U sigma V. We can think of that as a change of basis to get into a space where you've got a diagonal matrix. And this, I regret that I didn't put a picture of this in here, maybe I'll do this next time. A lot of times with PCA this shows up, you'll kind of have the picture of, you know, like what the principal axes are. And you can think of that as kind of the basis vectors of this new basis that better represents your data set. Yeah, so we've been talking about SVD in terms of matrices. But we can also think of it in terms of the individual vectors. And so if we think of SVD as giving us the vectors Vj and Uj, then we could have A times vector V equals sigma times vector U. And I kind of am showing the answers here, but my question was going to be does this remind you of anything? And the answer is eigendecomposition. And just to remind you, if it's been a while since you've seen eigendecomposition, if I define it here. Does anyone remember what the definition of an eigenvector, an eigenvalue is? Brad, and can Roger throw the microphone? For a given matrix A, an eigenvector is a vector that when transformed by A is just a scaled, that vector is a scaled by a value lambda, which is the eigenvalue. Exactly. So transforming V by A, which is like doing A times V is just scaling V. So it's equal to lambda times V. And so that's the definition of eigenvectors and eigenvalues. And so hopefully this looks somewhat similar to what we were talking about with A times V equals sigma times U. Kind of the key difference here is that V and U don't have to be equal. So those could be different values. But otherwise we're talking about when is matrix multiplication by a vector like scaling a vector? So going back here, and I won't go into great detail about it, but SVD is a generalization of eigendecompositions. So not all matrices have eigenvalues, but all matrices have singular values. And we could probably guess that SVD was more general by the fact that U and V don't have to be the same vector. So we're going to switch from talking about SVD to talking about how to find the eigenvalues, which is a kind of very closely related problem. So everything we say about eigendecomposition, remember that it's relevant to SVD. And we've just seen this list of applications of SVD kind of throughout this course. And here I link to several other kind of resources for more information about SVD. So the best classical methods for computing the SVD are all variants on methods for computing eigenvalues. And eigendecompositions are useful on their own as well. So a few practical applications of eigendecompositions. One is rapid matrix powers. Rapid matrix powers. And so the idea is that if you knew that A was equal to V times lambda times V inverse, here V is the eigenvectors, lambda is the eigenvalues. What would be true of lambda as a matrix? What property does this matrix have? Do you want to grab the microphone? Exactly, yeah. Lambda has to be diagonal. And that's kind of this idea of going back and forth between, you know, we can write eigenvectors and eigenvalues individually as vectors, you know, kind of what I had here. Or we could kind of put all the eigenvectors together and say, really this is A times a matrix V where the columns are the different eigenvectors equals, oh, accept. To make that work, you have to make lambda into a matrix. And what that matrix is, is just diagonal with the little lambdas along the diagonal. Questions about getting between these. And so here this would, often these would have a subscript. So if I say these are all sub i, then we're putting those together. Actually, let me write it out. I'm going to say everyone's been very quiet today, so it's hard for me to read. So if you want me to go slower, quicker. But you could think of this as A times V1, V2, V3, and so on. Equals lambda one, lambda two, one down, slide over and down to lambda n. Times V1, V2, up to Vn. This is kind of a way to consolidate all those individual equations for the different Vi and the different lambda i into matrices. And then the idea with the rapid matrix powers is, you know, suppose we're interested in taking A to some power k. Doing that the naive way would be doing A times A, multiplying that by A, multiplying that by A. And what's the run time for matrix multiplication? Tim, do you want to throw the microphone to Sam? Yes, exactly. It's n cubed, which is really slow. So another way we could think about this. So now we've got A equals. Did I write this backwards? I'm sorry about this. AV and then this side is V lambda. Yeah, I think that's correct. Thank you. Yeah, and the reason why that's true is because we want to be taking multiples of... Yeah, we're interested in the columns V on this side. And so writing it like this. What this gives us is basically taking the first column, multiplying that by our matrix V is just going to give us lambda one, B one. Then we're doing lambda two times B two and so on. Sorry about that confusion. This should be AV times V lambda. Which then can be rewritten as A equals V lambda V inverse. So then going back to our problem of wanting the kth power, we can do A equals V lambda V inverse to the kth power. Any ideas about how this could be reduced? Tim and Sam, can you throw the microphone back to Tim? Exactly. So if you were to write these out a bunch of times, as Tim said, you end up with V lambda V inverse times V lambda V inverse times V lambda and so on. And this V inverse times V and V inverse times V are going to cancel out however many entries you have and you end up with V lambda to the k V inverse. And why is it okay that we have lambda to the k? Matthew? And do you want to throw the? It's diagonal so it's really easy to take the kth power. Exactly, yeah. So since it's diagonal, taking the kth power is not a big deal at all. So this is a much more efficient way to compute the kth power of A. This is one application of eigenvectors. You can also use them to find the nth Fibonacci number, which is kind of neat. I'll just briefly show this one. But basically, you can think of getting the Fibonacci numbers is going taking this matrix 1, 1 and 1, 0 and multiplying that by 1, 0 and then kind of continuing to do that to get the, you know, this is giving you on the top row the sum of the previous two things you have. If you enter the previous two here and then apply what we just saw with taking nth powers. It's kind of a fun application. And then we're not going to get into this in this class, but the behavior of ODEs is often if you're interested in the long term behavior of ODEs, what you'll end up needing to do is finding the eigendecomposition and then Markov chains, which we will kind of be seeing today. Yeah, so we watched the three blue, one brown video. Gilbert Strang, who's written a kind of classic linear algebra textbook, had a nice quote, eigenvalues are a way to see into the heart of a matrix. All the difficulties of matrices are swept away, but there's something, you know, really a kind of fundamental about a matrix that's expressed in its eigenvalues. And then just some vocabulary that you might come across is Hermitian, and that means the matrix is equal to its conjugate transpose. In the case where you're dealing with real values, that's just saying that it's the same as being symmetric, it's equal to its transpose. This is only if you have complex values, then you'd be flipping the sign on the complex values. But so for our purposes, when you hear Hermitian, you can think symmetric. And then two useful theorems are if A is symmetric, then the eigenvalues of A are real and A equals Q lambda Q transpose. So that's really handy that here Q is, its inverse is its transpose. And then if A is triangular, its eigenvalues are equal to its diagonal entries. This is a little bit of a spoiler, but if you are interested in the eigenvalues of a matrix, it would probably be nice to have it in triangular form. Because then you can just get them from the diagonal. So today we're going to start with the power method, which finds just one eigenvector. And that's the basis for page rank. And there's a paper, I really like the title of this, the $25 billion eigenvector, the linear algebra behind Google. Yeah, so this is a kind of very real world application of an eigenvector being important. And so we're going to be using a data set from DBpedia, which I think is a really neat resource. But they've kind of compiled a bunch of Wikipedia data, as well as a lot of different kind of classifications and categories of it. And they have it in a bunch of different languages, and it's all kind of freely available. And so the data set we'll be using is kind of showing which pages link to which other pages. And so we'll be doing, so kind of the basic idea behind the original page rank algorithm for Google search was that pages that a bunch of other pages link to must be more important. Kind of that so many people are linking to that page. And prior to that, like the very early kind of like Yahoo pages had editors that like hand went out and selected links to be listed. And so this was kind of a big change to have an algorithm do it and kind of tell you how important different pages are. And so what we'll be doing is using this Wikipedia data of what pages are linking to what pages to kind of see what the most important pages are based on links. And I guess the other piece of that is you're normalizing for, you know, if a page links to 100 other pages, it's less special that they're kind of linking to each of those pages than if a page only links to two other pages. And that kind of carries more weight. So yeah, here, this is just a little information about the full DBP data set has 38 million labels, abstracts in 125 different languages, 25 million links to images. So this is, I think, something to keep in mind for future projects. And so I sent a Slack message this weekend, this can be kind of slow to download, so you might want to wait till after class if you haven't done it already. So here, this is just kind of opening the data. So what we're going to do is construct a graph adjacency matrix about which pages point to which. And so this is a kind of very simple example of if you just had four pages, if A is pointing to B, C, and D, we could represent that here in the first column. This is a little bit confusing. A is a matrix here, up here it's the first node. We're putting ones to indicate the first node points to the second, third, and fourth nodes. Then B only points to C. So the second node is only pointing to the third node. You can see in the second row of this matrix that there's just a one in the third spot, but nowhere else. So that one represents B's pointing to C. Down here, C is just pointing to A. So you can see that we have a one in the first spot saying C is pointing to A, C doesn't point to anything else. And then D just points to C. So in the third spot, we've got a one. So this is a directed graph, so the order definitely matters. It's not symmetric. Are there questions about this representation? Okay, so taking the power A squared will tell you how many ways there are to get from one page to another page in two steps. And actually, I found these notes, which I liked, and they use the example of airlines traveling. So they just have this kind of smaller graph, I guess in Massachusetts, between, you know, which towns can you fly between. They represent that as a matrix with these zeros and ones. And then they kind of take the power and say, okay, by taking the second power, we see there's one two-step sequence from C to F. So that shows up in A, B, C. We've got a one here. There's one way to just with two legs of your journey to get from C to F. There are five three-step sequences between C and F. So coming over here, yeah, so C is the third row, F is the sixth column, and we've got five. This is for M cubed. Other questions about this? This would show up in logistics problems. All right, so the format of our data is it's kind of in files in these lists, and so we have both the redirects and the links, and we need to use both. And the redirects are just to kind of figure out which page is redirecting to other ones. And so the first one is kind of the source page that it's telling you it's a redirect or a link, and then the third argument is telling you the destination page it's pointing to. So this is just some kind of data processing to read in the lines, to split them. The redirects we're putting into this dictionary. I want to kind of come down, I think it's more interesting to kind of look at what we've created. So we have something called index map, and I've just, an index map is a dictionary, but to see what's in it, I'm just kind of popping off a random element from it. And in general, you don't want to do this because I think that that alters your dictionary, but I just wanted to kind of, it's always important to be able to see what your data is like. And I think it's more informative to see how our process data looks than to go through each step of the processing. But here it says 1940 Cincinnati Redstein issue relates to this index 9991173. So then we've got two lists of source and destination, and those are just, they're listed the same length, and their list of indices, and so they're kind of telling you, you know, the seventh spot in source is the index of which page is a source pointing to the seventh spot in destination. So this is a very inefficient way to be working with these lists, and I'm doing this to kind of illustrate what they represent, but this isn't how you would actually be using them. So okay, I know Cincinnati Redstein issue is 9991173, where does that even show up in the source list? It just shows up at index 119077649. And so then I can look that up in the destination, and I get 9991050. So we have, I guess, two different types of indices going on. It's the indices being used by the index map, and then the indices of source and destination, which just correspond to each other. All this to say, then we look up 9991050 and find out that that is page W711-2. I just want to say, I actually tried popping like several elements off to see if I could find something more, maybe more interesting, but some of them were like for pages that have been deleted because I think the DBpedia data set is not super current. Wikipedia gets changed a lot, so this is actually the best I could do. I went to Wikipedia and I looked up Cincinnati Redstein issue, and it redirected me to W711-2. So that's showing that our data is representing what we want, and we can get information from it. Although it does involve this kind of, you know, like having to search for where does the index appear in source, and then what does that correspond to in destination. And this is all talking about a pack of baseball cards, if you are wondering. And we can even open this link in a new tab. And it says, you know, W711-2 is also known as the 1940 Cincinnati Reds team issue, which is a baseball card set. And we'll take a look at it just to see some of the things that show up about baseball, Cincinnati Reds, Detroit Tigers, and then there are a bunch of names of players in here. So let me just say that kind of one more time how this is working. Index map is a mapping between names of Wikipedia pages and a number representing them, and then we can look for those numbers in the source or destination list. And then the source and destination lists kind of need to be paired together because things show up, you know, like the third entry of the source list is pointing to the third entry of the destination list. So are there questions about this? Okay, so now I'm interested in, so 9991050 is this W711-2 page, which represents this pack of baseball cards, and I want to know where that shows up in the source. So kind of what pages is the source pointing at? And I get back, let me check if this is, I get 47, so there are a lot that's saying that this Wikipedia page has 47 outgoing links on it, which seems very plausible to me. And then now I look those up in my index map to see what pages they correspond to. And so that's what I have here, and they correspond to baseball, Ohio, Cincinnati, Flash Thompson, 1940, 1938, Lonnie Frey, Cincinnati Reds, Ernie Lombardi. So this seems correct, like our data is what we think it is. We've got the information that this W7 page is linking to all these other baseball related pages and names of the players. I just had a screen capture of that. So any questions about that, kind of that aspect of the data processing? Okay, so then now we're going to use sparse.coo to create a sparse matrix, and then we're going to convert it to CSR. So two questions. First, what are COO and CSR? And Matthew has the microphone if someone wants to call C. I think COO is the coordinate representation. Exactly. So these are two different sparse representations. And so why would we create it with COO and then convert it in the next line? Kelsey? Exactly, yeah. So COO I think kind of makes the most sense logically, because with CSR we have to do that kind of counting of, we keep track of a row pointer. And so how many values are in each row, when do we need to update our row pointer, which would be kind of a pain to do by hand. And there's efficient conversion between the different types. But COO, it's very natural to say this is our data and we want to do it by destination comma source. So this is our rows are going to be the destinations, the columns are the sources. Put that into a sparse matrix and then convert it to CSR. And I wanted to highlight this. So this is the page that we've seen several times explaining kind of about the different sparse formats. And it points out over here at the bottom, advantage of CSR method over the coordinate wise method is that, so the number of operations to perform matrix vector multiplication are the same for the two. However, the number of memory accesses is reduced by a factor of two in the CSR method. So that's, and this is a nice example of, you know, if you were just thinking about big O, it's going to be the same in terms of operations. However, this idea of memory access being something that matters and can be, can slow us down, CSR is a lot better. Questions about that? Okay, and so we'll be doing, not surprisingly, some matrix vector multiplications and so it's going to be faster to use CSR and that will have fewer memory accesses. And then I highly recommend if you're doing this at home, all this processing and creating your matrix can be a bit slow. And so when you get to this point, it's a good idea to save your matrix and you can use Pickle, which is a Python library for kind of compressing things. And then that way if you want to come back to this later, you're not having to recreate your matrix every single time you're using this notebook. This is something I recommend in general for any sort of data project where it's slow to compute the data. So here we save our index map and our data matrix X. And we can check here, X is a, this is quite a large matrix, about close to 12 million by 12 million sparse matrix. And then it's nice that it tells us this. So it was, was I asked by a fact or? Oh, okay. Yeah. Thank you. 120 million by 120 million matrix. And then it's got 94 million non-zero entries. Which is a lot less than 120 million squared. So we're saving a lot of space. Any questions about the setup? Okay, so now we're going into the power method. This is going to be how we're calculating our eigenvector. And again, our idea is that if, you know, if we were just looking at Wikipedia as kind of like a stand in for the Internet, we're trying to find like what the most important pages are. Actually, I'm going to skip this part. Well, so this is similar to what we talked about below with the matrix powers. But the idea is that a matrix, so a matrix is diagonalizable if it has n linearly independent eigenvectors, v1 through vn. And then any, any vector can be expressed as a linear combination of the eigenvectors. And that's really handy because when we're looking at the matrix times a vector, having how it's expressed as an eigenvector, or expressed in terms of the eigenvectors, lets us just multiply them by the eigenvalue instead of having to do a matrix multiplication. And let me even say I want to I want to write that one out. So let me start on a new page. So if any vector w can be written as a linear combination for some scalars, call them Cj, of the eigenvectors vj, and that's because the v form, the eigenvectors form a basis for your space. And when you want to do a w, it's just like A, times this linear combination. You can pull this A inside. Remember C is just a scalar, so we can pull the matrix into that. And then how can I rewrite this? The hint is that the vectors v are eigenvectors of A. So can we just substitute A vj with lambda j vj? Exactly. Yes. And this is just kind of the definition of what it means to be an eigenvector, is that instead of having to multiply by a matrix, you can just multiply by a scalar. And so this is really nice because there is nothing special about w here. W was any vector and it was being represented as a linear combination of the eigenvectors. And so we're kind of saying, oh, for any vector, you don't actually have to do the matrix multiplication. You can just use the eigenvalues, kind of multiplying by the basis. Questions about this? And so something to keep in mind is that if you were taking powers of A, this, and I'm not going to write it all out, but this basically becomes the eigenvalue to a power. So that's significant what the eigenvalues are. And so with our adjacency graph of the connections between the different web pages, this is also basically kind of like if you normalized it, it would be like the Markov chain, you know, like probabilities of going from one page to another. So you can think if someone was randomly surfing the web and just happened to click on different links, where would they end up? You can kind of get that behavior from A. And so you're thinking about these kind of repeated powers of, you know, someone's kind of on the internet or on Wikipedia. And if they're just like randomly clicking links, you know, over time, kind of what are the most important pages or what pages are they going to go to most often? And so that's why we'll be kind of talking about powers of A because you're kind of going from page to page again and again. So we're going to normalize our matrix, and this is necessary in terms of thinking about probabilities also. Yeah, this keeps it as we take these huge powers from getting too large. And you can do that using NumPy's sum method over a particular axis. Yeah, so down here we have the power method. What we'll do is kind of use our data, then we're going to want to get the kind of the indices. So A.indices will give us the indices that are non-zero, and we want to select those to sum up. So we kind of going back to if we just had, you know, that small four by four graph with the zeros and ones, we're interested in the non-zero entries. I would want to, you know, if we had three non-zero entries, one, one, one, we would want to change that to one third, one third, one third for that row. We'll take scores is a vector of ones of length n times the square root of A dot sum divided by n squared. And this is just kind of an initial guess that people are or that the pages kind of all have equal importance. And then we do eight times scores, get the norm, normalize, and continue iterating eight times scores again and so on. And so what this is doing is you can also kind of think of it as, you know, if you have a thousand people on Wikipedia randomly clicking links, you know, you're seeing where they go step after step. If you did this with enough people for a long enough time, you would find this kind of distribution of like, okay, more people are on this page and not many people are on these pages. Yeah, so this is kind of the idea behind the power method. Why do you think we're normalizing the score on each iteration? And the score is just kind of the percent of people on the different pages. Matthew and Roger has the mic. Great catch and throw. So that, this is a little bit confusing, we actually normalize twice, so we kind of, that's what we're doing up here is trying to like normalize the counts by row. And then down here, I called it scores because it's kind of like the importance of the different pages. But you could also think of that as like the percent of people on each page. So kind of the question I was asking was, why are we normalizing the scores here? Yes, yeah. So the issue that could arise was the values could get way too small and under flow to zero. Or we could also have problems with the values getting way too big if we don't normalize and exploding. So that's why kind of whenever it's good, whenever you're doing an iterative process where you're multiplying every time, it's really good to think about normalizing because you don't want things to be exploding or vanishing. And so then here we can check like what are the scores we get. Here we've done it for 10 iterations. And the top pages are living people, year of birth missing, United States, United Kingdom, race and ethnicity in the United States, census, France. And so what this is is these are the pages that have kind of the most links pointing to them. And this is actually not as interesting as it would be if we were using like the whole internet or something, right? Because here, all people have a link pointing to the living people category on Wikipedia. So that's why that's a super, seen as a super popular page, because there would be a ton of links pointing to that for any entry of a person. And then year of birth missing. It seems reasonable that actually, a lot of people probably don't have their year of birth listed on Wikipedia, and that those pages all point to this year of birth missing page. So this, this makes sense as a like, okay, these are pages that I think a ton of links could be pointing to. So in that sense, they're important, but it's not the same sense that you would probably get with the broader internet of like, hey, this is a really popular page. Linda, can you throw the microphone? So in the score, so like 3.5 answer, and the score is from low to high, and then you show X score, does it mean that Germany is most like, brought up? So this is, this is confusing. What I'm outputting here is actually the norm, and that was just to kind of keep track of how the norm is, is changing. And then also one question for the algorithm is that what is like the third one? And so this is what's kind of turning all those ones into fractions of kind of percent. So, you know, before we had like, you know, A links to B, C, and D, and we want to convert those to one third, one third, one third, because that's your probability of going to each page. Yeah, just actually to answer Linda's, oh, I was going to see if I could display the last, the last 10. I'll modify that for next time and we can see, yeah, like what the least important or least linked to pages are, because that might be fun. Other questions about this? So I have, I have some notes, and these are more just if you are interested in this field or terms you might hear if you kind of do more reading about it. There's something called Krylov subspaces, and those are kind of the spaces spanned by A times B, A squared times B, A cubed times B, A to the fourth times B. And so I just wanted to highlight that we're, we're kind of getting that here, or we are getting that here by taking powers of A each time we go through this for loop. The other thing to note, and we'll see this when we get to the QR algorithm, is that the convergence rate of this method is the ratio of the largest eigenvalue to the second largest eigenvalue, so that could be good or bad depending on what the eigenvalues are. And there's something called adding shifts, basically where you're kind of subtracting off one of the eigenvalues to move it over, which speeds up the convergence, and then you add that value back on once it's, once it's converged. And this technique of shifts or deflation show up in a kind of number of numerical linear algebra algorithms. And so we're not going to get into great detail about them here, but just to be aware also that, so deflation can be used to find eigenvalues other than the greatest one. So here, you know, we were just finding the most, the largest eigenvalue, the most significant eigenvector, but there are ways that this method could be modified to find others. Matthew? Pass the microphone. So intuitively, that's just how, kind of like how many iterations you're having to do to get to a reasonable answer. So you can think of that. In a lot of cases, that would be like how your error is decreasing with each iteration. So there's always a fraction between you and one. I'll look into how it's formally defined. It's kind of often talked about in this more intuitive way, but yeah, I'll look at the formal definition of that. Are there questions? Okay, and then we're about at the end of class. And I hope for next time, I do want to cover the QR algorithm. I think that we won't finish all of this lesson seven notebook, because we've run out of time a bit with the course and I want to, I want to cover stuff in the lesson eight notebook. So we'll see a little bit more here and the notebook will be up if you're interested in this topic and want to go further, but we won't. I think it's very unlikely that we would cover the Arnoldi iteration at this point, just given time. So definitely be sure to make sure you've downloaded the lesson eight notebook before next time. And we will do a little bit more with the lesson seven notebook. All right. Thank you.

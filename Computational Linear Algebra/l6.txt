 Okay, I'm gonna go ahead and get started. I wanted to announce first I put up the second homework on GitHub, and that'll be due next Thursday the 15th, and that's also the day that the draft for your writing project is due, so one week from today. Yeah, and so I wanted to start by kind of following up with some questions that came up in previous classes. One of those was kind of what's going on with the randomized projections, like why does this work? And so another way to think about it is, so here we have our matrix representing the video, you know, where each column is a single point in time, and what we do with the random projection is basically take a random row vector, and so it's like taking a linear combination kind of of all these columns where you've got random values for how much you're taking of each column, and so in the case where you take one of those, what would you expect it to look like? So this is kind of remembering what we saw, kind of I wrote it down in the note, of like you can think of a matrix times a vector is like taking a linear combination of the columns of that matrix. So what would this matrix times a column vector look like? Do you want the, I think Tim had something to say, Jeremy? And keep in mind what your, this random vector you're using to take your linear combination of the columns, that's appropriately normalized, so assume that everything in there is maybe positive and sums up to one. Sam? A linear combination of all of the columns, it would look a lot like one of those columns. Exactly. Yeah, so as Sam said, a linear combination of these columns, notice these columns are actually very similar to each other, like that's why we have these horizontal lines. There's a little bit of feedback, Jeremy. Do you know what's causing that? Or like the microphone, or speaker's humming. Okay, I'll keep going. So these columns that make up this matrix all look very similar, and that's why we have horizontal lines, because you know, kind of here at the bottom, yeah, that looks like there's a black line, and then a dark gray line, and then a black line. That's the space where every column in that matrix has the same value. And so if you were to kind of take this linear combination of them that's appropriately normalized, you're gonna get kind of the same thing at the at the bottom. Like there's no way that you're gonna get, I don't know, like a very white pixel for your result if you're taking basically this weighted average of all these black and dark gray pixels. So it's gonna be pretty close to the mean column? Yes, yeah, so this would be very close to the, to taking an average. Because the average of the weights is just gonna be a number of columns, you know, a number of columns. Yeah, so if we were perfectly taking an average, what would happen would be that we would kind of have smears, oh and I actually I should be pointing here, kind of smears where the black lines are. And so we won't get that because this is, it's important that this is a random weighted average. But the the horizontal lines are going to have to show up in kind of, and what we get is just a single column, but if you, it would match up with these horizontal lines. There are questions about that, kind of with this idea of, so for right now we're just talking about taking one, kind of multiplying by one vector, okay. I see a lot of puzzled expressions. Linda? So you're talking about using one factor times the matrix, like one single value, like factor as like one value? Okay, that's a good question. Let me grab my stylus in my bag. Okay, so kind of going back here, what I'm thinking about is kind of, this is our matrix M, this is made up of columns, and I'll call that C1, C2, C3, and so on. And so I'm thinking about multiplying it by a single vector, but that'll have a lot of values, so, and I'll call these like maybe R1, R2. And these are random, but they are normalized. I can't remember exactly, exactly how, but the idea is, so kind of these are normalized, and we're taking this multiplication, and then just to, actually I should show, kind of to tie it in with the picture on the other plate page, the horizontal lines are still here. But you can think of a horizontal line as just meaning, you know, maybe these all have the value like 120 kind of in this particular row, but it's kind of several separate columns like that. And so when we multiply across, what we get is, I should zoom out a little bit, kind of R1 times C1 plus R2 times C2, and so on. So it's like a linear combination of those, but for this one, so kind of in, in this spot down here, actually I should make, so this is going to add up to some kind of final, final sum, which is going to be a column vector. What's going to be in this bottom spot where the 120s were? Exactly, 120. And so the idea is kind of no matter how we do the random weights, since this is normalized, if this was 120 in every single one, you would have to get a 120 there. Matthew? What's the advantage of doing a random vector versus just averaging it? That's a great question, and that kind of leads into what happens, let me go back to the notebook. It goes back into what happens when we take multiple of these. So when we take multiple ones, if we were just taking an average, we would get the same thing each time. But now, so now we're picking out two different vectors of coefficients, taking two linear combinations to get two columns. Because they're random, they're going to be orthonormal to each other. OK, so because we're taking two different ones, they're random, we end up with something orthonormal, roughly. And basically the idea is just because we have so many values that, let me go back maybe to the notebook, or to the note, that it's just so unlikely that we would end up getting something that was a multiple of something else, given that we have all these random values. And this is nice because it lets us kind of get this new space that's capturing more information about M. You know, like if we just took the average, we would just get the average. But here we're going to be able to kind of pick out different pieces of it. So I think like with the kind of background video, you could think of like if you had two different backgrounds that show up in the video, you would be able to get enough information and you would probably have to take more than two vectors, but you could probably reconstruct both of those backgrounds. This is like a, in some ways, like a very simple example that, you know, we just have this one background we're catching. Or maybe if there's a person that kind of is, I don't know, sitting in one place for a large part of the video, you would end up catching a lot of that person, but you would also catch maybe what was behind her when she moved. So, yeah, so you're kind of able to get different information because these are random. Terry, do you want to hand it back to you? It looks like it's a follow up question. Yeah, I guess I'm just not sure what your random vector is going to be in the connection with time. So the tough thing is it's kind of capturing different points in time, you know, like it's randomly. Yeah, yeah, you're right. It's random in time and that it's kind of saying like, let's take, I don't know, 0.15 from, you know, like second two and 0.3 from second three. And then what the interpretation, is there any way to interpret the information added by that vector? So let me remind you that kind of the overarching goal with this randomized projection is to get something where the columns span the same space as the columns here. So kind of if we did several of these, we would get a matrix because this is this is a very wide matrix and we want to get something that's narrower, but has all the same information. Sam has his hand up. So if we had a picture that had, or we had many pictures, there was a lot of background and in maybe a third of them there was a person standing in the middle. Then we took two random vectors and multiplied it by the matrix and got two columns. Then we could represent the person, let's say in one of the vectors it multiplied zero by some chance. Yes, yeah. Then you could reconstruct just the person by some positive number times the second column minus the first column. So the information of the person is captured because now we can map anything else in any of the pictures by some linear combination of these random vectors. Yes, yeah. That's a good example. Thank you, Sam. Because this is randomness and it's not going to work out perfectly, you would probably want more vectors in practice. But yeah, that's the idea. Can you still go over why they're orthonormal? Like that's a pretty strong condition. It is. And this is kind of approximately and it really just comes down to dimensionality. So the idea that if, I can't remember, this is 4800 tall. So if you had two vectors of length 4800, for them to be pointing in the same direction is just statistically unlikely. Kind of if you have this random... It could be pointing with slight deviation away from each other, right? It could just be a slight perturbation where they're not this... Yeah, I guess it's more that... They're not multiples? Yeah, they're definitely not multiples. It's also really unlikely that they're exactly... Yeah, right. Yeah, and I maybe... Yeah, I want to emphasize that it's kind of the key thing is that they're not exact multiples of each other. So is the idea that we want them to be orthonormal or the idea that we don't want them to be dependent? Yeah, it's more that we don't want them to be linearly dependent. Yeah, I should change that. Yeah, sorry about that. Well, the average correlation of two random projections will on average be zero. It could be surprising if they were highly correlated in a high dimensional space. I'll say maybe not highly correlated. I was thinking like in America, you take two random vectors and you dot five them. It'll be close to zero. What if everything is slightly positive? But these are random Gaussians, so zero and one random Gaussians, so what's the probability of having 4800 positive numbers? It's not high. Two to the 4800. So you're picking the center at zero. Yes? So my question is that we randomly pick these columns. So how do we decide which columns to pick? I know we kind of randomize it, but then just for our own purposes, do we just see, oh, we pick these five out of 100 randomly, see what kind of background we get, and then try another one and see whether we get something different? So really what we're doing, and let me open up this one, we had this randomized range finder method. And so we're kind of doing this in the context of trying to find a matrix that's like our original matrix, but has far fewer columns. Go back to it. And so remember this is kind of like one step in getting the randomized SVD, although in some ways this is the hardest or most non-intuitive step. But we're taking kind of random normal of shape A, and then we're multiplying A and Q together. So this is, I guess going to the question of how do we know which ones to take, it really is just random. But we do have, remember, that number of oversamples. So we are taking kind of more than we'll end up needing. This is towards the goal of getting a randomized SVD, which is a good way to get the truncated SVD without having to calculate the full SVD, which would be really slow for a large matrix. And so here we'll have like a number of components we're going for, and then we take this number of oversamples to kind of give us extra information. So in this method, we actually need to define the number of components and the number of samples. So is there an optimized way to decide what is the best example of the number of components? So I think the number of components would come from whatever problem you are working on. You would need to have either some intuition or something specifically that you're looking for. I mean, you can, sometimes you can take like, take a number, and then we kind of did this here where we looked at what the singular values are and see where it suddenly dips. And so that can be, this is looking at our error from the reconstruction. You could use that to be like, OK, it seems like, I don't know, really getting 50 components, like my error is already kind of like leveled off a lot. So that could be one technique. But if this was, if you were doing this for data compression, you might have particular constraints about, I don't know, this is what acceptable error is or, you know, this is how much space I have available that I'm trying to compress into. So kind of your problem will give you the number of components. And then the number of oversamples was recommended to be 10 in the paper. That's what the authors kind of found to be a good, a good number. Yeah, great. So I guess the randomized range finder is an algorithm that is in place that is doing the optimization for us. Yeah, so the randomized range finder, so I wrote it out so we could see what was going on in it, but it's actually implemented in SKLearn. And so, you know, in most cases you would probably be using, really if you're using SKLearn's randomized SVD, that calls SKLearn's randomized range finder, which is here. Oh, yeah, that's true. It's not, it's not specifically optimizing anything, but it is doing this calculation for you. Although it is, it is random in that it just, actually I can pull this up on here. Yeah, so here randomized range finder. And this is very similar to what I did. I just kind of left out a lot of the kind of like fancier conditions or checks. But it, you know, it is just taking a bunch of random normals. Yeah, so we do iterate, and here we are using math to calculate the best stats. No, we're not getting the best of anything here. Yeah, OK, I'll go to my code because that's shorter. So this, this iteration, and this I kind of just wanted to share like an intuition about, but the idea is that, so this is kind of going back to our goal is to find a matrix that has the same column space as A, our original matrix. So we're trying to find something with fewer columns, but a similar column space. And here kind of taking, taking powers of A kind of gets stuff that's like really in the column space of A. You know, if you did A times a vector and then, you know, multiply that by A again and again, you're like kind of getting stuff like, OK, this is definitely in the column space of A. The issue with that is that unless A kind of has exactly the right size, your answer is either going to be getting like bigger and bigger or it's going to be getting smaller and smaller and you're going to run into numerical issues. So you can't just take a bunch of powers of A. And so what's going on here is it's basically taking a power of A, you know, A times Q, and then it's doing the LU decomposition to kind of normalize it. It's much more complex when you have this loop there and a lot of implementations don't have that loop at all. So like this really simple version of something has the two lines of code. Create a normal random matrix and then take the QRP decomposition of that random matrix times your matrix. You really want to know what this is doing because the first line of code and the last line of code is actually all you need. Yeah, thank you, Jeremy. This is, yeah, you could have the number of iterations set to zero and you could just be getting a random matrix and doing A times your random matrix and then the QR decomposition. And in this context, the QR decomposition, actually, this gets back to Tim's case. This is going to kind of like normalize what you're getting and make sure it's orthonormal for Q. Any other questions? And we may even revisit this again because I know this is weird. The idea that you buy a random matrix and that's it is weird, but yeah, that's all it does. Random matrix theory is shopping counterintuitive. But we just multiply by a matrix and it happens to work. Great. Yeah, and so on that note, I also wanted to just kind of remind you of the Johnson Lindenstrauss lemma that we talked about, which is that a small set of points in a high dimensional space can be embedded into a space of much lower dimension in a way that preserves kind of the important structure of the space. And so that's what we're doing here with taking this really wide matrix and trying to put it into a narrow matrix that's going to have kind of the important same structures. Then for something a little bit wider, this was really fun. Matthew had asked last time about kind of like the history of Gaussian elimination, and it's a lot more fascinating than I realized. So the first written record of Gaussian elimination is from 200 B.C. I'm going to pull this up. Let me hide this. Middle. Oh, I see. Thank you. So I found this history of Gaussian elimination. And so 200 B.C. in the Chinese book of arithmetic, there's a problem. Three sheafs of a good crop, two sheafs of a mediocre crop and one sheaf of a bad crop are sold for thirty nine Dow. Two sheafs of good, three mediocre and one bad are sold for thirty four Dow and so on. And so this is kind of like a classic Gaussian elimination problem. So I thought that was really neat. And then they even apparently had a system where they used different colored bamboo rods on accounting board to do Gaussian elimination. And so I linked to this page because I think it's really interesting. So in those days they sold it with a computer. Yeah, a much earlier computer. Anyway, so then it goes on that Gaussian elimination, kind of the next next record of it was in Japan in the 1600s. Sekikawa actually invented the determinate, although did not get credit for it. And around the same time Leibniz independently invented the determinate and also did not get credit for it because the ideas didn't take off. And then, OK, I thought this was neat. So then Gauss described the elimination method as being commonly known. So he was not even the first European to to discover it and never claimed to have invented it. But he did possibly invent the Cholesky decomposition, which Cholesky did not come up with until later on, but gets credit for. So that's just a kind of fun bit of math history. But this writing kind of emphasizes like how important Gaussian elimination is and just that it's been like a problem that people care about for over 2000 years and is still widely used. I thought that was fun. Any questions or comments about the history of Gaussian elimination? OK, so then another question that came up last time was about ways to speed up Gaussian elimination, and I did not have time to dive into this as much as I want. This is on my to-do list after this course ends. But LU decomposition can be fully parallelized. And it looks like there are actually kind of multiple ways to do this. So I found this slide show. So kind of here they've shown what the different dependencies are and then talk about ways to parallelize that, including one approach is kind of to break it down into conglomeration, but you know, kind of what depends on what. Like grouping those together and other tasks. And then also this is a 2016 paper that you can do a randomized LU decomposition, which lets you run it on a GPU without having to do GPU to CPU data transfer. So that would be much quicker. So I just want to kind of let you know we're not going to go into these. Let you know that they're out there because I think that's exciting. And it's also interesting that this is still an area of research that people are working on. So another another question that came up last time was, Valentin discovered that using scipy.linauge.solve, so we had this kind of pathological matrix that we looked at, and actually let me do it for a lower dimension just so you can kind of remember what it looks like. So this matrix that was ones along the diagonal, ones in the last column, and negative ones in the lower triangle. And you may remember that when we did Gaussian elimination by hand on that, basically what ends up happening is that you're kind of doubling at every row. This final column of ones, and so you end up with these powers of two that's getting very large. And so we get the wrong answer. We get a wrong answer when we do use lusolve in scipy for that, however using.solve was getting the right answer. So looking into that more, this is not a significant difference, but lusolve was quicker. This is scipy's implementation of lusolve. And so I can imagine if you're doing a ton that there are maybe advantages there. Particularly because if you're doing this a lot of times you only have to do the LU factorization once of A. If you have like a lot of different columns B you're solving for, you can kind of save that decomposition. But looking at scipy's lin-alg-solve, I looked at the Fortran source code for what it's calling in lawpack, and in the comments, actually I'll pull this up. Let's go to the right place. I kind of mentioned several times that it's looking at the reciprocal pivot growth factor. And so if you'll remember we defined this term rho back in the notebook, rho was the growth factor, and that's the kind of the greatest ratio of the AI over UI. And in this case it's 2 to the 59th. This is where n equals 60. So it's 2 to the n minus 1. So the lawpack's method is actually taking that into account, whereas straight LU solve is not. And I think it can be, like you probably don't want to go too far down this rabbit hole, but I think it is, it can be interesting to kind of look up the lawpack methods that are being called by a particular kind of scipy method, just to get an idea of what it's doing. Any questions about that? Kelsey? What property, when you say row pivoting, you mean when we're shuffling the rows but not the columns? Yeah. So we saw when we weren't shuffling the rows, we had that example with 10 to the negative 20th, where we looked at this matrix, 10 to the negative 20th, and here we had problems because we hadn't shuffled the row order. And what's happening here is that you're, I guess, having to multiply by 10 to the 20th in order to, no, you have to multiply by 10 to the negative 20th to zero out this 1 on the next row. And kind of multiplying by this really tiny number is setting you up for these kind of like potential underflow errors, which is kind of what happens here. That like in general, you don't want to be multiplying by tiny numbers. And so what switching did is, so we had, oh, did I pass it? Yeah. OK, so then we had this other matrix where we had switched them, and here when we calculate the LU decomposition, actually I can change it. I guess like the fact that we multiply by 10 to the negative 20th and cancel those out. If you're multiplying by something large. So this is a good question. So like part of the issue, I don't think this is the heart of it, is that like over here you end up with 1 minus 10 to the negative 20th, which rounds to 1. And that's an OK way to round. Let me let me do this one out by hand. I think might be helpful. Kind of like why the negative. Why the one case breaks down. So in this A, we had 10 to the negative 20th is the first one. 20th, 1, 1, 1. Oh, you know what? The other issue is going to be that we're like leaving this as a pivot. So later on, we're going to have to come back and multiply through by 10 to the 20th, which doesn't seem like seem like a good idea. And actually, let me even put down so that would mean L would be. This negative. 10 to the negative 20th. 1, 1, 0. Well, aren't we we're multiplying this one by. Oh, I see. Right. We're multiplying. Yeah. Thank you. Thanks, Tim. Oh, because the subtraction is implicit. Yes. Thank you. That's 10 to the 20th. So this was a. In the case with a hat, we're getting or starting with 1, 1, 10 to the negative 20th, 1. Is this entry correct? The 1 minus 10 to the negative 20th. That should be. What. So we're multiplying by 10 to the 20th. Yeah. Plus one. One minus. Thank you. And then this one will be one minus 10 to the negative. OK, so in this. Oh, OK. In the first day, the unstable one, we're ending up with pivots that are like we have one pivot that's like tiny, like dangerously tiny, and then our other pivot is dangerously huge, which is kind of like the worst of both worlds. Whereas when we've switched them, actually, both these pivots are good because they're close to one kind of much more reasonable numbers. So having having this 10 to the negative 20th as a pivot, I think is the issue, because that's what then kind of led to us getting huge and tiny pivot columns. The other thing that's going to happen is like when we're solving this system, you know, kind of doing the UX part, like we're going to have to divide by 10 to the 20th. And then on the next row, we've got these coefficients, I guess, that we're now like dividing by that are huge and tiny, which is bad. So it's only a problem because we do have the composition going down. So it's only like the ratio. Yes, yes. Yeah, the ratio is kind of what's creating these, but it's also like the ratio is kind of artificial because you could put the rows in any order and it would. So what it is is you would actually when you do this, if you had like a matrix that was more than two by two, is that on each iteration, you want to choose like the largest value that you have. Yeah. And then swap those two rows. But kind of once you have more rows, you can't you can't do it all in advance. Like you kind of have to go, you know, like see how it changes your rows to do one, one of the outer loop where you zero something out and then for the next one to, you know, pivot, which is, you know, swapping two rows. But kind of each time you're going to like find what's the best row to swap with. You're welcome. Any other questions about this? OK, let me go back down. OK, and so then that was it for kind of following up on LU factorization. I wanted to return to a question from last week about block matrices. And this is something that I wanted to teach at some point and kind of wasn't sure where to stick it. So let's talk about it here. First I wanted to talk about ordinary matrix multiplication to contrast. And so I want you to take a moment to think about what is the computational complexity or big O of matrix multiplication if you have n by n matrices? I just want to raise their hand if they know the answer. Roger. It's n cubed. Exactly, it's n cubed. No, that's good. I was going to ask you how you got that. So to calculate one entry, you need to multiply a row by the column and that's n multiplications? Yes. And then there's n squared for the entries, so that's n squared times n. Exactly. Yeah, thank you. Yeah, so matrix multiplication and n cubed is slow. Like it's not a good computational complexity. And that's what matrix multiplication is, though. But a different or kind of an additional perspective to think about is, you know, we've talked about this memory hierarchy. And in general, what can we say about slower types of memory? Or how much slower are they? I'm just looking for like a phrase, not a number. Yes, a lot. So slower. Yeah, I was thinking order of magnitude. But yeah, like typically going to a slower, the next slowest type of memory is going to be like an order of magnitude slower. And much cheaper. Yeah, they are also much cheaper, which is a benefit and why we have slower memory. But if we think about matrix multiplication and kind of take into account memory, what's happening is, say we're reading row i of A into fast memory in an outer loop. Then we'll read column j of B into fast memory. Then we'll multiply that row times that column. So this inner loop is kind of like the n to get a single entry in C, our product. We're doing A times B. And here it's written. Oh, here it's written with a mistake. This was supposed to be plus equals. With the idea that you've, you know, you're kind of, you know, multiplying pairwise the elements of this row of A, this column of B, and adding those to your kind of running total to get this element C. And it takes n of those just to get one entry of C. And then kind of you'll write that element back to slow memory once you've got it. Are there questions first just about kind of what this algorithm is doing? This is just standard matrix multiplication, only kind of like assuming that you can only fit one row of A and one column of B into your faster memory. Okay, so I want you to think about how many reads and writes are made in doing, in doing this. So if you're multiplying two matrices together. Question. So how is fast memory and slow memory separated? So, yeah, this is kind of hard because this is hidden for you. So you're not explicitly telling the computer to do these things. And by slow memory here, I probably mean cache, or not, sorry, fast memory. I mean cache, slow memory, like main memory. Yeah, you're not explicitly controlling this, though. It's kind of like a lower level of the languages handling this. I mean, often when you write these, the people that write the libraries we use do do this. Yeah, that's true. Either it's like really big parallel stuff, they read and write to the network from time to time, or for the fast stuff on their computers, they try and manage the cache. Particularly on GPU, you have to manage it manually. And I guess this also comes up with like, so LaPAC is like specifically optimized for each type of computer. You know, unfortunately, it's the standard, so it's out there. But LaPAC and BLAST are being kind of optimized to the specific architecture of your computer. In practice, probably a lot of people in the past will end up having to deal with this directly because they'll find themselves at jobs as data scientists where they're working on clusters, and so you're basically pulling stuff off the network. When you multiply matrices, you're probably using these kind of techniques to do it on multiple computers. Yeah, that's a good point that this is like a helpful way of thinking because it applies to so many different levels of the memory hierarchy as well. Yeah, you'll see this kind of if you're doing large amounts of data on a network. Yeah, so take a moment, write down how many reads and writes are being made to do this matrix multiplication. And sorry, I want you to make a distinction between... Actually, yeah, never mind, just write down how many reads and writes. Okay. Okay. Hello. Okay, raise your hand if you want more time. Does anyone have an answer they want to share? Yes, and actually before you throw it back, how does that break down for kind of A versus B? Yes, exactly. Right, right. Yeah, thank you. That's great. So yeah, Sam said kind of this outer loop. We just read each row of A into memory once, which will end up taking n squared total. It's n each time, and we go through the loop n times. However, with B, we're doing for each row, we're going to read in each column. And so that ends up being n cubed reads for B. And then C. So for each j, we're making n reads to read column j in, and we're having to do that for j equals 1 to n, so that's n squared. And we're doing that then for each row of each row of A. The issue is like each column, because it's a column, so it has n length n. Yeah, sorry, so this is a number of element-wise reads, but it's important to note that it's n more reads for B than for A, because you've got this redundancy of your pulling in each column of B multiple times, because you have to do it for every single row of A. And then in the k loop, you're assuming that doesn't count, because it's fast memory now. K loop, well, they've already been pulled in, yeah, so you're not having to, yeah, right, right. Thanks, Jeremy. So questions about how I got, and then you're doing n squared writes to kind of write C back out to slow memory. Questions about that calculation? Okay, so now we're going to contrast it with block matrix multiplication. And the idea behind block matrix multiplication is basically that you're just kind of subdividing A and B into several smaller matrices. So we could say you're making big N blocks. Each one will be of size little n over big N times little n over big N. Here, big N is just two. So we're just getting four blocks, two by two. And it turns out like kind of the matrix multiplication works exactly the same. So the kind of top left quadrant of C is going to be A11 times B11 plus A12 times B21, which you get from just thinking about rows of A times columns of B are giving you this result. Similarly, over here, you're taking the top, and here when I say row, I mean the top row of blocks of A times the second column of blocks of B to get this value. And so on. And so you kind of are just doing these smaller matrix multiplications and then putting them together to get your result. So what this looks like in pseudocode, and here I'm just referring to them as block IK. It's kind of talking which one of these, you know, four smaller matrices of A are you getting. So you have three nested loops and kind of an inner one. You're taking block IK of A, block KJ of B, and then saying block IJ of C plus equals block of A times the block of B. And you have to kind of go through each of your Ks, keep adding those to your running total, and you get block IJ of C. First, are there questions kind of about just how this matrix multiplication is working? Okay, so what is that? What is the big O of this? Pass the microphone, Jeremy. Looks like we should have the same amount of small operation. Yes. It shouldn't be magic, but we could have a little bit more operations because of moving those blocks, but it's negligible. So in terms of the like actually doing like additions and multiplications, it's the same. So it'll be n cubed again. And we'll talk about the movement as the next question. But we've not changed our computational complexity. So, yeah, next question. Now, how many reads and writes are made? And here, like before, I want the answer to kind of be element wise. And this will be in terms of both little n and big N. And I should note that here. Here your loops are just, you're just looping through big N because you're kind of just going through the blocks. And you can talk to the person next to you about this as you think about it. Oh, yes. Jeremy can pass the. Before we answer this question, would you briefly explain why this is considered a very different method from the previous one? Because it seems to me that it didn't seem to me obviously that this has found something very different. Yeah, so it's not very different. It's it's different in that just the previous method, you're kind of thinking of A and B as a whole and dealing, you know, like row by row, column by column. Here you've broken it into several small matrices. And so kind of inside these loops, you're now thinking a small matrix at a time. And I've even kind of cheated here because I say block of A times block of B. Really doing a block of A times a block of B would be like calling the previous method. I guess if the answer to question two turns out to be different, then that's the answer. That's true. Yeah. So we are. Spoiler alert. We are going to see that there's a different number of reads and writes being made and then we'll talk about some other benefits of doing this approach. Yeah. I was just saying from an algorithm perspective. Yeah. Yeah. Who wants more time? Does anyone have an answer they want to share? Yeah. So there's three for loops with big N each. So if you're just doing like an operation on the order of linear time within those three for loops, it would be big N cubed. But since you're reading a block in each of those and the blocks are of size little n over big N, then it winds up being big N squared times little n for the number of reads. Yes. Exactly. Yes. Let me write that out. Okay. I was trying to pull up the thing where I could write on the screen. So let me delete what I have. Yeah. So as Vincent said, it's important to remember our blocks are of size little n over big N by little n over big N. So that's little n over big N squared is the amount of work. And then we've got big N cubed worth of loops since we have those three nested loops. And this reduces to big N times little n squared. And you'll have yet it's two times as Tim indicated. So you're doing that for A and then also for B. So I'll just multiply by two. And then C is still going to be little n squared writes. So this is an improvement in the amount of reads from, you know, going between slow and fast memory that we have to do, because we're assuming that big N is substantially less than little n. It's the number of blocks that we've made. So this kind of illustrates, yeah, one big benefit of block matrix multiplication is you've really reduced how much you have to kind of read from slower memory. And you're kind of taking better or the reason behind this is you're taking better advantage of locality of kind of when you, you know, pulling things in a block, you're getting to kind of more fully utilize them than before with B. It was like we're pulling a column in of B only using it once and then like throwing it back and pulling in a different column of B. Are there questions about this? Can anyone think of what another benefit of using block matrix multiplication might be? So this shows better locality. Kelsey? That's a good point, although that's actually possible with ordinary matrix multiplication as well. That's true. Yeah. Yeah. You could ignore blocks that are zero. I like that you're thinking about sparse matrices. Tim? Yes. Yes. I was thinking I could parallelize this more easily. Because this is kind of already clearly separated. What about memory? You never actually read the whole matrix into memory. So Tim's point is basically another scalability thing. Yeah. If you had a giant matrix where even reading in a single row is excessive. A vectorization. We have block IJFC plus block A. Instead of in the original iteration, you're adding each entry one by one. If you're calling all the blocks together, it's faster because you're calling the underlying. That's a great point. Thank you. I have to say we're a little bit late for our break, so let's go ahead and take a break now for seven minutes. We'll be back at twelve thirteen. I'm going to start back up. Any final questions about block matrix multiplication? And I also wanted to remind you again, I forgot to say this at the beginning of class, but if you haven't filled out the mid-course feedback survey, please do that today. And the link for that is in the Slack channel. Valentine and I have a question. Just in terms of the difference between vectorization and parallelization, my understanding is that parallelization divides the course into computing tasks, while vectorization is more like computing some number in kind of matrix form. Vectorization often refers to SIMD, which is single instruction multiple data. It's kind of like you have a few, like a little vector that you're doing kind of like the same operation to. It turns out that modern processors have these processor-level instructions that can operate on four or eight things at a time on a single floor. And that's vectorization. Yeah. And then, as you said, SIMD is often described as vectorization. And then, as you said, parallelization, you have kind of different cores handling the processes or even different computers. So is it correct to say that like for vectorization, we write it in a vectorized form and then we hope then that the compiler or whatever will parallelize it on their hardware level. But for parallelization, we make it parallelized. Yeah, so NumPy handles vectorization for you. Yeah, so we don't have to usually explicitly vectorize it because NumPy is doing that for us. Whereas you have parallelization, you are having to write out more explicitly. But it would kind of vectorization, it depends what libraries you're using, like if they automatically do that. Thanks. You're welcome. Good questions. All right. Now we're going to be starting a new lesson. Notebook 4, so compressed sensing of CT scans with robust regression. And we're going to start kind of before we get into the CT scans or compressed sensing with just a few foundational concepts that show up a lot of places. And the first one of these is broadcasting. And the term broadcasting actually originated with NumPy, although it's now used by a bunch of other libraries. And it describes how arrays with different shapes are treated during arithmetic operations. And so some of this I think you've already seen and maybe haven't thought about because a lot of it feels very intuitive. So here we have an array A that's one, two, three. It's got three values. B is just a scalar, two, and we can do A times B. And what happens is really it did two times one, two times two, and two times three. So in a sense, it's almost it's almost like A was treated as being two, two, two, so that the dimensions matched. Another example, if we take a matrix, where we can define a matrix then kind of using this to be V comma V times two comma V times three. And what we get out is one, two, three, two, four, six, three, six, nine. So the first row is one, two, three, our original matrix V. Then we've got two times each entry and then three times each entry. We have a three by three matrix. And then we can say M plus V. And remember M is a three by three matrix and V is a vector one, two, three. And so typically kind of in written math, you would not want to do a matrix plus a vector. Like what does that even mean? But here NumPy kind of handles it for us and says, OK, you probably wanted to get back two, four, six, three, six, nine, four, eight, twelve. And so what was happening is that it just added one, two, three to each row. Are there questions about these two examples so far? What if I wanted to add a column wise? That's a good question. And that actually leads kind of directly into the next example. So we transpose V, make it a column. So you'll notice up here the shape of V was three comma, which is like saying three by one. And we've made V one, which is oh, sorry, this is like one by three. V one is three by one because we've transposed it. And now when we do M plus V one, we get two, three, four, four, six, eight, six, nine, twelve. So this highlights that it's important to kind of keep track of these dimensions. So if you were doing M plus V and you had a specific answer in mind, either adding it by rows or adding it by columns, you would want to make sure that you you did what you were hoping to do. So NumPy has a set of rules that kind of govern how this is happening. And those are that NumPy compares the shapes element wise, and it always starts with the trailing dimensions. So those are the dimensions kind of on the far right hand side. And it's looking for them to either match perfectly or one of them to be equal to one. And I think this is helpful. NumPy documentation is pretty good for this. They have a lot of examples, and so I want to walk through maybe a few of these. So here, and this is one that comes up a lot, if you were dealing with RGB values, so a picture, that channel is three, and maybe add a 256 by 256 picture or number of pixels. And then if you wanted to scale it by three, or sorry, scale, if you're scaling red, green and blue all by different values, you would have three values. So it's kind of dimension three. And these line up. So the last two dimensions are the same. You can kind of think of, since this is shorter, you can think of it as being like one by one by three. And so this this works. Down here, if you had A being eight by one by six by one, B being seven by one by five, you can add those and get something that's eight by seven by six by five. So that's pretty amazing because A and B don't seem to have any dimensions in common starting out. But fortunately their ones line up properly. So A has one in the last dimension. B has five. It goes with five. For the second to last dimension, we've got a six and a one, so it goes with six. Third to last dimension, a one and a seven, goes with seven. Final dimension, eight. And so it goes with eight. Are there questions about this? Matthew? Yeah, starting with the right to left. And these, I think it's really good to try, to kind of play around with trying a few different examples. There will be, in the homework, there's some of just kind of, I give you different sets of dimensions and you have to say, you know, yes, this would work and this is what the dimensions would be, or no, I'm going to get an error. These don't line up properly. Yes. Yeah. So we just start by looking at the end. And so they've kind of written these out very nicely that you can see, kind of like, OK, like let's look at the far right and see if these line up. Because you'll notice, like here, A was a 4D array. It was eight by one by six by one and B was seven by one by five. We don't start comparing like eight and seven. Those don't match. We're not going to do this. We start with the far right side. Yes. Yes. So the two things that have to be true is they either need to be exactly equal or one of them has to be one. It makes intuitive sense because if there were anything else, it wouldn't be obvious how to broadcast it. Yeah. Yeah. I mean, even. So let me try making a matrix that's larger. We can make a new matrix N that is. I'll also say that I think dot shape is just a really helpful. Why is that making it an array? That shape is a really helpful attribute. Just to be able to see kind of what you're dealing with. And like, I think it's a good thing to check on your data in general, like, OK, do I have what I think I have here? And so like here with N, and we can even kind of confirm, OK, what was the shape of it? M was three by three. Would I be able to add N and M? If one is two by three by three and the other is three by three. I heard a yes. Thumbs up. Yeah. And that's correct. We could. And that's because we would start with, OK, the trailing dimension is three for M and three for N. Those match. Then the second to last is three and three. Those match. And then we have a two. And this is like having a one. So we're OK. Any other questions? Jeremy? I was just going to mention I love broadcasting. And I find when I read other people's code, they don't normally use it, even in popular open source libraries. And often I'll rewrite it with broadcasting and it'll be four times less code and ten times faster. It seems to be something that's underappreciated. I actually like it so much that in my favorite GPU library called PyTorch, I wrote a broadcasting library and made this code much easier. In two weeks time you'll be adding a bunch of PyTorch broadcasting. That's awesome. Thank you. Something that's nice about broadcasting is that NumPy is vectorizing the array operations for you. So it's very efficient. You also found that thing the other day. In the last couple of weeks they've changed it so it doesn't even have to reallocate memory. That might have been a month ago now, but that's a recent improvement to NumPy, which was already a great library. Our next general or foundational topic I want to talk about is sparse matrices and specific ways of dealing with them. So just to remind you, a matrix with lots of zeros is sparse. Here's an example. So all the light gray values are zeros and then we've got some other values but not a ton. So it's more memory efficient to just save the non-zero values and not allocate a memory position for everything. And sparse matrices show up a lot in various engineering problems and they often have kind of interesting patterns and you'll get them from multi-grid methods and also from differential equations and so this is just an example of one kind of with a pattern. And here the black entries are all non-zero, the white entries are zero, and you can see most of the matrix is zeros. So I wanted to go into more detail about, okay, I've said we're just storing the entries, but what does that actually look like? So there are three really common sparse storage formats and those are coordinate-wise, which SciPy calls COO, compressed sparse row, which is CSR, and compressed space column, CSC. And I found a page that has, I think, some nice examples. And actually, yeah, I found two pages that we're going to look at. All right, so this is the coordinate-wise storage method we'll talk about first, and I think in some ways to me it kind of seems like the most natural one. Yeah, so here we've got a matrix, and are you guys able to see this okay? Kind of the matrix on yellow, and everywhere there's no number, that's a zero. So they've only written the non-zero entries in this matrix. You can see this is a eight by eight matrix. And then they've color-coded some of the entries, and that's just nice because then you can kind of see, so down here on green is the representation. Yellow is kind of how we would think about the matrix, green is what the computer's storing. And so in spot 00 there's an 11, and we see down here they're storing value 11 is in row zero, column zero. So on there's a 22 in spot 11, and so it stores value 22, row one, column one. So just storing the coordinates for each one. There's a 75 in row six, column four, and so it stores 75, six, four. So all kind of the computer needs is these three vectors, one of the values, one of the rows, and one of the columns to have all the information from this matrix. And so in this example there were 20 entries in the matrix. And so you are having to note that for each entry you are having, you know, you're having to store the row and column, so this is taking, I guess like 60 spots in memory to have this information. Which you can imagine many cases where that's, yeah, less than, you know, here this is a seven by seven matrix, that would have been 49 spots in memory. Yeah, are there questions about coordinate wise storage? So this is kind of slightly more per entry, but overall if you have a lot of zeros you're saving memory. And you're also, this is going to change how you access, access points. I mean here you would, I don't know if you wanted to see what the value at a particular coordinate was, you would have to check like is the row column paired even in this matrix, if not it must be zero. Okay, so some properties of coordinate wise method, the rows or columns don't need to be ordered in any way, you can put them in kind of in any, any order. And then you have to store, nnz stands for number of non zeros, but yeah, the number of non zero entries plus two times nnz, integers for the indices. I should have noted that above, like if these, if these were decimals, I don't know, we're not really getting into memory types, but row and column are integers, so they can take up less memory. Okay, so the next, oh, actually and first it talks about matrix vector multiplication. And so with that, what you can do is just basically loop through all your entries. So here it's just looping through values. And then for those values, you know, looking up the column value and the row value, but you don't actually have to go through every spot in the matrix, you're just going through the number of non zeros, because you can kind of loop through that value vector. Questions? Okay, so then it, go on to the compressed sparse row data structure. So this set, this is actually kind of even more consolidated. And the idea is we can store things in row, sorry, in order by row, and then we're only having to keep track of like when we go to a new row. So we have a row pointer that does that. So here you'll see in row zero, we've got 11, 12, and 14. So we store the values 11, 12, and 14. And then we're saying those are in row zero. Or here, let me come back to the row pointer a bit in a bit. So we're saying, okay, those were in columns zero, one, and three. And then we don't enter anything into this row pointer vector until we go to the next row. And so basically we're saying, okay, entries zero, one, and two are in row zero. But then entry three is in row one. So for this row pointer, you actually, if you look back to the index, that's kind of telling you, okay, here row three is the first, sorry, the third value, which is 22, is the first thing in row one. Then a new row starts with the sixth value. The sixth value we can look up here is 31. And so row two must start with a 31. And it does. So this takes a little bit of getting used to how it works. Let me do another one. The third row starts with a ninth, no, the ninth entry. Yeah, so we go over here. That's 42 is the ninth value. And that is the first, the first one in row three since we started indexing at zero. Then we can go to the 12th value is 55. 55 is the first thing in the fourth row. So everyone see kind of what this is doing. So what do you think a benefit of this is? This is on the surface harder to, I think, figure out at first. Jeremy, can you pass the microphone? So kind of the coordinate method also gives us a way to put them in a table. What's the benefit of this over kind of that previous coordinate method? I think the previous coordinate, we can't really put every single value, right? This is a lot more compact into one fixed length fixed width table. But previously, if we extend it out, it's no longer valuable. So we have to compact it into this format. Okay, so this is more compact for the row pointer. You're just keeping track of kind of when you switch rows. So you've got less redundant information. Can anyone think of any other potential benefits? Matthew, you want to grab the microphone? So I think this may be similar to what you're saying, that this makes it really easy to look at particular rows. Like if you needed to access your data by row. So like if you were interested in getting row with index three, you know, you can just look up, okay, that starts at nine. And then it's stopping right before 12. And so then you can come over here and say, okay, let me get the values that start at nine and stop right before 12. And I've picked out this row in a fast way. So I'd say this is, row access is easy, I guess is how they say it. But notice it's difficult to look up columns this way. In fact, it's going to be like, I mean, so you can look up, you know, like go through here and say like, okay, I want to know everything. Whoops, didn't mean to enlarge that. You know, you might say, okay, I want everything that's in column four, which is a lot of things. But notice going to, okay, here's something column four, 25. If I want to find what row 25 is in, that's going to be a little bit more of a pain. Like you're going to have to kind of like go through your row pointers. You know, here there are a bunch of, like even, yeah, you could say like, okay, 45 is also in column four. But now I have to figure out which row it's in. And that's less obvious. So kind of knowing if you're going to be accessing your data by rows or columns would be really important here. Valentin, can you grab the microphone? Thanks. So why don't we just transpose the matrix, then it will be easy to find the column, right? So when you say transpose, so there's also a compressed sparse column format, which is, yeah, the same thing only by column. Okay, and then just to the question, so if we just traverse through this representation to find like row i, the complexity is V O of n, where n is the number of rows, right? So in the worst case scenario, we'll have to make n steps until we go to the row number n, right? Say that again if you're trying to find what? So if I look into the row i, in the worst case we'll have to make like i comparisons until we get to the row, right? It'll, yeah, I mean, I do think you'll have to walk through the number of row pointers, but that's going to be, when you're saying i is like, in this case, eight by eight, like the number of potential rows there are, I think that's true. Why not just keep another hash table or binary tree with indices of every row? Then it will be much possible for the graph index of the row, right? So you kind of want to, you're suggesting almost like combining the coordinate storage where you do have all the rows. Yeah, and the rest will be a big O of number of rows, right? I'll say like these have different benefits, like so if you really were going to have to be looking things up by row and column a lot, you might want just to go with the coordinate wise method of storage, since that does let you look things up by row or column. You know, like kind of if you start having aspects of both, you are taking up a lot more storage space. So then there's a trade off of like, you know, if you have both like coordinate wise storage and the compressed row storage, you've now really increased kind of how much memory you're using. Sam? Wouldn't compressed row storage always be faster than the first method, COO? Because if you're trying to select your column in compressed row, you have to do a search through all of the column indices. So it's, yeah, so if you're, if you're, the issue with the pulling a column from CSR is that you are having to do more work, though, to find the row coordinates that correspond. You know, like you can iterate through and you're right, you have to go through everything to find all the fours since this isn't in order and COO is not in order either. But with COO, when you find the fours, you're also automatically getting with the row indexes. Whereas here you have to kind of do this like bookkeeping of like, okay, you know, here was a four at value 10. Which row corresponds to value 10? So that is more work to figure out the rows. Yeah, these are good questions, though. Yeah, this is good to think about. Any other questions on this? And I'm not going to go through, and actually I think they don't have compressed column on this one because it's so similar to compressed row only. Now it's easy to look up or easy to get a column, harder to get a row of data because you would have to like work backwards to find the column. So here the amount of memory accesses is reduced. It's really, or they even kind of say advantage of CSR over coordinate wise method. Yeah, fewer memory accesses. But yeah, it will be difficult to look things up by column. And then actually this other web page, let me just do one more example. And are you able to see, I just like here they have a single matrix and they show it in, oh this is translating numbers, that's not, okay let me stop this and open it again. This also I thought was fun. Just at the top it lists a ton of different sparse matrix compression formats. I had never heard of most of these, but I was like wow there are a lot of methods out there. Jagged diagonal format, non-symmetric skyline format. So depending on, you know, if you had a very, if you really knew the algorithms you would be using, you would kind of know like okay what are the most efficient ways to be accessing my data. But I like this page and that it's kind of color coded this matrix and then it has it in compressed row storage, also in compressed column storage. And then, yeah, it's even got additional modified ones. But here, yeah, you can compare and they kind of represent the row start index a little bit differently. But just showing that 11, 12, 13, and 14, this is like light red. Those are in row one. You're just storing a one once, but you have to store the column indexes all these times. And then next up for rows you just store a five because you're up to value five. Yes, and like that is something that people do. It's like so, you know, and we saw with the kind of the coordinate wise multiplication that you do, you know, it is possible to only look at the entries you have. Yeah, there are, yeah, like you can store your kind of block matrix as all being a zero. Yeah, any more questions on these sparse formats? And so then I took this from the SciPy documentation. So, because I was seeing this in like code that I was finding online, the COO format can be a way to efficiently construct matrices. And so sometimes they were constructing them with COO format and then transferring them to CSC or CSR because that, and I guess this is like it would be, it would be a pain to figure out how to manually enter CSC or CSR. So you probably want to use a different way. So I just want to note that multiplication or inversion are better done with CSC or CSR format and all conversions among the different types of formats are pretty efficient linear time operations. So that's nice that you can go between them. And that kind of gets at, I guess, the earlier question of, yeah, like what if you want to be able to access rows at some points and columns at another. Okay, so we'll be, we'll be seeing some sparse, sparse matrices in a little bit. But yeah, now I wanted to kind of start our next application, which is CT scans. And I found this article that I thought was really nice. And it starts with, let me pull it up, saving lives, the mathematics of tomography. There it goes. And it starts with the quote, can maths really save your life? Of course it can. And this is, this is written in a pretty accessible way. It felt like it kind of talks a bit about, makes this, this analogy with milk bottles kind of stored in those kind of stacking crates. So they give, actually let me just go up to their problem. So they're saying milk and fruit juice is delivered in bottles that are placed in trays, you know, a three by three grid. And so maybe you're wondering which type of bottle is in which compartment. And you can't see because there's some in the middle, you know, like it's been stacked up and we've got these grids. And the idea is that different amounts of light passing through milk and juice, you could get a total like from the lights. And this is a bit like a Sudoku puzzle where you just know, you know, some of the totals going across a grid and you're trying to figure out the individual entries. Are there questions? And this is just kind of like as an analogy. So here they're saying, OK, maybe like five units of light have made it through here, six, four, six, three, six. Let's work backwards to see which is in each of these. I don't know if you're amplifying. Yeah, the lights are on. It was. It's off now. Try again. It just went off again. Batteries might be dead. Go ahead and just ask your question, perhaps. Wow. Yeah. So just to emphasize, Jeremy was just saying that earlier diagnosis can increase survival rates tenfold. So doing CT scans well is really valuable. Saving lives. Let me go back to the notebook. So kind of what's going on with a CT scan is you've got this source of X-rays going through, you know, semi dense object kind of being the person. And then we've got a detector picking up kind of the strength of the signal to get information about the density that it's passed through. And I should say all this is coming from a scikit-learn example, but I've kind of added more explanation and visualizations of it. So in our last lesson, we used robust PCA and we saw that that was an optimization problem where we're trying to minimize the nuclear norm of L in order to get a low rank matrix and the L1 norm of S. And what's special about the L1 norm? What does it give us? Sparsity. Yeah, exactly. And so that'll be useful today. And let me show you. Let me show you what the. OK, so these are what our pictures are going to look like, and actually I'm going to show you where we're going and then I'll come back and kind of show you more about how to get there. Basically, where we're going is that we'll have this data where, you know, this is kind of an artificial data set that we're going to create. And the idea is that we're going to pass out lines through it at different angles from all different directions. And we're just going to get a single number from this line going through this complicated picture, basically about what it's hitting. So here you can see it. This kind of intersects these circle globs at several points. And we'll get a slightly larger number, 6.4. I'll talk about how this is boiled down in a moment, but thinking about a CT scan, you know, it's kind of sending an X-ray along this line and then just measuring something on the other side. And then here we've got a line going through, you know, different direction, different angle. This is not intersecting with much, like it's just kind of catching the tail of this blob. And we get a much smaller number, 2. So before we had 6.4 or something, here we've just got 2. And so it's kind of amazing that you're just, you know, for each like X-ray shoot, you're just getting a single number, but kind of knowing that these are coming from, you know, different angles and different directions. The idea is that we want to work backwards to being able to reconstruct our original picture. Oh, no, that's good. So Jeremy was just pointing out that this is an analogy in real life. They're doing 2D projections onto 3D. Sorry. Well, person is 3D and you're coming up with these 2D pictures. Here to keep it simple, we kind of just have this 2D data set and we're just getting one dimensional data to reconstruct the two dimensional data set. Yeah, and then it's 1255, so we'll stop now, but I just wanted to at least kind of introduce the problem that we'll be looking at next time. Thanks. Matthew? Oh, that was 6. Let me go back up. So there is, it's kind of just a measure of how much it's intersected with these other lines. So I think in a person it's more about kind of like the density or like the, kind of what the material it's traveling through is. Yeah, in real life it's not as direct as this, they do like wavelets and transforms, like it's not, real life CT stands out sparser, it's all that always sparser. In this case it's just a dot product. Yeah, you're welcome.

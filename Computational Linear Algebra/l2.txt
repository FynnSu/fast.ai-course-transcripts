 We'll be starting with Notebook 2. Sorry, I'm getting over a cold. Topic Modeling with NMF and SVD. Okay, so I briefly showed this diagram last time of works of Shakespeare. There's different plays going across the top of this matrix, and then the rows are different vocabulary words. And this is a way that you can represent a body of work in a single matrix. And this is a type of bag of words approach, because we're not looking at any of the syntax or the structure of the sentences, we're really just interested in the frequency of the words that show up with them. And so today we'll be talking about two different ways to decompose this into a tall, thin matrix times a wide, short matrix. And something I want to emphasize is the reason that we like kind of matrix, or one reason we like matrix decompositions is that the matrices we're decomposing into have special properties that are helpful. So I wanted to start kind of just hypothetically thinking about an extreme case where suppose you wanted to reconstruct a matrix just using an outer product of two vectors. So let me switch over to that, okay. So if you had kind of your words are still the rows, the documents are the columns, and as motivation you could think of just doing this outer product of two vectors. This is not going to be a very good reconstruction, so actually I should put that as a very approximate. But the best you could do with this would be to have kind of this first vector be kind of the relative frequencies of each vocabulary word compared to out of the total word count, and then this could be the words per document. And so this would really just kind of reconstruct a very average version. If you wanted to kind of extend this to the next simplest case, that would be to let this be two columns and this one be two rows. And if you did that, what you would kind of be wanting to do is have your documents in two different clusters, and so you could be capturing the relative frequency of words for documents in one cluster with that first, you know, tall skinny matrix and the relative kind of words per document with the two. So this is just kind of some motivation of kind of thinking about these very simple cases. Yeah, so today will be, oh and actually I wanted to show, so here I've just gotten my import statements, and again let me know if you have any problems getting your imports working or Jupyter notebook running properly. I wanted to highlight, so I've linked to a few different resources that I thought were helpful. I particularly, I almost used it in this class, but didn't. And there's this really nice, it's called text analysis for the humanities and social sciences. And it's a pretty lengthy tutorial that kind of walks through using French and British literature. So it's got Victor Hugo and Jane Austen doing a word analysis, and they use some different techniques. But if this is a topic that interests you, you might want to check that out. Okay. So we'll be using a built-in data set from scikit-learn today. I mentioned last time scikit-learn has a lot of different built-in data sets. And this is newsgroups. So newsgroups, this is kind of before the World Wide Web took off. These were kind of discussion boards and they're still in existence where people are talking about different topics. So you can kind of think of this as similar to something like Reddit where you have people posting within different categories. And so we're using this kind of functionality from scikit-learn to fetch the newsgroups data. And you can pass a parameter to say which categories you want. Today we're just going to look at four categories to keep it simpler. And something to note is we're getting the categories because it'll be helpful for us. But what we're doing is unstructured, or sorry, unsupervised machine learning. And so we're not actually using the categories. We're kind of going to be thinking about, like, have we clustered these into topics that make sense? And we'll check with the categories to confirm that. So whenever you get data, it's great to just check what the dimension of it is. So here that's 2,034. So that's how many kind of separate posts we have. We'll just look at a few of them. So the categories, and I want you to guess which category each post is from. The choices are atheism, miscellaneous religion, computer graphics, or space. So let's look at this first one. I've noticed that if you only save a model with all your mapping planes carefully positioned to a.3 DS file, which category do you think this is from? Someone say it louder. Yes, from graphics. And this next one says, it's talking about Koresh, was a deranged fanatic who thought it was necessary to take a whole bunch of folks with him, mentions Jim Jones. Which category do you think this is? Anyone? So this could be atheism or it could be religion, and we'll see some overlap between those. Next one, I actually had to look this up. It's talking about Perry Jove's, which is, turns out, a point in the orbit of a satellite of Jupiter. So what might that be? Space exactly. And so that's always kind of good to just see like, okay, this is what the data looks like. So we'll see, like we're seeing some kind of like numbers and things, not all the words are going to be helpful to us. So here we can check, the target is giving us the label. And then scikit-learn, so kind of if we wanted to make that matrix with all the word frequencies, we could write something ourselves, which would be a bit tedious to get those word frequencies. Scikit-learn has a feature extraction, count vectorizer, that does that. So we'll just use their count vectorizer. So that's what's happening. This input cell, count vectorizer, and we're passing in stop words to say that we don't want to, don't want to include those, since they don't contain information. But oh, and stop words are, you know, where it's like, is, the, a, that don't have much meaning, or don't really have much like kind of category or subject meaning. And so note, after we do this, we check our, so we kind of get our vectorizer, and we're doing a vectorizer fit transform, we pass the news groups, the training data, and then we're converting this to dense. And last time I talked about sparse versus dense matrices, we'll go into more detail on that in the future, but what scikit-learn is returning is sparse, and we're gonna make it dense, because that's easier for what we're doing today. Now we can check the shape, and so we've still got 2,000 documents, and then it looks like we have 26,000 vocabulary words. Oh, that's a good question. I don't think we have any guarantees on what order it's in. Oh, actually, never mind. When you get, so there's a vocab dot, or sorry, vectorizer dot feature names, and down here I'm putting that into an array, and they are alphabetical. Let me actually pull up the, oh, thank you, that's what I wanted to illustrate. Here I've, was inside the parentheses for count vectorizer, hit shift tab several times, and it will get, show you kind of what the parameters and the doc string kind of documentation about it is. And there are a lot of, a lot of features that we weren't using. So you could remove accents, do some sort of pre-processing, but yeah, that's a good way to kind of find out more about your methods. So yeah, down here I kind of use vectorizer, get feature names, put those into vocab, and then looking at the vocab, it's alphabetical, and so here's a section in the C's of some of the words that show up. Any questions so far? Do you want to stem words so that, you know, they both meet, so that you can use the feature space? For example, we have councils and councils, they're pretty much the same thing. That's a great suggestion. Yeah, so this is kind of a lazy approach where we have not, not stemmed the words, and stemming refers to kind of taking different variations of the same word and putting them all in the same thing. So that's something like, and then the word walk, you would take walking and walked and walks and kind of classify all those under walk, and we're not doing that here, but that would be a completely legitimate thing to do. Okay, so first up, we're going to use singular value decomposition, and actually I'm curious, have you covered SVD in previous courses? What context did you see it in? Image compression. Okay, great. That's a great application of this. So Gilbert Strain has written a classic linear algebra textbook, so that SVD is not nearly as famous as it should be. And so here kind of the key thing with our decomposition is we're saying that we want the matrices we're decomposing into to be orthogonal, and the motivation behind that is that we're thinking that words that are frequently in one topic are probably less likely to be in other topics, otherwise they wouldn't be a good thing to kind of define that topic of what makes that topic unique or different from the others. And so that's a great question. What's orthogonal? That said, any two, so if you have a collection of vectors, any two vectors that are different, dot product is zero, the vector with itself, oh so orthonormal means that the dot product with itself is one, and that the dot product with other vectors is zero. And a little bit confusing when you talk about matrices being orthogonal, that means that their columns and rows are orthonormal. So this is a great, a nice diagram from a Facebook blog post that we're going to refer back to later, but so here they're talking about the words being the rows and then having hashtags as columns, and there these are frequencies of how often a word shows up in a post with that particular hashtag also in the post. And then U is the kind of first matrix you're getting, and the columns of U are orthogonal to each other, and then we've got this kind of middle diagonal matrix that the white part is all zeros, it only has values along the diagonal which is blue, and that's giving you the relative importance. So kind of those values are called the singular values and tell you relative importance. And then the pink matrix on the right there, the rows are all orthonormal to one another. The rows are, yes. Oh sorry, the rows are orthonormal. Something to note about SVD is that it's an exact decomposition when you're doing full SVD, and so that means you can completely recover your matrix, and that this is, well so in this case if R was equal to N, and so it's the same dimensions, this would be an equal sign. So SVD is very widely used, and we're gonna see this, see SVD in multiple of the lessons in this course, but we're using it today for semantic analysis. It's also used in collaborative filtering and recommendations. The winners of the Netflix prize were all kind of more complicated variations of SVD. Data compression, which you've seen, it can calculate the more Penrose pseudo inverse. So this is for matrices that don't have a true inverse, there's a kind of the pseudo inverse and you use SVD to find it, and then principal component analysis, which we'll also see. Yes. What does orthogonal mean like intuitively, and why is that useful for this example? Good question. So what orthogonal means intuitively is that you're capturing very different information, very different directions, and so if you were thinking about directions, it's like perpendicular. You're not even having stuff that's somewhat in the same direction, but capturing very different. So in this context of thinking about topics or categories, we want very different topics and kind of finding what makes this different from everything else. And if you think of the kind of math definition of this dot product going to zero, it's that everything has canceled out. But thank you, Jeremy. So now I've, oh and this should show using percent time to time what I'm doing, in Jupyter these are called magics when you have a percent or double percent, and this is really handy and we'll use this some more to kind of get comparisons of how long things are taking. Yes. And so the question was, is it possible just to set it once and have it throughout the notebook? I don't know of a way to do that. Cool. Okay. Neat. So now I wanted to ask you, so we're using, and this is from SciPy's, Linal, JSPD. We get U, S, and VH back. Just take a moment to confirm that this is the decomposition of the input. So it's always good to kind of check that you're getting what you think. So this should be something that you type in. No we don't, it'll depend what you're doing. And this, I'm trying to remember, it's gonna be later on that we're using the fact that we convert it to dense, but a lot of a lot of operations can be done sparsely. Sure, and I'll, there's a future lesson, let me exit full screen, there's a future lesson where I'll give more detail on exactly the SciPy sparse types, but just in general with a sparse matrix, so you can kind of think of a dense matrix like an Excel spreadsheet where you have a square for every space in the matrix and you're, you know, putting in the value. If it's zero, you're storing a zero there, and you kind of have that block of memory. For a sparse one, you only store the non-zero values. So there, it could conceivably just kind of be this list of you're gonna need to have, you know, the row and coordinate, because then to keep track of where it is, actually let me show the picture again, yeah, so this is what a sparse matrix looks like. You have all these zeros, and so what you could do, let me try this writing on the screen, oh, I don't want this one, okay, okay, great, so what I was gonna do is say, so this is kind of showing the, the dense storage because you wrote out all these zeros again and again. Instead you could just be like, okay, it places zero, coordinate zero, zero, I have a one, a coordinate one, one, I have a one, a coordinate two, two, I have a half, and so on, and kind of construct that list, and that would be sparse storage. We'll give more details around that. Okay, so do you guys have your answer for confirming the decomposition, and what did you do? Okay, so S was being stored as an NP array, I couldn't get it to convert into a matrix, so what I did was, I did U times S, and then at BH, and that was basically the right side being multiplied, and then I did vectors.2dense, subtract that, and I just summed all the way through. Oh, okay. So that's one approach. Did anyone do anything else? Okay, I'll say what I did, so I used, NumPy has a method called np.diag that will, so I found that you had kind of done a vector matrix product, yeah, to get the same thing. Diag is handy, it takes in a vector and makes a square matrices with those as the values. Actually, it's a little bit confusing, diag can go in both directions. If you give it a vector, it returns a matrix, if you give it a matrix, it'll return a vector of what was on the diagonal, and that's kind of a shortcut. Oh, okay, and so the question was about at, and this is in Python 3, at is matrix multiplication, and we talked briefly last time about how you can have both Python 2 and Python 3 installed on your computer. I definitely recommend switching to Python 3, or for this class, or in general I recommend Python 3, but you don't have to and it's fine to do this course in Python 2, and if I was in Python 2, I would use np.matmall, which is matrix multiplication, from, oh, np.dot, that's kind of cool, but also kind of annoying, it does a lot of magic, so it tends to, regardless of what you're doing, it tends to make a lot of noise, so sometimes that's good, but sometimes it's like you have a bug and you can barely realize because it took something, but in this case, they'll do the same thing with two matrices. And then actually, oh, yes? Could you also use np.op for it? Yes. Yeah, let me do that, and I actually, normally I prefer np.allclose, so I'm kind of surprised that I did this subtraction here, let me do that too, oh, okay, well I will look into this later, did you use allclose on, I might have changed these variables lower down and be getting something different, so, okay, great, yeah, no, I like np.allclose. And that's checking, it takes matrices and checks that all the, each element is similar. Great. Any questions? I thought these, I mean, so I actually haven't been running this as I go through, oh, but, because my vectors have been converted to dense. I would prefer to keep going, I think that this is an artifact of, that I haven't been running the previous ones. Yes? So what is the function into the linear, whatever the norm, that norm is? That's a good question. So that, so norms in general give you kind of the distance between two things is a way of thinking of it. And this I would believe would default to the L2 norm, but that's a way, what? The Frobenius norm. The Frobenius norm, okay. And you can always pass an argument, but so what that's doing is we're taking the difference and that would be a matrix of things that are close to zero, but I just wanted to see a single number, so I used the norm to kind of get that down to a single number. The norms you can think of as size, typically. Yes? Yeah, so you're kind of squaring each element and adding them all together for the Frobenius norm. Okay, next up, I want you to confirm that U and V are orthonormal, so just take a moment to write in some code that does that. So that's something I do. This depends on how people define it, but a lot of definitions of SVD distinguish that you're actually getting back the transpose of V as opposed to V itself. You can ignore that, but you will kind of see this difference about what people define as V versus V transpose. And the H stands for Hermitian, which is kind of the equivalent to transpose when you have complex numbers, but we'll be sticking with real numbers in here. Okay, next up, I want you to confirm that U and V are orthonormal, so just take a moment to write in some code that does that. Okay, raise your hand if you want a little bit more time. Raise your hand if you have the answer. And I should be clear, when I say U and V are orthonormal, I don't mean to each other. I mean that the columns of U are orthonormal to the other columns, and the rows of V are orthonormal to the other rows. All right, does someone want to share their answer, what they did? Okay, thank you. So what you can do is multiply them by their transpose and then compare them to the identity and the reason this works is multiplying U by U transpose is multiplying, and actually one of these I guess I have backwards, join each column, but the kind of multiplies each column of U, yeah, so taking the transpose you get the rows by the columns, that's kind of column of U by column of U with V, you're getting row of V by row of V. And then when they're kind of the same row by itself, that's the diagonal of the identity, that's why you're getting ones, different kind of two different vectors, those are the off diagonals, the zeros. Are there questions about that? And actually let me draw that briefly. The idea is, so if this was U, you've got these columns, this is U transpose. When you multiply those together, you'll kind of end up taking this vector times this vector and so on, and so that should be one, doing this two different ones, that's going to give you zero and so on. All right, and so now I've kind of confirmed that we got what we expected with the SVD. We can look at the singular values, and remember that these are giving us kind of this measure of importance, and notice that it drops off very quickly, so kind of there are some pretty high values for importance here with the first few singular values, and then it really drops off. And it's hard to know exactly kind of what these numbers correspond to, but it's helpful to see the relative importance, and the vector S of singular values is always ordered, so you kind of have the biggest values first. And I think this will be easier when we see the topics, but we can pass in, and so remember we're doing, going back to the picture, U times S times V, and kind of larger ones are saying like, okay these columns of U and these rows of V are more important or make up a bigger component of our original matrix. They contribute more, yeah, to the original matrix. So we have this little helper method, show topics, and we're gonna pass in our matrix V, and then it's gonna look up the words that correspond to the values. So remember our vocabulary words were in alphabetical order. This is basically gonna find, okay, the largest values showed up in, you know, these particular columns which corresponded to these vocabulary words. And so we get VH, so we're looking at, sorry, the first five columns of VH and the top eight topic words. So something to notice is, one, this first topic is very weird, so we'll ignore that for a moment and look at the other ones, but so we have a topic that is largely represented by the words JPEG, GIF, file, color, quality, image, format, thinking back to our categories. Which category might that correspond to? Right, computer graphics. The next one is graphics, edu, pub, mail, ray, FTP. And so, any guesses? Yeah, I think that's also computer graphics. And so notice that they're kind of different, possibly different subgroups within a topic. But then Jesus, God, Matthew, people, atheist, atheism, and this is possibly either religion or atheism. And then another one on graphics. So yeah, graphics is well represented. Actually, let me try taking ten topics to see if we can get some more. Okay, here's a space one, space, NASA, lunar, Mars probe, moon, mission. So there are some space ones in there too. So yeah, we got topics that matched with the kind of clusters that we would expect, even though we had never passed the categories in. Any questions? Yes. Could you specify, like, what each column of U is actually, like what column of U is? Yeah. The situation is what V is, like the row of V. I think the row of VH will be the topics, right? Right, yeah. So the columns of U correspond to the particular post. So we could use that if we wanted to look at a particular post and see, okay, how much of each topic shows up in that post. So from the beginning, kind of when we read somebody asking a question about computer graphics, we could look back and see. And so this is, these are sometimes called embeddings, but U is kind of giving us like, okay, these are how the individual posts were embedded and into the kind of what the topics and then V is giving us and these are how the words kind of correspond to those topics. Yeah, I'll try. That's a good question. I'll try writing a method. I don't want to do it on the fly, but what you would have to do is kind of pick off the largest entries and then yeah, look up which really you would want to tie it back to the words eventually. So I think then kind of look up, yeah, which topics are biggest and then what words are in those topics. Any other questions? Okay. So in your picture, R is the number of topics. Yes. Number of documents by number of topics and then the next one is number of topics by number of topics. Right now our R is equal to N, but Jeremy was just pointing out that this matrix is estimated. So the blue matrix is words are the rows, hashtags are the columns, the purple matrix words are the rows, the columns are, what, oh sorry, I thought I said, okay, purple matrix words are the rows, the columns are the topics, the pink matrix V, the rows are the topics, the columns are the hashtags, oh sorry, I see what you're saying, sorry, Facebook example was different, by hashtag I mean document. Okay, so let me say it one time with documents. Sorry about that. Okay, purple is words by topics, pink is topics by post. Okay, don't worry, so we're actually going to return to SVD more later in this lesson, but we're going to kind of take a break from SVD and look at NMF and so I think it'll be good to revisit SVD later, so there'll be more chance to ask questions about SVD. So NMF stands for non-negative matrix factorization and some motivation, we can look at decomposition of some faces and here kind of the red pixels are showing negative values on the faces. So we kind of have found almost the topic equivalent of face, you know, like what are different components of the face, but how do we interpret this? Like what does it mean to have a negative part of your face, you know, that you can like add these together to form faces? And so this is the motivation for NMF, that with a lot of data sets having something negative doesn't really make sense and is hard to interpret. And notice in SVD it was completely possible to have negative values. So here we're kind of swapping out before like the key thing with SVD is this orthogonality, you know, it is assuming that things are orthogonal to each other, now we're gonna have the key thing kind of be we want everything to be non-negative. So NMF is a factorization of a, and your original data set should be non-negative if you're using this, otherwise you won't be able to construct it. Also if you have negatives in your original data set, the negatives probably make sense in that context. So we're just factoring into two matrices here, W and H, and we want each entry in W and H to be non-negative. And so for this face, face idea here if each column of our original matrix was an actual person's face, what we would be capturing would be a matrix of different facial features and then the relative importance of each of those features in a particular image. And I believe you saw the eigenfaces data set in Yanet's machine learning class. So NMF is a hard problem. One is that it's kind of under constraint so you could find different answers and typically you'll add kind of additional constraints. There's also it's NP-hard. So coming back to the problem that we're looking at today with topic modeling. So we have our original matrix again, words by documents. And here that'll get decomposed into matrix W that is words, words for the rows by topics as the columns and then topics importance indicators. So here topics would be the rows and their importance would be the columns. Question? So I mean here you're breaking the words and document to approximate topics and then you would be able to do the same thing and then you would be able to do the same thing, right? So this is an alternative to what we saw with SVD and I'll talk about some of the pros and cons of NMF versus SVD later on. But I would say they're both valid approaches. Yeah, I just wanted you to kind of see a different way of tackling the problem. Oh right, and that's a kind of a common way of interpreting what they mean, yeah. And here also with all of these there's not a clear answer of what the number of topics should be. You know, even something with our data set where we like knew we were bringing in these four categories, you know, we've seen that they're kind of multiple subtopics within computer graphics. There's also some overlap between religion and atheism. So it's often not going to be clear even kind of what the quote best dimension to use would be for the number of topics you're looking for. Right yeah, so for each for each document it's the relative importance of each topic. And you can think of that because when you multiply W and H together for a particular document you are kind of taking a linear combination of the topics and you want to know what the coefficient is. So scikit-learn has a built-in NMF that we'll use first and that's in the scikit-learn decomposition module which we imported above and so kind of as I mentioned we're telling it how many components we want so that's kind of a decision we're having to make. It returns that when we do kind of our classifier dot fit transform that will return the W and then we can get H just kind of stored in this components on our classifier. Yes and this is non-exact meaning we're not going to get in most cases we're not going to get our original matrix back perfectly. We're getting something as close as close as we can and that's a good question. We will we will head into that in a moment of yeah different ways to do this. So here we can check we can check do our topics make sense again looking from the second matrix H and yes they seem to be fitting with what we know our categories are so JPEG image GIF file color very reasonable computer graphics topic. The third one is space launch satellite NASA commercial reasonable space topic okay. I have a section on topic frequency inverse document frequency but term frequency inverse document frequency. I'm not gonna go super in-depth into this this is something that's highly are not highly but often comes up as a way to kind of normalize your inputs. I tried it though with what I was doing and didn't see a significant difference for this particular data set and these decompositions but I think it's good to be aware of it. So here we kind of want to take into account how often a term appears in a document how long the document it is and also how common or rare the term is. So when kind of so far we've just been dealing with these raw frequencies which don't really take into account any of that. And so what TF-IDF does instead is it's got this term TF which is the number of occurrences of a term in document over the number of words in a document. So if a document is super long we're basically kind of giving relative less weight as opposed to before we would have just had a lot of high frequencies showing up. And then the IDF term takes the log of the number of documents divided by the number of documents with the term T in it and that's a measure of how common or rare a word is. So if a word only shows up in very few documents it's pretty rare and probably has more significance. So yeah we won't go too much into this but I wanted to let you know that it exists. And then I guess we'll break soon and we'll just kind of say non-negative matrix factorization. In summary the benefits are it's fast and easy to use. The downsides are it took years of research and expertise to create. So this is a version to kind of get back to the question about stochastic gradient descent, a version that was not using stochastic gradient descent but that is a lot more specific and optimized to NMF and so it's great but the the people that created it had to have a lot of kind of expertise and knowledge to do so. So we'll look at kind of some alternative ways to calculate NMF ourselves when we come back but let's break and meet back at 1207. Alright we're gonna look at kind of return to non-negative matrix factorization. So before we were using scikit-learn's implementation which is very specific to the problem of non-negative matrix factorization and we're gonna try writing our own in NumPy and we're gonna use stochastic gradient descent which is a lot more general. So first I'm going to introduce that just kind of the or remind you of the basic idea of standard gradient descent which is you choose some weights to start and then you have a loop that uses your weights to calculate a prediction, calculates the derivative of the loss and the loss is the also known as the error function or the cost but that's what you're trying to minimize and then you update the weights and you kind of keep going through this loop to do that and you get better and better weights. And the key here is that we're trying to decrease our loss and the derivative is giving us the direction of steepest descent for our loss. And so for this I'm going to use the gradient descent, oh yes, I'm gonna use an Excel notebook and this is something Jeremy originally developed for the deep learning course and it's um I think it's very helpful and it's good and I think many many programmers can be kind of snobby about Excel but Excel is really visual and it's kind of a good way to see things. So I'm just gonna kind of walk through gradient descent in here, oh sorry I misspoke I'm gonna do an IPython notebook the Excel is for stochastic gradient descent which we'll get to next. This is also from the deep learning course and so this notebook is called gradient descent intro it should also be on github in the class repository. So here we've got a method for a line that takes in a b and x and returns a times x plus b. We're gonna choose our slope and intercept. This is what typically you don't know so we're gonna kind of choose these to make a fake data set and then this will let us check kind of how good our method is if we can get back to them. But in the real world you don't know what the quote true a and b are and that's what you're trying to figure out. So here we generate 30 random data points so we've got this array x 30 points y is just a times x plus b and we can plot them they're perfectly aligned which is what we would expect since we use the line line method to create them. And then here we have a few methods and SSE stands for sum of squared errors so that will take a kind of true y and our predicted y subtract them and square it and sum it up. That's gonna be our loss here so loss method just calls the sum of squared errors and then we can get the average loss over so we have a bunch of points by taking the square root of the loss divided by the number of points that there were. So let's start off guessing negative one and one. So our loss is 8.9 so we're not doing great to start. We'll choose what's called a learning rate so 0.01 is where we're starting that's something though that you'll typically adjust as you're going and then each time we update what we'll do is kind of make predictions using our guess for A and our guess for B. We know what the derivative of the loss is since this is a line or sorry some this is the derivative of sum of squared errors so we're using that derivative and then we're updating our guesses A and B. And this is really neat this is an animation that shows kind of what's happening and this is a bit stop this so we're kind of starting down here when we've guessed one on one and one for our A and B which is not a very close approximation at all and we can see as it runs so here inside of animate we just have a for loop that's calling our update method from up here UPD and so we can see that the line gets closer and closer until it's a really accurate guess. Are there any questions about this? Oh down here? Yeah so the derivative is giving the direction of steepest descent so we want to subtract that from our guess and the reason we use a learning rate is if this is a large number we could end up kind of jumping back and forth from the true answer and so it's kind of better to take smaller steps and so the learning rate is basically the kind of the step size of like okay the derivative gives us the direction we want to go in and learning rate is telling us how big a step to take in that direction. Any questions? Okay so this is standard gradient descent there was oops nothing stochastic about that and so the idea with SGD is so with standard gradient descent we evaluated the loss on all of our data in that example we only had 30 data points so it was very quick and in the most problems you'll use you have way too way too much data and it's really slow to evaluate the loss on every single point and it's also kind of unnecessary particularly when you're far away from the true answer to do that much of an evaluation and so stochastic gradient descent is the idea of evaluating the loss function on just a small sample of the data and that's typically called a mini batch and that's why it's stochastic because depending kind of which batch you choose to or mini batch you calculate your loss on you'll get different answers but it turns out that kind of an aggregate because this is part of a loop it's good enough and you get a huge huge improvement in speed and so this is what I was going to show inside the excel spreadsheet and this one will start in a very similar way so we're just and this excel spreadsheet is also available in the github repository and there are tools like open office that you can kind of look at the spreadsheet without having excel so we've just chosen kind of an a and b to generate our data from and this is helpful because it lets us see how good our answer is but this is kind of the part where you would typically have a real data set so we've got our data and then the basic sgd we're going to make some guesses for what the intercept and slope are here's this learning rate which is what we're going to multiply by to figure out how big a step to take our data has been copied in and here are many batches of size one so we're going to just calculate what the derivative of the losses so here's our prediction here's the air and then we calculate the derivative of the air just at that single point and then update a and b for that and so we can see this actually might want to increase the learning rate this is going pretty slow so the the air is getting smaller oh no that's horrible okay i won't do this on i won't i won't improvise this right now but you can adjust the learning rate to take some smaller bigger steps if you take too big a step and this is what can happen to you and you shoot off in the wrong direction but there are questions about stochastic gradient descent and then this notebook includes a bunch of other optimization techniques we won't be covering them in this class but they might be of interest to you if you're interested in optimization on the spreadsheet you don't need to use the macros or the buttons it's all within the macro so as jeremy was kind of saying what this does is it takes the new a and b down from the very bottom and is that what the macro does and then puts them i guess back up here for for intercept and slope yeah so you can see that's kind of picking off the and so it's a stochastic gradient descent is a really really general approach and it's something that you can apply to a lot of different problems anytime you have a derivative for your loss and in fact you actually don't even need to have a formula form for it so i think it's something that's really useful useful to know and i've linked to a few few resources including the SGD lecture from Andrew Ng's Coursera class from the fastai wiki this is kind of a nice blog post on a variety of optimization algorithms but so applying that to our specific problem of nmf and we're trying to decompose v into the product of two matrices where all their entries are non-negative so we're minimizing the Frobenius norm of v minus wh we want to get that as close to zero as possible because that would be in the wh is a good approximation of v and then we really want w and h greater than or equal to zero so in order to use SGD we need to know the gradient of the loss function and here we've looked that up and are just going to kind of use it this is also something you could calculate if you like multi-variable derivatives multi-variable calculus so we've got kind of our vectors from before this is still again the same matrix with the the words as rows and the columns or documents down here we define the gradients it takes in m w and h and it returns the gradient and so here we have the this is the gradient from w and then we also have this penalty term and the penalty is going to be and this is for w or kind of for each of our matrices the penalty is to penalize the matrices when they're negative so we need a way to kind of force force the algorithm to go towards positive values for w and h so we want to have a penalty when w and h are negative and that's what's happening here so we call penalty which has got this and p dot where if the matrix is greater than we're going to choose mu is very close to zero if the matrix is greater than mu there's zero penalty we're happy because it's positive however if it's negative or super close to zero we're assigning this penalty and then we update w and h by calculating their gradients which includes the penalty and then just just like what happened in the kind of simple version with the line in the notebook we do the learning rate which is our step size times the gradient and this report method this is just so that we can get a sense of how we're doing as we go along so the first term is showing m minus w times h so just kind of seeing how far apart they are you know is w h a good approximation of m then we're going to look at the minimum of w and the minimum of h that's something where I don't know if we got like negative a thousand that would be a really bad sign ideally those would be zero and then also the sum of how many of the terms are less than zero any questions so far about these methods we're defining sure yeah so the because we kind of have so we have two separate things we're trying to do one we want w times h to be really close to m a good approximation and the second is we want w and h to be positive so we have these two separate goals and taking the derivative of the loss will help us get w times h close to m because that's kind of the particular derivative we've we've calculated and so that's what's going on with this kind of this first part so that's that for w this is that for h then to deal with wanting w and h to be greater than zero here we're deciding okay we're gonna have this penalty that is equal to zero if they're positive because we're happy so no penalty and that's what NP dot where does NP dot where kind of takes a truth true or false statement if it's true it uses the first value so if w is greater than or equal to this tiny number that's close to zero we go with the first value zero there's no penalty however if w is less than you the penalty is going to be larger for the further away that M is from you yeah and you can think about like another way to think about it is there's not a clear or obvious way to take a derivative from okay we want these things to be greater than zero whereas for having this m close to w times h it's much easier to be like okay this is how far away they are we can take a derivative of that that's a good question so here um this will be negative what we're doing with our w and h is we're doing w minus equals dw so that would come back as so this is being returned in the grads method we have like minus and negative is positive but that's a good point this the issue is we could flip the signs on everything but then you would also have to flip the signs on the derivative here yeah but the minus and negative it basically means the further the more negative it is the bigger a positive number you add to w okay these are great questions thanks everyone yeah so now to kind of start it we need to choose random values for w and h so we'll just use kind of a random normal to get back matrices of the size that we want and then we're taking the absolute value of that because we should at least start with non-zero mate or non-negative matrices and so we'll just kind of check the initial and as a reminder this report and this is just a few values that we thought it would be interesting to kind of monitor as we go along to see how we're doing so this is how far away m and w h are is the first one the second is the minimum of w the minimum of h and then the sum of their negative terms oh the count of notes oh okay great um count of their negative terms thank you so then we run update which just calculates the gradients updates them once and we can check the the air has improved a little bit so this is slightly smaller forty four point four one nine now as opposed to forty four point four three nine before and here we have introduced some negatives into w and h and if we run this and I'm actually gonna do this for fewer okay so one as this runs you'll notice it's a little bit slow so we see that the product w h is getting closer to our matrix m that's the first column is going down here the negative terms actually kind of seem like they're saying about the same I didn't do that many iterations anything the count is going up so kind of a key thing here to notice that this is really slow and I ran this for longer and it continued to be very slow and there's also a lot of parameter fiddling because like right now looking at this I'm thinking I would probably want to increase the penalty on negative values just because you know these counts seem really high and like they're not going down particularly well I guess H is much bigger bigger than W but so we have like a lot of different parameters so even though we have this generic method that mostly seems headed in the right direction and it does have some some shortcomings sorry about that all right so one way we could speed this up is to use PyTorch PyTorch is a Python framework for dynamic neural networks with GPU acceleration I was just released in January and many of the core contributors are on Facebook's AI team and it's used it's used at Facebook as well as many other companies Twitter is using it but it actually has two purposes so kind of in addition to being a deep learn and I think it's a really excellent deep learning framework it's also a replacement for numpy that uses the GPU and a lot of the right now it has less functionality than numpy just because it's so new but a lot of people are actively working on it and the methods it has are very similar to what numpy has so I have a link so we'll kind of just be using it in this lesson that's a thing I really wanted to expose you to and it's something that you if you want you can use for your project you could try to implement something else in PyTorch because I think it's a really really nice framework and I have a few links to find out more so you do not have to have to be using a GPU for this course that's fine you will not get the speed up of running stuff on a GPU just in the code below here whenever you see a dot CUDA you'll need to delete that if you don't have a GPU oh and so if you're interested in getting a GPU well one so the MCN program has a box with a few GPUs that people can share and then something else that's great is AWS instances and you can watch this setup lesson this was from the deep learning course that kind of walks through how to request p2 from AWS and what that does is it's you know letting you spin up a kind of computer in the cloud that has a has GPU capabilities yeah and definitely feel free to ask if you want kind of more help or advice about that but this is a kind of thinking about this course of like the general goal of making matrix computations faster using a GPU is a great way to do that and so I mean we'll see in a moment although do you have a ballpark you want to throw out oh yes significantly faster so we have to do some imports from um I guess also so pytorch is in it's a Python library but it's torch is a Lua library that's been used in computer graphics for years so it's even though pytorch is newly released it's kind of coming from this really well developed library that grew up around computer graphics which have to do very fast matrix computations we're also like really all indebted to the video gaming industry for kind of keeping GPU technology advancing so much so something you'll notice with with pytorch is well I'll come back to this so in pytorch you have tensors and variables and they kind of have the same API meaning you can do the same things to them however variables from remember or keep track of how they were created which will be necessary later on we'll find out about that so we've just converted our vectors to dense V then we're casting them so V as type NP dot float 32 and putting that into a torch tensor remember a tensor is just kind of a generalization of a matrix and so we had to specify our type and this is something that you'll see with a lot of these options to speed things up and they need to know you to know the type so here we kind of redefine the methods that we had above and you'll notice that they're fairly similar I'm a key thing that's different we have these dot mm for matrix multiply so now this W dot mm H that's just doing W times H let me see what other differences jump out at torch clamp is a method that takes a value and then we'll cut it off with a kind of minimum or and or maximum that you feed in so here we want M minus mu but we're cutting it off at zero because if M is large and positive we're happy or not even it doesn't have to be large if M is positive and bigger than you we don't want any penalty so that max zero will take effect and there's zero penalty so this is handling that penalty from before here that dot sub underscore that subtraction and it's a minus equal subtraction from the original and this you don't have to understand all the syntactic details of pi torch but I wanted you to get a general general feel for what's going on so we need to kind of declare our float tensors and said this last time but a float is basically a decimal as opposed to an integer when we talk about data types again this is in a little bit of a different order but we're doing the same thing W is being initialized with a normal random variable and then we're doing dot apps to do the absolute at the end but that's kind of just saying the order that these happen in take the absolute value yes great thank you so now you can see this is going much faster than before and the one above I actually probably should have run for yeah so this one each loop was only 10 I only did 50 total and that felt kind of slow now each loop is a hundred and we're doing a thousand and that was thousand iterations that was much quicker and so you see that these have improved you still are going to need to do some more but we're able to kind of get through a lot of iterations much quicker any question but oh this is as good as I guess I thought we got better then here are the topic topics that we found so objective morality values moral subjective science absolute claim I guess that maybe that's that's atheism God Jesus Bible believe atheism Christian perhaps religion miscellaneous space NASA launch shuttle orbit lunar moon is a good space one so these are some nice topics so Jeremy was referencing what we got from the built-in so these are the topics from the built-in and again since this is an unsupervised learning problem we don't have an absolute measure of like these are what great topics are we're just kind of using our intuition about looking at them well I mean that's giving you how good your approximation is yeah so these are these are good topics okay so in that version or actually I should ask are there any other questions about this version yes you can still use it but you're not getting the speed up so there's less less reason to all right I mean you could still question of deep learning aside if you're using it as a numpy alternative kind of the main motivation for that is the GPU speed up but it will still work yes also when you're using pie torch if you do have a GPU you need to be sure to use dot CUDA otherwise pi torch will not put your stuff on the GPU which is what you want to get the speed up so you have to tell it to do that and you will have to install pi torch and to be able to do this but anaconda but yeah I recommend using anaconda okay so in this version note so this was very similar to what we just did in what we had done in numpy from scratch we still had to know what the gradients were and this can really be a pain particularly if you have a more complicated method having to calculate the derivative and so pytorch has something really handy called autograd and autograd will do automatic differentiation so if you give it give it a method it it finds the derivative for you and so this is great because you can use it on stuff where you don't even know what the derivative is and so the approach that I'm about to show would work for I think almost any optimization problem and so here we're going to have to use variables for this and with so again kind of the variable has the same API as a tensor but it's got this memory of how it's how it's created which is then can be used to automatically calculate the derivative and you'll want to pass requires grad equals true and so you're saying this this needs a gradient so here we create calling them now pw and pH and so then down here with our methods notice we no longer have a grads method because we're not having to give it a formula for the gradient we do still want a penalty term so that's kind of taking our matrix and checking that it's less than zero or sorry if it's if it's less than zero we're taking its value otherwise the value is zero or yeah we're maxing it at zero and then in this case we ended up squaring that to add and I we probably should have done this in the methods above to add a greater penalty because you would see above we were still kind of getting a lot more negative numbers than we would like yeah you can have more complicated formulas without the downside of knowing that you'll need their their gradient let's see so report is basically the same again again we're having to use the dot CUDA to tell PyTorch put this on the GPU and then here we're going to use an optimizer so this is we're kind of telling it this is how I want you to optimize things and Adam is I think from a few years ago it's a relative it's a modern optimizer as opposed to you know SGD which has had this more classic classic technique give a learning rate we can still get our report and so so this um this code here this for loop is really useful this is something that you could use with a lot of different problems we just up pytorch requires you to set your gradients to zero kind of manually with your optimizer so our optimizer we're saying the gradients are zero they calculate the loss and this is a pytorch method backward and kind of taking a backwards that calculates your gradient so it's kind of like we actually this kind of surprises me because so we're doing our optimizer step I would think you would want backwards afterwards no so l equals loss calculates the value of the loss l is now a variable which is remembered how this actually multiplies the weights and just to mention Adam actually is a type of SGD so yeah Adam is stochastic but it's a much fancier version and there is a version of Adam in that excel spreadsheet if you wanted to look at the details and again we're going to print out the report every every hundred steps the other thing we're doing is every hundred steps we're decreasing our learning rate this is called learning rate annealing but you see inside this nested loop we're doing learning rate times equals 90% so it's getting a little bit smaller each each hundred steps and so again yeah you want to calculate what your loss is get the gradients and then use that to take a step but note that these backward and step are kind of built-in methods you're getting from pytorch we can check and we still have great topics so space NASA shuttle launch orbit lunar moon data well we do have this one one random topic that we've been seeing we saw with the SVD as well yes so with with so the idea of decreasing it as you go is that you're getting kind of as you get more in the neighborhood of where you want to be you want to take smaller steps so you don't overstep where you're going with stochastic gradient descent remember that your your directions are less accurate since you've just used a subset of your data so there's kind of more of a risk of overstepping in the wrong direction and it's okay that you know you're you're always going in slightly wrong direction but it's close enough yes learning rate annealing clear the page I was just gonna show that like if this was the true direction of that you want it to be going in with stochastic gradient descent kind of what you're doing is you're going close to to that direction but you're kind of zigzagging a bit around and so you probably want your zigzags to get like a little bit kind of tighter here the end as you start getting close to your goal so now I want to kind of compare some of the approaches we've taken to talk about or first are there any other questions on on pie torch or kind of what we've done with this automatic differentiation okay so using psych hits like it learns built-in and a math and that was fast we didn't have to deal with tuning parameters which was nice so you know these later ones there was this question of do we need to adjust our learning rate do we need to adjust kind of how much of a penalty you were putting on the the negatives but it used a lot of academic research kind of the people that implemented it and this is interesting this is from another library that is specifically a live Python library for non-negative matrix factorization but it's really kind of neat they show a list of like variations on NMF and like relative research and there's this is still an area where a lot of research is happening and there's a lot going on and so unless you want to specialize in NMF this may be more detailed than you would want to go into to be able to build something like this using pie torch and STD it was nice that it was much quicker to implement we didn't have to be kind of particular experts on NMF the parameters were kind of fiddly and it was also not as fast as scikit-learns built-in one but we did see that kind of going from numpy this should say numpy was slow enough that we wanted to switch to pytorch to get the improve improvement on the GPU questions about this oh yes actually hold on a moment the gradient good question oh I'll start the next section although we'll have to yeah we'll revisit this next time yeah so that was that that was NMF now we're going to return to SVD so we saw that something that we were doing an NMF that we weren't doing before with SVG is we were choosing D kind of a number of topics that we wanted and so going back to these matrices so we had this matrix that is words by documents and actually let me go back up to the NMF picture that is up here okay this picture so words by documents then we're getting a matrix that's words by topics and then this is topics by the importance indicators for each document so it's basically topics by document and we were getting to choose how many topics there were and so we can get that with SVD as well and it's called truncated SVD so instead of getting this exact decomposition we could just choose okay I'm gonna have r smaller than n and that's the number of topics that I'm interested in and remember that the singular values which are the diagonal values in this matrix Sigma in the middle that those are ordered from largest to smallest and the larger ones kind of are contributing a lot more to this reconstruction of a so we're gonna keep the larger ones and this is called truncated SVD and then I also went at this picture I've taken is from from this blog post on the Facebook research page on fast randomized SVD and we're gonna be talking about some of the ideas in it below but I encourage you to check that out so some of the shortcomings for classical algorithms our matrices are really large data is often missing or slightly inaccurate and why spend extra our computational resources when kind of if you're you know if your data was not that precise to begin with you don't need an exact solution because it's not a you know you have these errors in your input data transfer now plays a major role in the time of algorithms so we talked about this last time that traditionally this idea of computational complexity or big O has been how speed is measured and it's still definitely an important concept but in practice a lot of time is spent taking things kind of from from disk or even from RAM into into your cache or registers and you really want to take advantage of that and so techniques that require fewer fewer passes over the data can be substantially faster even if they technically have more steps in them we also want to be able to take advantage of GPUs and then a lot of the methods that are used on sparse or structured matrices are unstable and we'll kind of be getting to prylov subspace methods later but the computational cost ends up being kind of more about what you're doing to stabilize your algorithm as opposed to kind of the actual algorithm you're doing itself and these have shown a highlight a paper that we will be seeing a fair amount of and that is a really nice paper is Halco finding structure with random randomness and it's about kind of using probabilistic techniques yes that's a good question this is more so it's this particular family of methods crylov subspace methods which are really useful well I guess okay so there are issues of when sparse things are kind of becoming dense I think this is less about the sparsity and kind of more about the methods that are being used to introduce this instability and we'll see examples of crylov subspace methods later but yeah it's not it's not so much having to convert between and actually working well depending what you're doing working with sparse there are kind of like implementations that handle sparse data format very efficiently so it's not it's not a problem to be storing things sparsely yeah so I just want to kind of introduce this idea of randomized algorithms maybe is they're kind of more stable their performance guarantees don't don't depend on kind of these matrix properties so the spectral the spectral properties of a matrix are kind of based on what the singular values are and then a lot of matrix vector products can be done in parallel and so we're about at time I'll come back to these ideas on Tuesday and so we will be using a randomized algorithm to calculate the truncated SVD more efficiently okay great thank you

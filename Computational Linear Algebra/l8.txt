 I'm gonna go ahead and get started. I wanted to just briefly return to the problem of the compressed sensing of reconstructing what the CT scan had detected, and so kind of thinking about Tim's question from last time of how would we represent this, so you know kind of as linear regression. So here if linear regression is matrix X of data times a vector beta of coefficients equals some vector Y, here X is the projection, they call it the projection operator, and it's this matrix as you can see is black and then has some some white lines that seem to have a little bit of a pattern to them. Does anyone know where these are coming from? Tim? Um close, so you're actually getting a little bit ahead of this, so this is just the matrix X, but we'll get there soon. Matthew? So it's actually not the readings. So what you're getting at, I'll actually skip ahead, is the vector Y, so here the density readings. So yeah this is kind of the measurements that the CT scan was recording, and what we do to get our vector Y, so this is the right-hand side of the equation for the linear regression, is we're just unwrapping this or raveling it as the NumPy operation to get this really one and a tall vector that's 2,000 by 1. That's Y. Any other guesses of, I know this is kind of a tricky question, what this X could be? Matthew? Good catch. Not yet, so that's that's why kind of the reading from the different angles. Then Sam behind you has a guess. Is it the total intensity, like before when we looked at the projection operator at just one slice, there's a line going through a space, but now are we looking at, like is the X dimension, the position, and Y is the angle, and then the dot is like the length of the line almost? That's very close. Yeah, so let me kind of go back up to, it was this section where we had these pictures of lines at different angles in different locations, and so this was part of a, so this was originally part of a four-dimensional matrix where we had the different angles and the positions, which is the vertical position of how high up the line is, and then an XY coordinate for each, and so here we were kind of just showing pictures where we'd index on the first and second dimension, so the angle in a vertical position, and then see the XY for all these different lines, and so this mystery matrix X that I was showing you is actually the collection of all of these, so this is kind of what the x-rays look like before they've intersected with anything, and here we've kind of unwrapped them, or not unwrapped, reshaped them, so we took this, this was in 40, and we said look, okay, like let's make, I think the x-axis is the angles and positions, and then we've unwrapped each picture to be just a single long row, and so that's why it has the appearance that it does down here of like these do contain little lines, but they've been unwrapped. Yes? Oh, could you throw the catch box, Sam? This might need an intermediate, oh, good throw in, good catch. Yeah, so the the y-axis here is the angle and the vertical position, and then this one is the X and the Y, correct? Well, so what 2Dense is doing is we're storing this as a sparse matrix. Yes, yeah, and actually kind of going off of that, why would it be a good idea to store this matrix as sparse? Anyone can answer this? So Valentine said because it's sparse, and that's correct, yes, so we see we see a lot of them black here which represent zero, so the white is where they're ones, and that's a relatively small portion. So we've got this stored as a sparse sci-fi matrix, and we're just converting it to dense so we can look at pictures of it. So to kind of actually do the math, we don't need it to be dense, it's more just to be able to look at these pictures. And I also just wanted to highlight that this matrix is a lot wider than it is tall, and that's because the width is L by L, which is 128 by 128, and we only had, so we have 128 vertical positions, but only 1 7th of that number of angles, which is 18, so that's why it's much shorter, is we have a kind of smaller number of angles. Any other questions about this this X matrix? Okay, so let's move on to beta. So beta is what we're solving for with the linear regression, and that's the image, so we're trying to kind of recover the image from, you know, given knowing just knowing what angle, what position the x-rays are coming from, and knowing what the outputs are in terms of these kind of measurements of density that were picked up by the CT scan, we're trying to solve for this image, which we wouldn't actually have. And so in order to do that, or kind of to express that as our linear regression, we have raveled the image from a 2D array into a 1D array. So you'll see this is just this very tall, over 16,000 length vector, and it consists of just a little bit hard to see, but it's mostly black with a few, some white spots, which fits with this picture being mostly black background and just a little bit of white. And so you can can see that how the kind of the measurements, how they were gathered, were basically just by taking these x-rays and multiplying them by the image we were x-raying, only as a vector. Any questions? Does this help to kind of have it as a more standard linear regression right out of matrix times a vector equals a vector? All right, and then just to review, what penalty for our linear regression gave us the best answer? And I hear mumblings, but I couldn't hear what was said. Yes, L1 is the correct answer, and that's because the L1 norm induces sparsity. And so we saw, I'm going to briefly show those again, that when we did a linear regression with the L2 penalty, we don't get a very good answer. However, doing it with the L1 penalty gives us almost exactly the right answer. Any final questions on kind of notebook 4 compressed sensing? Okay, head back to notebook 5. So we talked about this some last time, but I want to review some of what we saw. So we're looking at this data set of diabetes data, and we've broken it into a training set and a test set, which is really important so that you don't overfit. Our training data is 353 rows, those are the measurements, times 10 variables. And these are often called features in machine learning. We did a linear regression in scikit-learn, just using, is this large enough? I should probably make this bigger. Yeah, so we just use scikit-learn linear regression. We get a prediction, and so here kind of as a starting point, our L2 error is 75, our L1 error is 60, so not a great fit. And we talked about one way to try to get a better fit would be to add polynomial features. And so kind of linear regression for, here I just did a problem where you had three features, x0, x1, and x2, you're trying to find the best coefficients beta naught, beta 1, beta 2. And adding polynomial features just means taking, still have x0, x1, x2, and then adding x0 squared, x0 x1, x0 x2, x1 squared, x1 times x2, and x2 squared. Why do you think this is still a linear regression problem, even though we have squared terms in here? Wait, Linda, can you throw that catch box to Connor? Exactly, the coefficients are still linear. So we're just solving for the betas, and so we're now solving for eight coefficients. And so this is a way to kind of create a model that's going to capture a little bit more complexity in case their interactions between x0 and x1, or if there's a squared relationship between how the dependent variable, which in this case is I think like a long-term health outcome for the diabetes patient, how that depends on your inputs. But yeah, this still, you kind of calculate these additional features and then you still have a matrix of values that are linear with beta. You just have more coefficients you're finding. Questions about the setup of using polynomial features? All right, so we started off Scikit-Learn has a polynomial features method that will create them for us. So we did that to get some additional training features, which it's printed out here. And in our case, since we had ten, ten features that we started with, we're getting back, we're now up to 65 features. And so it's just calculated all the possible combinations. So H squared, H times sex, H times BMI, blood pressure times serum level 2, but just any possible combination. So this reduces our error when we run linear regression again with our kind of having more features. The only issue is that time is squared just to even create these features, and so this could possibly get slow as we have a larger and larger data set. So we saw Numba last time, and to make it clear Numba is a compiler and it is a Python library that compiles code directly to C. I wanted to highlight, so I think Jake Vander Plaas has some really nice, it's kind of like a nice introduction. He also has a later blog post, let me open this, where he tries to do a, or not tries, but does a non-trivial algorithm, which I thought was nice since often we kind of see these test cases on simpler algorithms, which is what I gave you, but so I definitely recommend checking out this this blog post. And then I wanted to highlight a quote of his. This is from his kind of intro post. He says that, so for this problem he does, Numba is a thousand times faster than a pure Python implementation and only marginally slower than nearly identical Cython code, and Cython is another Python library slash compiler, you know, that compiles directly to C. And he says that he has years of experience with Cython and only an hour's experience with Numba, and he was using every optimization he knew for the Cython variation and just the basic vanilla syntax for Numba. So I think this is a really strong endorsement of Numba over Cython, that it took less knowledge, and I've only used Cython a little bit, but this was my experience that there was more more to learn about Cython and it felt closer to C, whereas Numba is very simple to use, and I saw kind of similar results for the problem that I, problems I've tried it on. And again, Cython and Numba are both, both ways to compile your Python to C for additional optimization. So I wanted to walk through kind of our example again that we saw last time. So we have this Python algorithm that is bit contrived, and we'll be using it for polynomial feature generation though next. This is just as an introduction, and so we're going through this for loop with kind of doing a number of operations with X and Y to calculate a new vector Z. We time it and it's 49 millisecond, yeah 49 milliseconds, and then we use NumPy to vectorize it. So here we've taken out the for loop and are just acting on the vectors. So you can see this pure Python got a for loop where we go through each element of the array, and then NumPy has this built-in vectorization where we can just treat X and Y as vectors and do our operations on them. And this is a lot faster. This is 30, 35 microseconds. So again, we've gone from milliseconds to microseconds, which is a thousand times speed up using NumPy. And so what's a, what's a downside to this NumPy approach? Any ideas? Sam? Oh wait, Connor threw the catch box. Thanks. Yes, poor localization. You want to say more about what that means? Yeah, instead of doing one operation on a single value from the X vector and a single value from the Y vector, it's going to read in all of X and all of Y at every stage and process. Yes, exactly. So that I can give this back. Thank you. Yeah, so as Sam said, what's happening here is that it's gonna read in all of X and Y, and if these are very long vectors they might not all fit in cache. And so that would mean it's reading in the first part of X and the first part of Y, doing this computation, then reading in, you know, a later part of X, later part of Y, still on the same line, you know, doing this X squared minus Y times 55 computation, and then another section of X and Y, and then it goes to the next line and it has to do all that again. You know, it has to read in the first part of the vector X, the first part of the vector Y, read in the second part, and so on, if the vector, you know, is large enough to not fit all in all in cache. And so, yeah, as Sam said, that's poor localization and can be more time consuming. So then we get Numba. Numba offers several different decorators and we're just going to talk about two of them. One is at JIT, which stands for just-in-time compilation. It's a little bit confusing because I think all of Numba's compilers are just-in-time, but this is kind of the most general one. And then we'll also look at vectorize, which doesn't require us to write a for loop, and so that's really useful when operating on vectors of the same size. And what vectorize is doing is it's automatically writing something called a numpy u-funk for us. So a numpy u-funk, that's a, I'll pull up the the page, and it's a universal function. And that's something that's a kind of like lower level within numpy and more of a more difficult to write yourself where you are writing in C to be creating those. But Numba will kind of create those for you if you use that vectorize. Yeah, we'll start though with at JIT. So you just put the decorator on the line before, kind of before you define your method. We still have our for loop. We're going through, and now when we time it, it's six microseconds. So again, just to remember, the numpy version was 36 microseconds. So this has been a six, six times speed up over numpy, and numpy was a thousand times speed up over pure Python. And then with at vectorize, this is kind of the same as the at JIT version, only we don't have the for loop. And there we get five point eight two microseconds, which is kind of almost the same, although I think this is cleaner to read because you don't have the for loop. However, if you were doing something where you weren't acting on vectors of the same size to kind of get back a vector of that size, at JIT would be good to use. So I just want to say numpy, or sorry, numba is amazing, because this is really fast and fairly simple to write. Sam? Yes, and so actually a ufunk, so really what's at vectorize is doing is creating this numpy ufunk, which is required to have broadcasting for you. So yeah, this would have broadcasting. And that's a benefit of numba is you're not having to like code the broadcasting yourself to create the ufunk. Correct. Yes, and that's because numba is kind of has smarter optimization than numpy. So the issue with numpy is it's not able to look ahead, like numpy is still going kind of line by line through this algorithm. So it's basically kind of completing, you know, x squared minus y times 55 before it even looks at this like, oh now I have to do something with x and y again. Whereas because this decorator is like for the whole method, numba is optimizing kind of that whole method as a unit. So it's got a little bit of, you know, this like bigger picture of like, hey I'm gonna be using x and y again on the next line. Any other questions about this? All right, so let's, oh, oh wait, wait for the microphone. That's a good question. Numpy is gonna have a lot more specialized methods written, like numpy is a much larger library. So I think it's possible that if you are doing kind of like some fancy specific numpy method that numba may not have that written. Right, yes, yeah, yeah, and you can use them together, yeah. Yeah and I I really feel like this is something that numba just doesn't seem that widely known to exist, but I think it's a relatively simple tool that gives you a nice speed-up. I'm not sure about that one. And the other, I want to say a numba has more functionality that I'm not getting into, but numba gives you the option of explicitly passing types or like explicitly setting like this is what the types of the method will be. Okay, so let's go back to the problem we were interested in of polynomial features. So we're adding our at JIT decorator and here, and actually let me go to, um, this gives you different, different options. And no Python disables the use of pi objects and Python API calls. And there's also, I'll show an example of this later, but there is also, you could pass the types in, I believe, here if you want to say explicitly what kind of types the method's taking. So I wanted to just kind of show the punchline of when we use, we use this method, our polynomial vectorization, the time is seven microseconds. And so to compare that to what we were getting above with the scikit-learn of polynomial features, we were getting 600 microseconds. So this is about a hundred times speed up, which is pretty impressive because everything at scikit-learn has been kind of written by experts and pretty heavily optimized. And so here, I should maybe go through a little bit. Here basically what we're doing is just kind of looping, looping through our, our features and taking all possible combinations, multiplying them together, and kind of in order to increase our number of features. And this is just adding something, kind of adding all the squared terms. Okay, yeah, so, so a big speed up there. And then something I skipped over, let me go back to this, is I basically I wanted to talk a little bit about row major versus column major storage, and I found there's a nice blog post on it. And so the idea of row major storage is you're storing things by rows and column major you're storing by columns. So here kind of first column, second column, third column. And so it's important to know, or it can be very important to know how your data is stored, you know, and and in many ways it doesn't matter beyond just knowing because it makes it easier kind of to access your data. If you're using column major, it's easier to access it by column. If you're using row major, it's easier to access it by row. Actually I should show so here with row major storage, if you wanted to access the first column, you would have to kind of grab this element and then this element and then this element, you know, which are not not contiguous in memory. Whereas column major storage, it's easy to get a particular column because they're all next to each other and so it'll be quicker to access, but vice-versa that it's easier to grab a row from row major storage, whereas here if you wanted the second row, you have to kind of jump around to grab the elements. So this somewhat unfortunately is that this is not consistent between languages and so FORTRAN is column major layout and it was kind of the first major language to use column major. And then MATLAB, R, and Julia also use column major. C, C++, Python, Pascal, and Mathematica all use row major. And this really just kind of matters when you're writing algorithms. If you're using column major, you probably want to loop by columns. If you're using row major, you want to loop by rows. And so NumPy gives you the functionality to switch between those. So there's this as FORTRAN array to convert it from row major to column major. And so we're using that here. That's kind of the logic behind that. Questions about row major versus column major? Okay, so here we've converted our train and test because that's what we need for this our polynomial vectorization. Since we're, you know, taking columns of data and multiplying them by each other to create our new features, we want column major layout. And so again this is this is giving us a big speed up over scikit-learn. And this is a kind of real problem we might want to solve to to improve our linear regression. Alright, so then as a final step we want to add some regularization to our model to reduce overfitting. So here lasso regression is what uses an L1 penalty. We're gonna try using that here. And I've linked to, there's a Coursera video on lasso regression if anyone wanted more. And at this point we've already seen the L1 penalty a few times, or at least two times. Can anyone remember what the two times we've seen the L1 penalty is? Someone raise their hand and get the microphone from Sam. Exactly, yeah. So robust PCA for the background removal of the surveillance video and then for the CT scan compressed sensing problem. Yeah, so here scikit-learn has a lasso CV. We're not going to get too deep into, have you have you all seen cross-validation in your other courses? Okay, great. I see a lot of nodding heads. And that's just kind of comparing different parameters. And so here, so alpha is the weight of how much weight to put on the air and this will compare ten different alphas and tell us what the best the best one is. So we do that and we get an improvement in our regression, our metrics. Again, this is the L2 norm and the L1 norm. Those are now down to 50 and 40. And so kind of when we ran this cross lasso cross-validation we were getting back a model that had alpha set to you know what the optimal one was. And this does have additional parameters like if you wanted to set yourself which which alphas you want to try. And finally we're gonna add some noise to the data. So we do that here and this can be another way to try to kind of reduce our overfitting. Then I also just wanted to kind of briefly show you the the Hoover loss and that's a yet another loss method, loss function. I mean it's less sensitive to outliers than squared error loss. Basically what it is is it's quadratic for small error values and then linear for large values. So this is what this method is doing. It's saying for X less than a certain value this is going to be quadratic. For larger X this will be linear. And the use of the 1 halves are to make sure that it's differentiable. So even where it meets up at point delta it's still differentiable. So we tried to try using that that or actually so I guess here this is kind of illustrating you know if we had this had noise our method is giving us a worse error but we can kind of improve it by using a better loss function. Any questions Roger? And Tim can you throw the microphone? So here oh that's a great question. Really that would be the error. So that would be the difference between the prediction and the actual value. Good question. Any other questions? Matthew? And can you throw that microphone? That's yeah that's that's a good question. I think some of it also depends on your problem like if you think that outliers are kind of less important it could be good to to use the Hooper loss method. Yeah choosing a loss method in general is kind of involves some of kind of like your perspective on the problem. You know so like with the going back to the CT scan we could like look at the pictures and know like okay this L2 picture is not what we want. We know that we're supposed to be getting something that looks more like the picture getting from the L1 but yeah I think they're kind of it requires some domain knowledge about your problem or what you're what you're trying to find. Yeah it's tough because you know each like the loss method itself is what's saying a good solution is but you have to still choose which loss method. Thanks. Any other questions? Yeah we're a little bit early but I think this might be a good time to stop for our break and not not start the next notebook until after that. Yeah so let's meet back here in eight minutes so that'll be 11 11 56. Okay I'm gonna go ahead and start back up. So yeah I just wanted to kind of say the key takeaways from lesson five are kind of one you know polynomial features as relatively easy way to try to kind of improve performance while still using linear regression and you know let you capture kind of this interaction of how how features interact with each other and then number as a is a great way to speed up your code. Any questions? Let's start start on lesson lesson six how to implement linear regression. So we've been using scikit-learns implementation and now we're kind of gonna go a level deeper and think about how would we implement this ourselves. First so we're still going to be working with the same same data set as before the diabetes data set. First let's look at how scikit-learn did this and so you can check the source code and it's really handy actually I didn't go there but on the scipy documentation so let me show this typically when you search for something you know you're interested in scikit-learn least squares. Actually I should probably do scipy and it's kind of nice on the documentation there's always a link to the source code and so I think this is can be really helpful for getting more more information about what you're doing. Let me just go back here to make sure. So here I wanted to look at scipy linear regression that we've been using and see kind of what is it what is it doing and then we'll be there'll be stuff you can kind of skim through more where they're just you know like maybe checking the inputs or creating an object. And here the interesting part to me was I was saying okay how did they get the coefficients and here they're handling the case if it's sparse or not. We were working with a matrix that wasn't sparse and we see it's calling linouch.leastsquare so then we can go there and just confirm that they are importing linouch from scipy. So we want actually, sorry, they had called it LSTQ, right? Yeah, linouch.lstsq. So come here to the documentation and then into the source code. Then we scroll down and so we're kind of getting a hint here that so they're using a law pack driver. Actually can anyone remember what is law pack? The linear algebra package and also known as B pack and MKL, the most known name. So close. Does anyone want to elaborate on that or clarify a little? Yeah so law pack is a low-level linear algebra library and it's using BLAST which is kind of even lower level library for basic matrix computations. So BLAST is gonna have things like this is how you do matrix vector multiplication, this is how you do matrix matrix multiplication, and then law pack is a layer above that and is kind of has you know these decompositions we've seen like this is how you do SVD or QR and this is something that has been heavily heavily optimized. And law pack I believe came out in the early 90s so let me bring up the notebook just to make sure I get the facts right. Law pack has kind of been optimized for each computer and so no libraries like numpy are calling law pack and pretty much any scientific library in any language is going to be calling law pack at a lower lower level. Just briefly bring that up. Okay I'll come back to this. Sorry this is somewhere in in this lesson. I have a little bit more detail about it. Oh you know what it's up in the matrix computation section. Okay sorry about that I'll find that and show that next time. But yeah law pack low-level library being called by pretty much all scientific computing libraries. So yeah we were back here in the scipy source code. It'll become important that kind of that taking one of these says law pack drivers GLSD GLSY GLSS and it's gonna call them yeah down here it's kind of is checking for the driver and then calling that from law pack. And so kind of the information we have is that options and this shows up in the comment at the top the options are GLSD GLSY and GLSS. The default is GLSD however GLSY can be slightly faster. GLSS was used historically. It's slow but uses less memory. And so this can be kind of helpful to read and this is something that we could override if we wanted kind of still wanted to be using scipy's least squares but wanted it to use a different method at this low level. And so this is also kind of I think interesting to see that scipy has you know at least two two main options. You know it's not using the same algorithm necessarily all the time. And then just here I did not get into the details of this of this at all. You can kind of just get like the very basic kind of oh this is computes the minimum nor solution to a linear the square problem using the SVD decomposition and a divide-and-conquer method. And so the key thing here is I didn't want you to get into the details at all but I do want you to feel comfortable looking at scipy source code and you know particularly at least like reading the comments in there because they can be helpful. Yeah just to show that there are these three different methods that could be happening under the hood. And then I glossed over this but we saw that it was you know had this if statement if is sparse call these methods if not call these other ones. We're not going to dwell on the sparse one at all but I wanted to say that scipy's and then this was happening at scikit-learn it was calling scipy sparse if it's sparse, the scipy lin-alge normal one if it's not, that it's using yet another method for the sparse ones. And so that's something that's typically hidden from you when you're just using scikit-learn's linear regression that these different things could be happening. So lin-alge dot least square here I've tried calling it and overriding so passing in la pack driver GLSD Y or S and comparing the times on those. And so I got that GLSY was it's about twice as fast as GLSD and notice it defaults to GLSD so that could you know in this case maybe you would want to override it and use GLSY but it's only a 2x speed up. And I actually don't know what's going on here but it's kind of interesting to me that the GLSD is slower than GLSS which is what it had said is like the older version that's generally slow. And these things would all depend on what problem you're doing so if you did this on a different problem you might get that a different method was faster. Alright so now we're gonna kind of step back so that was still using SciPy's implementation. We'll step back and kind of remember the definition of the least squares problem is we're wanting to minimize ax minus b and trying to find the best best values for X to do that. Another way to think about this in linear algebra is that we're interested in where vector b is closest to the subspace spanned by a which is called the range of a. And kind of think of this as the projection of b on to a because if you think about all possible vectors x multiplying those by a give you the range of a you know any possible yeah any vector that could be in the range and you're trying to find the one that's closest to b. And so that that can be expressed as b minus so b minus ax must be perpendicular to the subspace span by a that's kind of getting from b to ax since it's a projection would be the the perpendicular and let me um maybe I should go to the drawing pad for that. Oh I actually wanted the other one. So if we think of ax really we've got a plane where this is a times x for all X's will give us some you know two-dimensional plane and then we've got some so that was a bad angle some vector b we're trying to find the X to minimize a X minus B. And so that's the projection of B onto onto a X and that would yeah we could write oh let's write this as a X hat is in that plane and so then this is where so this line here you know is B minus ax hat and that must be perpendicular to a. This is the whole plane and I'll show I'm gonna show another three blue one brown video in a little while so this is kind of getting back to a more geometric way of thinking about matrices and vectors. Any questions here? Okay so going back to the notebook you know we end up with a transpose times B minus ax hat must be zero and this is where the normal equations come from and this is kind of the closed form solution for linear regression least squares problem. And so we could we could do this I call this least squares naive and this is an approach that you would not actually want to use in practice because you're taking a matrix inverse which you generally want to really want to avoid doing but I just kind of did this as as a starting point of this is something that would work assuming I guess it would work assuming that a is not something we're gonna end up dividing by a number close to zero to get its inverse. Matthew and who has the microphone? Nice. Yeah so it's both it's both computationally expensive and it can also be unstable because you end up dividing by things. Yeah good question so here this was just kind of a first stab at implementing something that would give us an answer for linear least squares regression. Alright so then I'm coming off of that so then this these are called the normal equations this equation we got above and really this is that this equation has been rewritten and we're just taking a transpose B or I guess we're taking a transpose ax to the other side so we have a transpose ax equals a transpose B and and so if if a is full rank a transpose a is gonna give us a if a is square well actually not even square if a is full rank we're gonna get a square Hermitian positive definite matrix for a transpose a and square Hermitian positive definite matrices have something called a Cholesky decomposition and the Cholesky decomposition or Cholesky factorization finds an upper triangular matrix R such that R transpose times R equals your matrix and what would our so if R is upper triangular what would our transpose be yes lower triangular that's correct not all questions I ask are hard so so what we're doing so in this case we're looking at a transpose a we're gonna take the Cholesky factorization and that'll give us back R such that a transpose a is R transpose times R a transpose times a equals R transpose times R here I've just pre computed a transpose a since I'm gonna be using it a few times as well as a transpose B and then I'm gonna use sci-pies implementation of the Cholesky factorization as a warning and numpy and sci-pi default to different one of them returns the upper triangular matrix and one returns the lower triangular matrix which I discovered the hard way I didn't realize I was switching between numpy and sci-pi and was like using this and other operations and getting different answers but yeah so we can and so it's always good after you do this to kind of check that what you're doing makes sense so I'm passing a transpose a in and actually let's even look at oh okay that's difficult to see so remember an NP dot set print options is super useful you can do suppress equals true to not show zeros in this format with an exponential 0.00 yeah 0 0 which is not very readable and then you can also set the number of digit digits you want to see although this is a matrix I guess is still kind of too large to be that that viewable but I can believe that this this looks like an upper triangular matrix that I've gotten back and then I'm just confirming that a transpose a minus R dot transpose times R is giving me something close to zero so a transpose a is approximately equal to R dot transpose R and so then the way that the Cholesky factorization is useful in solving the linear least squares problem is so we can plug it in now we have our R instead of a then we have our transpose and here I'm calling our X saying that that equals W you can see this last line setting that equals to a transpose B and getting our X equals W so why is that why is this helpful right like I've just kind of written this equation a few different ways why might this be better than just sticking with our a transpose a X equals a transpose B oh wait Matthew okay throw it so now it's much easier to solve the last equation exactly exactly yeah so that's it solving linear systems of equations for a triangular matrix is a lot easier because you just start with the last entry you know divide by one thing then you plug into the second to last entry and so on so it's much simpler than solving a linear system where you have a non triangular matrix and so we've kind of just changed this into having to solve two triangular systems of equations so we solve I guess we would solve this one first to find W and then we solve the one on the last line to find X which is our original answer this I think kind of illustrates some a lot of numerical linear algebra is about getting these factorizations and often it looks like okay you've just rewritten the problem you know you still have all these factors but it's because of the properties of the matrices you factored into you've made things easier for yourself in some way any questions about this there are questions about why it's easier to solve a triangular system of equations okay so yeah this is just kind of rewritten the least squares linear regression problem so here kind of working our way working our way towards X first we're going to solve the triangular system our transpose W equals a transpose B which is what's happening in this line and so sci-fi Lenouge has a solved triangular method even where you give it a triangular matrix and that has a transpose argument so here we're letting it know this is our transpose again we're kind of checking that our result is what we were hoping and that our transpose times W minus a transpose B is close to zero so we've successfully solved this third equation and then from here we solve triangular R with that W on the right in order to get X which I called a coefficients Tolesky here and we got that that works and so I can put this all together in a method and say that solving least squares with the Cholesky decomposition takes in a and B gets the the Cholesky decomposition of a transpose times a and then it solves those two triangular systems and returns the returns the answer any questions Kelsey and throw the microphone yeah that's it no that's a good question so I think here I have to look this up that's probably the cost of calculating the Cholesky decomposition and so I'll check on that with you I think it's also to remember the the naive way you have this instability that comes with getting the inverse so that's kind of less good in that regard yeah I'll check about the time yes yeah I'm sure that it like this is small enough that I think constant terms matter more so when you get to you know like really huge matrices it does come about become more about like the highest order term but yeah the size could matter correct so we'll come back to kind of a comparison so I'm gonna kind of show you several different ways to do it and then we'll like compare the different methods we got good questions any other questions about this approach okay and then you know I think I brought this up last week when I was looking at the history of Gaussian elimination but the Cholesky decomposition is the one that Gauss first found that Cholesky got the credit for oh yes hey you can always walk it to yeah so um I see this right back up we actually get two different and he got Indian norm we have different value here why is 1.13 why is 6.86 so what has changed these results so these are so close to 0 that it doesn't like here I'm mostly trying to confirm that this is a number close to 0 and I'm interested in the exponent and wanting it to be you know 10 to the negative 14th is close to 0 although I guess that is a question why aren't we getting exactly 0 if this is a way to decompose and get the answer all right I mean does anyone have a an answer to this on why we're not not getting it but yeah because there's a so machine epsilon because we get rounding errors and with floating-point numbers we're trying to you know represent these continuous numbers with discrete values and so when you bring up gradient descent are you talking about like a stochastic gradient descent approach so stochastic I actually originally have had stochastic gradient descent in this lesson with pi torch and it was actually much slower than any of these linear algebra approaches and so I took it out because it just didn't look reasonable compared to them a benefit so yeah STD will often be slower than if you have you know kind of like a well-studied method for your particular problem but a benefit of STD is that it's so general and so if you have a problem where you don't have a closed form solution or you know people haven't studied it to come up with this optimization method you can use STD there so kind of the benefit of STD is yeah that it's so general and it'll work on problems where we don't have closed form solutions Connor and can you throw the microphone or pass the microphone yeah no and that's quite possible to yeah that a certain size it might be worth it yeah and that's important note I think with all of these of what algorithm is best can really depend on your problem and we'll see that later and so different things like both the size of your problem kind of properties around the stability of your problem yeah we'll see later two methods that kind of are to two particular matrices that don't work well for a particular method yeah so there's a lot of variation with these these are good questions any other questions about Cholesky factorization before we go on to our next next approach all right so next up is the QR factorization and this again next week we will actually talk about how to code a QR factorization we're still just using numpy's and someone remind me what what the QR decomposition gives you you know although you have the right words in there but I think Kelsey has it has a guess I believe so if it's square they would be orthonormal be I guess for the non-scar case images I would have to check yeah so Q is orthonormal R is upper triangular thank you and where did where did we see the QR factorization before um I'm not sure that we saw it with NMF yeah so inside of randomized SVD we saw it okay so here so kind of getting into these equations and we got next week we'll talk about how to code a QR factorization ourselves so ax equals B we're going to get the QR factorization for a equals QR plug that in and then say our X equal equals Q transpose B and why why was I able to go from Q on the left side to a Q transpose on the right side yeah so a few people shouted this out Q's orthonormal so multiplying by Q transpose we get the identity on the left and that's also nice because you're not having to calculate an input inverse you're just transposing the columns and then why is it nice to have our X equals Q transpose B he grabbed the microphone exactly yeah so now we have a linear system of equations with a triangular matrix that'll be fast to solve so here we do QR equals using side pies QR factorization and then we get a triangular system to solve we solve this and all right we're getting the same air kind of that we've been getting questions about this approach with the QR factorization and why is this one even slower the question is why is this one slower let's actually save the time in comparison ones for a moment but yeah well we'll come back to that any other questions before we start our next approach okay so now yet another approach we're gonna look at SPD and so again we're still trying to solve the same problem of a X equals B and finding the optimal X now we're gonna use the singular value decomposition can someone remind us what what the singular value decomposition is you compose the matrix into a product of u sigma v transpose it contains a block of a square matrix where the diagonals are exactly yeah so Sigma is a diagonal matrix everything's zero except the diagonals and those are the singular values and you and beer orthonormal so they talk about I didn't cover this in class they talk about like a full SPD and also a reduced SPD and one of them yeah and one of them it's not square but you're just adding a bunch of zeros to get the dimensions to work out yeah and actually I guess I think for the reduced one really it's just that you has orthonormal rows and B has orthonormal columns and then for the full version you're like making those expanding them so that they're full bases yes yeah I'll bring trepethin has a nice picture of this yeah yeah I'll talk about that next time full versus reduced for SVD okay yeah so that's the SVD which we've seen a few times before here we plug that in for a and then actually similar to what we did before we can multiply each side by you transpose since you transpose times use the identity because you has orthonormal rows we get Sigma VX equals you transpose B then we have Sigma W equals you transpose B X equals V transpose W and so this I actually think you should kind of be seeing some common themes between these three different approaches watch I guess first I should say what's nice about solving Sigma W equals you transpose B so anyone else want to answer and so here we've just said VX equals W and now we're going to solve Sigma W equals you transpose B you Roger Tim can you pass the microphone exactly yeah so solving a diagonal system of equations is even better than solving a triangular system because you just have a single non-zero coefficient for each equation so you can just divide through so that's very quick to solve and then we're left with well VX equals W since V is orthonormal we can rewrite that X equals V transpose W and then this is this is that in equation form so we're using psi pi's SVD here getting back use Sigma and I called it VH in this case then I see for W notice I'm just dividing by Sigma dividing you transpose B by Sigma and here I guess a you transpose is possibly larger than I need so that's why I'm chopping off the the end and then I'm returning VH transpose times W questions about this approach and some people are noticing that this is much so slower we've gone from microseconds to milliseconds which is a order of mag car thousand thousand time difference any questions or questions about the equations kind of how we we solved solve this hey and then he had another technique that I'm not going to get into detail at all on but wanted to let you know that it's out there is that there's something called the random sketching technique for linear regression which involves using a random matrix and computing SA equals SB and then finding a solution to the regression SA X equals SB so I just wanted to let you know that that exists as an example of a randomized randomized approach and this would be where where you had a very large A and B okay so now I think you will like this part we're gonna do a timing comparison and do it a little bit more formally so above we were just kind of printing out what the times were for the diabetes data set what I'm going to do here is randomly generate some data and M is going to be either a hundred a thousand or ten thousand and it's going to be twenty a hundred or a thousand you can see down here is where I generate the data randomly a has dimensions M by N and I'm going to loop through all possible values of M and N and then we're going to compare these different techniques so we've got the naive solution with the normal equations the Cholesky factorization the QR factorization SVD and then also sci-fi is least squares let me see if there's anything else about the setup Oh in sci-fi is least squares it returns several items but we just wanted the first one so that's why I've kind of wrapped it in this other method and here I'm using so I've got my row names and then I have a dictionary that converts the name kind of list what the method is that we've defined for that who uh who in here has used pandas before pandas well yeah okay so looks like everybody and I'm just using it here because the data frame is kind of a nice way to to store a table of information and I'm using the multi index which can be handy to have M and N so now I just have a nested for loop and this says kind of formally for the linear least squares problem you want M greater than or equal to N like you typically have more data points than you have features so I'm just looking at those cases what I'm going to do is loop through them randomly generate data of that size so with M rows and N columns and then for each method I am going to get the function I'm going to use pythons time it module to time it and what this will do is run the decomposition five times and track the speed of each one and average them and it's good to run it a few times just to kind of reduce variation some then for the data frame I'm going to set the value of that kind of that method for that row and column this is how long it took oh and then I'm also I'm interested in the air not just the speed so I've got the coefficients back from this would be and this is a little tricky so function is a string and so time it you actually pass a string to here we're using locals to kind of look up the actual variable with that name call that on a and B and then check what the air is using our metrics coefficients and in this case for simplicity I decided just to look at the L to air so that's what I'm storing in my DF air data frame so I've got this DF data frame that's gonna hold the time DF air is gonna hold the air and so I run that and that can be a little bit slow to run so I'm just gonna use the results I have here since it's doing kind of all these matrix decompositions and some of the matrix matrices are larger so let me see if this will fit on the screen when it's larger okay are you able to read this set this table all right we should put them both up so the top table it's like perfectly fits top table is the time it takes and the second table is the air it's been a moment just kind of looking at this and tell me what you notice which methods are working best kind of in which cases anyone want to share some observations okay and then microphones between Aaron and Roger throw it to Connor like across the board in terms of timing it does yeah yeah I know the Cholesky factorization does I thought surprisingly well and these are there are much larger matrices out there that we could have done this on yeah there are observations I have to say I was I was personally surprised by how slow SVD is what do you notice about the air yeah it's the same in all of these so that's reassuring so here this problem is not not one or we're having to worry about kind of stability with different different ones Matthew paper the microphone oh that's a good question yeah I'm not sure about that holla think about that more good question so here I just used random uniform oh you think yeah maybe it was like an unlucky draw yeah it's random for each one okay and I also wanted to kind of share this as a I guess I do want to highlight that mmm Cholesky is fast yeah he's fast and a lot of these I want to highlight this is a technique for comparing methods I think that this can be kind of nice to do this kind of like looping through and getting getting data on how fast things are in a systematic manner and it also I think makes it a lot easier to compare as opposed to looking you know kind of having to scroll through like different places in the notebook when we've just outputted one line at a time so I think this is a good kind of good general purpose method for comparing comparing methods and then I just wanted to note there's also an l1 regression where you're using the l1 norm for your your air instead but we're not going to go into detail on that I'm going to take a moment to talk about conditioning and stability so condition number is a measure of how small changes in the input affect the output why would we care about what happens when you have small changes in the input Matthew that's true I mean so there is when you're using real-world data yeah there's gonna be this noisiness what's another reason that we're particularly interested in numerical linear algebra in this question Sam so if you have something that is the thing that comes to mind is almost the right answer to almost the right question that that is true yeah and what's that what kind of what's the cause though why we would be getting something that's just almost the right answer to almost the right question as opposed to having always exactly the right answer exactly because of floating-point errors so since with computers kind of having machine epsilon and floating-point error and you're almost never having exactly what you want there you know you have these tiny differences and it's important to know how to small small differences affect the output and then as Sam was getting at this can also kind of propagate through your problem if you're doing lots of operations and you've got just a little bit of air each time that could really add up so the relative condition number is defined as and this is just looking at the change in the output divided by changing the input for very tiny changes and then Trevathan I like that he put actual numbers on this on page 91 says a problems well-conditioned if Kappa the condition number is small such as 110 or a hundred and ill-conditioned if Kappa is large such as and that's a lot tech air typo on my part such as 10 to the 6th or 10 to the 16th and then this is kind of just a technical definition conditioning relates to the problem such as least squares where stability is what's used to talk about the particular algorithm and so an example that we saw before was computing eigenvalues here we have two matrices a is one a thousand zero one B is one a thousand point zero zero one one so these are very close that they only differ in the bottom left entry which is zero for a and point zero zero one for B when we get the eigenvalues the eigenvalues for a are one and one and the eigenvalues for B are two and then this is practically zero but one one and two zero that's like a huge difference and this is something that this isn't the computer's fault this is something about the problem of finding an eigenvalue is that for you know relatively small change we're getting a very different different answer are there questions about this idea of conditioning or of whether our problem is kind of well or ill-conditioned and then just even to kind of tie this back into this definition up here the change in the solution you know here we went from one to two and one to zero for our two answers over the change in X which was 0.001 you know that would be like one divided by 0.001 a thousand and that could actually get larger if we're doing smaller changes we're gonna have that this is kind of a poorly conditioned problem Tim Oh Sam can you pass the catch box to Tim okay thank you yeah yeah and that's like if you wanted to do an even more scientific comparison you could probably you know randomly generate several different matrices and average yeah yeah thank you Tim that's helpful okay so then in talking about oh this is another law tech typo so the product norm of a times norm of a inverse comes up a lot when you're like it appears in several different theorems around conditioning of problems and so it has its own name the condition number of a and so we've previously kind of talked about problems being well conditioned here we're talking about a matrix but the condition number of a relates to it shows up both in and I'm not going to give the the full theorems because they kind of I think have more technical details than I want to get into but computing B given a and X in a X equals B or computing X given a and B and the condition number is related to how well can well condition that problem is so this this quantity of norm of a times norm of a inverse is something you'll see I think we're about at time well we'll go over this kind of conditioning next time and then as a reminder I think everyone knows this homework 2 was due today as well as well as the draft of the blog post

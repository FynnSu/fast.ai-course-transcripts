 Okay I'm gonna go ahead and get started. I wanted to announce that Friday I'm gonna be out of town and that's normally when I have office hours, but I'll be around the other days so feel free to contact me if you want to set up another time. Yes. And I wanted to start with a little bit of review today just because I know we covered a lot of material last week. And so I want to start with a matrix vector product and kind of a different perspective on them. So this got a matrix A times a vector X. You can think of the matrix in terms of its columns. Okay so here columns are A1 through AN. We're multiplying by X. And this could be written as, and actually let me write out, X is a vector so it's got entries X1 through XN. This could be written as X1 times A1, the column, plus X2 times A2 and so on up to the scalar XN times the column AN. And so this is a linear combination of the columns of A. And this is a kind of different perspective on matrix vector multiplication than you see usually in a first linear algebra class because there you often talk about A being this transformation of X. Whereas here you can kind of think of X as acting on A. X is saying how we're gonna take a linear combination of the columns of A. So this is a pretty helpful perspective for numerical linear algebra. Any questions? And I think this might seem simple but it's surprisingly powerful this idea of kind of taking linear combinations of the columns of A. And so if you move on to thinking about matrix-matrix multiplication, again you can think about A as being a collection of columns, B is also a collection of columns, and then here we can write the product as the first column is B, or actually keep the order the same. The first column is the matrix A times B1. So you're still taking a linear combination of the columns of A and you're using the coefficients in the column B1 as your coefficients and that gives you one column of your result. And then so on. A times B2 is the second column of the result. On up to A times BK is the last column. So matrix-matrix multiplication could be thought of as just doing several kind of linear combinations of columns of A with different coefficients. And we'll see this, so we're going to, we covered a lot of material last week, so we're going to start with a bit of a review, but we'll kind of see SVD and NMF from a different perspective. I think this will be helpful to keep in mind there. No, so here the capital A is the whole matrix and the lowercase a is a single column. And so, and we could even, actually this might be nice to see, kind of write this out like a single column here is still, so this might get too long, but it's basically V11 times the column A1 plus V12, or actually put the row first, V21 times the column A2. You've got this whole sum just to give you a single column AB1, the first column of your results. And so this, yeah, this perspective of thinking of matrices in terms of kind of collections of columns will come up a lot. Do you have anything in particular about linear combinations, Jeremy, that you wanted to say? I was just thinking this is something I only came to kind of recently, like the idea that rather than going across the rows and down the columns, it's like a linear combination of columns, but in data science the way you're showing comes up much more, like it's kind of statistical, you know, like linear regression is just a weighted sum of columns. I just think this is super helpful. Thank you. Yeah, and to note, like in data science, often the A1, each column would stand for a different one of your variables or a different one of your features, and that's one reason it's nice to think of them as a unit. Yeah, we'll come back to linear combinations again. I'm going to show the three blue, one brown video, but probably, I was thinking a different week. Maybe next week we'll come to that. It's also kind of this notation of being able to switch between matrices and columns and individual entries gives you a lot of flexibility because it can be easier and kind of more concise to talk about, like, okay, we're taking a linear combination of the columns as opposed to having to think of it as all the separate entries. Okay, and so I remember we talked about putting matrices together, which is mostly through matrix multiplication, also through matrix addition, matrix vector multiplication, talk about pulling matrices apart, matrix decomposition, and what makes matrix decompositions meaningful are kind of what properties are in the decomposed matrices. It's typically kind of having it decomposed into something that's orthonormal or non-negative, then it's giving you kind of a new perspective or more information. And now I'm going to ask you some questions. Do you remember what the four considerations we talked about for algorithms are? And you can just say one at a time, so if you remember any one of the four. That's a good one here. Toss the catch box. Memory? Like with algorithms, you worry about efficiency, but you also take into consideration how much memory you take up when you write efficient algorithms. So there's a trade-off at times, so we have to consider how much space you have to work with. Does anyone remember a particular consideration with storing matrices in memory? So we discussed how we want to keep our matrices in a cache, potentially like when we're performing operations, so it doesn't, so like our computation doesn't read from, have to read from disk and perform our computation slow. So our algorithms should favor like saving towards like reading from like an L3 cache or something like that. Yeah, that's a very important one. That's about both memory and speed. This idea of something that's really slow is when we're having to take matrices in and out of our fast types of memory. Does anyone remember the fancy vocabulary word for that issue that Connor described? Okay, I heard it, does anyone want to say it? Is it locality? Yeah, so in here we've kind of gone from memory to speed because locality relates to both, but again classified it as under a speed issue. Does anyone want to say any of the other, well actually I guess to finish up memory, something else that comes up is the idea of sparse versus dense matrices, and do you want to store all your entries or just kind of store, okay these are the nonzero ones. So what else comes up under speed as a consideration? Okay, parallelization? Yes, yeah, do you want to say more about that? So this is the idea that you have a process where it doesn't have to be serial, you could like compute parts of the result all at the same time and combine them together. Exactly. Anything else under speed? Okay. You want to avoid redundant computation when you go through? Yes. So I guess they fall under the efficiency umbrella. Something that came up in the highlight video is that sometimes you have these trade-offs of like redundant computation could allow you to, I don't know, better parallelize or have to do fewer memory transfers. But yeah, in general, avoiding it is good. Okay, I'll go ahead and say one. Vectorization is another one I was thinking of, and that's when you can have a single instruction acting on multiple data, so SMPTE, and that's typically handled for you by lower-level libraries. So we briefly learned about BLAST and LaPAC, but they would be handling that. And so that's different than parallelization. Often in parallelization you have different cores doing the same work on different parts of your data. Let me check, I have a list. Okay, so I just kind of mentioned there, with parallelization, this idea of scaling to multiple cores. There's one other issue I mentioned with speed that's actually a pretty classic consideration around speed. Yes, I'm going to give you the microphone for that. Runtime complexity. Exactly, yes. Okay, so I think we've hit speed pretty thoroughly. So we talked about memory use, speed, scalability, and parallelization, and then there's a fourth big area that's important to us with our algorithms. Accuracy? Exactly, and can you say any of the categories under accuracy? I cannot. Okay, that's fine, accuracy is important. Approximate algorithms versus algorithms that have taken a very long time, but give you the exact answer. Exactly, yeah, that's an important one. Yeah, doing something that might be slightly less accurate, but much faster. And then what's another reason why sometimes approximate algorithms are appealing? Yes, so if we can't solve it, so NP-hard problems are ones where, kind of for large problems, that's no reasonable speed, no solution with a reasonable speed has been found. Something else I was thinking of is often your input data may be inaccurate, or not that precise, or have errors in it, and so having a highly accurate algorithm is kind of a waste then, because it's not going to be that precise given the quality of your data. Also, approximate algorithms, the randomness, they tend to require randomness, and the randomness often results in a better generalizability. Oh yes, yeah, so it's a way to avoid overfitting. You can kind of think of it as an automatic regularization technique. Any other kind of categories underneath accuracy? We talked about approximate algorithms. Any ideas? By setting up the tolerance not too tiny, very small, so set up a stopping point or the tolerance level. Yeah, the number of those will let you do that. And a kind of a related issue where this often comes up is floating point arithmetic, and so that's this idea, or kind of the system that computers have for storing numbers. And so math is infinite, it's continuous, and computers are finite and discrete, kind of by nature. And so floating point arithmetic is the standard of how computers store numbers, and it kind of means that numbers aren't continuous, there are gaps between them. Does anyone remember what the gap that is half of the distance between one and the next closest number is? Machine epsilon? 2 to the negative 53, I think. So machine epsilon, that's a kind of term that comes up a fair amount, so you can't get more accurate than that, since that's this gap that computers can't capture. Jeremy? Did you want to mention why it's not necessarily 2 to the negative 53? Oh, so the implementation may vary by computer, although IEEE does have standards that it kind of recommends for most, and most computers are following those standards. But yeah, a lot of these things do end up becoming more implementation dependent to the specific computer. And then one other big area of accuracy that we'll be talking about more later in the course. I'll give you a hint. We saw the eigenvalues, we saw a specific problem where we just changed the input a little bit, and the eigenvalues went from being, I think, 1 and 1 to 0 and 2, which is a huge change in the solution of the eigenvalue problem. Does anyone remember what that's called? Yeah. Stability? Yeah, that's actually conditioning and stability, and those terms are sometimes used interchangeably. One refers to the problem and one refers to the algorithm, and that's an issue that we'll return to more as the course goes on. So yeah, I think that was a good review. Definitely kind of stay mindful of those concepts since we'll see them show up in different places. Okay, thanks. Okay, so I wanted to return to NMF and SBD but from a very different perspective. So I have an Excel notebook, and I've uploaded this to GitHub. It's called Britlet. And here all the calculations, I actually did them in a Jupyter notebook with Python, but I want to use this as a way to really visualize the matrices to kind of get a different perspective on what's going on. Let me make this larger. Okay, great. And also at any time if you're having trouble hearing me or having trouble seeing something on the screen, please let me know so I can adjust. So here on the left are 27 works of kind of classic British literature. They're all written by the author's last name followed by the beginning of the title. So some of these I recognize. The second is Jane Austen's Pride and Prejudice, Sense and Sensibility, Vanity Fair. And then along the top are different vocabulary words that showed up in the books. So last time we talked some about a count matrix, which would have the counts of how many times the words showed up in each work. And this is a TF-IDF. Does anyone remember the concept behind TF-IDF? We kind of went over it pretty quickly. Is that a hand? Oh no, sorry. Okay, so that stands for term frequency inverse document frequency, and it's basically a way to normalize term document matrix. It takes into account that some words are super common and show up all the time. It also takes into account the length of the documents themselves. And so this, if we hadn't been using it, these numbers here would be integers. Since we are, this is just kind of a normalized equivalent, which is taking those things into account. And actually I did this both ways. And when I did it on the counts, I ended up with a lot more of words like like or his kind of taking more prominence. So I thought it was a little bit more interesting here to kind of give more attention to, really this kind of gave more attention to the proper names. But to take a kind of look at, actually here's a good one, the name Phineas is zero for most of these. It's the largest for a book titled Phineas. So that fits with what we would expect. Another one we can check is Cathy Linton is the protagonist of Wuthering Heights. And so you'll see Linton shows up in a few books, but it's much bigger. Are you all able to see the numbers OK? So Linton is point four for Wuthering Heights and then zero for most point zero zero one point zero zero zero four and some. So this seems reasonable of what we would get. And so this is kind of when we were talking about representing a class of documents as a matrix. This is what we were doing. And we don't have any information about the syntax. It's really just about kind of the word frequencies in these different documents. So I thought this was nice to be able to visualize it. Any questions just about this representation? OK, so let's go to SVD. So here with SVD, we've gotten the U matrix back, which is so again, we have 27 documents. It's a 27 by 10 in this case. And can anyone tell me what properties this matrix U has? The columns are orthonormal. Exactly. Thank you. And so we can check that in Excel. So I come down here. And so I looked at the correlation between the columns of U. So here, if you can see this, this is doing the sum product, which is basically what Excel calls the dot product of B2 to B28, which is this column in blue with itself. And we got one. And then here we're doing it with that column to the column next to it. And we get something 10 to the negative 16th, which is practically zero. And kind of ditto as we scroll over. And when you click on the formula in Excel, it highlights which terms are being used. So the dot product of these two arrays, 10 to the negative 16th, close to zero. So this fits with what we would expect for you having orthonormal columns. And then Kelsey said it was orthonormal. This is true if we were to have 27 columns, then the rows would also be orthonormal with each other. Here they're not since I've kind of chopped it off at 10, just to save space. So that's U. Next we have a matrix S. And what's special about S? You can just shout it out. It's diagonal, yes. So S is not the most exciting, but it's nice. It's simple. It's diagonal. It's also ordered such that the largest value is first and they're in descending order. And S kind of intuitively gives us a notion of importance. Here it's also what allows us to kind of get, since orthonormal matrices, you know, we're getting these dot products of zero or one, since our original matrix could have any values. And we would see this even more if we had done the one of the raw counts. In order to kind of get some magnitude back in there, we need to be able to multiply by numbers bigger than one. And S is letting us do that. So this is S. And then this is V. And V might remind you of U in that it's rows are orthonormal here. And so you can see kind of the dimensions of these. So U was the titles of the works by topics. And note that we're kind of assigning the meaning topics. You know, that wasn't anywhere in our input. And that's a kind of notion that makes sense for this dimension. Then S is topics by topics. And V is topics by words. And so here it's, I think we should look at a few examples. We can kind of go backwards. So if we look at Darcy, well, that shows up in several. It kind of shows up most in topic four and topic seven. So we might expect Pride and Prejudice to have a lot of topic four and topic seven. You can check that hypothesis. Pride and Prejudice has a lot of topic two as well. It does have a lot of topic four. What? Oh, negative of topic two. OK. So, yeah, the largest values are for topic four and topic seven. So we saw the word Darcy was very noticeable in topics four and seven. Pride and Prejudice has a lot of topic four and topic seven. Yeah, as Jeremy pointed out, I misread this topic two is negatively present in Pride and Prejudice, which is kind of hard to think about what that means. This is one of the downsides of SBD. There's probably less intuitive meaning there to talk about a book having a negative topic. And let's do it. Actually, I should check. Do any of you have any favorite British novels that are showing up on here? That you want to suggest a word from? Check. I do not think so. Yeah, this is kind of a bit older. Yeah, I did have to Google a few of these as I was kind of sanity checking my data to see. I know a lot of people say negative stuff makes sense in topic modeling, but sometimes it could, right? Like if you've got a really miserable book and then you've got a topic which is like happy and joy, then there would be a negative correlation. Sometimes it maybe makes sense. Yeah, so let's look at topic two and see if it reminds us of the opposite of Pride and Prejudice. Let's see if anything stands out as being a particularly large number here. I bet some Dickens book would have industry-type topics and probably Pride and Prejudice would have a lot about factories. That's true, yeah. Yeah, this is harder because it's a lot of names. I don't know, Phineas is, oh, well, although, so you can also get these double negatives. So topic two is negative 0.15 Phineas. So I would actually expect something with Phineas in it to then have, I guess, negative of topic two to get negative of a negative. Toby is big. OK. So, yeah, I don't remember a Toby in Pride and Prejudice, so that's fitting that it's negative. And let's look at, let's look at Kathy Linton again for Wuthering Heights. So if we come over here to Linton. That shows up most in, what is line nine, I guess topic eight. Topic eight. So let's go over here. See if Wuthering Heights has. Yeah, so a lot of topic eight is in Wuthering Heights, so that's fitting. Any other questions about kind of how to look at this? Yes. I'm doing it through the microphone, Jeremy. So here we have a lot of words, right? But we finally settle down into top ten topic. How does this ten come around? So and actually I cheated or I didn't tell you. So I cheated a bit. There were originally fifty five thousand words in these novels, and I used all of those for the Python part of this, but I didn't want to put that in Excel. So I just chose the top sixty four words. But really how this worked was so for a full SVE, we were getting twenty seven or twenty seven novels. We got twenty seven topics and each could involve all fifty five thousand words. So I just chose the top eight words from the top ten topics and put that in this Excel workbook. So each of the twenty seven words could have components from all fifty five thousand words. So I looked for what had the greatest magnitude. And remember, because the topics are because S is ordered in terms of the values, taking the top ten makes sense because these have larger values. And so what that means is they make up a bigger component of the original matrix. So remember, the goal here is that we want U times S times V to give us our original matrix back. And since I've only used ten of the topics, it's not going to give us the original matrix, but hopefully it's close. And that's why this is used in data compression. It's a general question. Would this be a block of our original matrix? Could we throw, would this be an actual block in the original full matrix? Oh yes, that's a good question. Let me write that down. This is a great question. Tim brought up, you're Tim, right? Brought up block matrices, which are a pretty important concept in numerical linear algebra. So let me go back to... So the idea of a block matrix is to kind of think about a matrix as having smaller components. So here... I guess if U is 27 by 55,000, we've just taken kind of this section that was... let me make it a little bit wider just so I can write on it. I've taken this section that was 27 by 64, but we could still think of the rest of the matrix being there, and it's 27 by... whatever 55,000 minus 64 is, so I'll round that to 54,000, but it's actually 54,900. Then S could be written as... So here S was 27 by 27, but we were only interested in the top 10 by 10, and that leaves us with like four other matrices. 10 by 7, 7 by 10, and 7 by 7, and then V... oh, that's not the right... Oh, sorry, I miswrote, guys. U is not 27 by 55,000. U is 27 by 27. Sorry about that. Going back, U is the works by the topics. I was confused. This was actually 27 by 10 and then 27 by 7. I was writing V there, which is 27 by 55,000. Yes. No. Oh, yes. 17. V is the one that's 27 by 55,000, and V is topics by words. Then here, because we've just picked off the top 10, this is where we get that 10 by 64 matrix. This would be 10 by 54,000, 17 by 64, and 17 by 54,000. Anyway, so block matrix is kind of breaking these matrices down into smaller matrices, and you'll notice that by how matrix multiplication works that the result can be written as block matrices, kind of products of these within them. So here, the product would be, if I call this U1 and U2, kind of the top square would be U1 times S1 times V1, and so on. I actually really should have done this with an example with just two matrices to start. And actually, maybe let me do that. This is getting to be a complicated example. But this is a really, can be a very efficient way of doing matrix multiplication or matrix operations, and it takes into account locality of the idea of, you know, you can bring in this matrix that's stored near, kind of, you know, the part that's stored together in memory, bring it into your cache, multiply it by other block matrices, kind of use it, and then when you're done, put it back. Okay, so, scratch that, we're going to do a much simpler example. If we have A is A1 and A2, and here A1 and A2 are both matrices, and say we multiply that by V1 and V2, the result is A1 dot B1, A2 dot B1, oh, sorry, okay, this is getting too convoluted. I will come back to this next time with a preworked example for you. So, yeah, we'll revisit this next time. I'm not doing this well on the fly. But that was a good question, and we'll go back to it. So, back to kind of our perspective of SVD from within Excel. Any other questions about SVD, or kind of this way of looking at it? Oh, and then I guess kind of one fine point is that the words I've picked out aren't all clumped together, so I was kind of having to, you would be rearranging your indices. Alright, let's switch to NMF. So, NMF stands for non-negative matrix factorization, which kind of gives away that the key property of the matrices is that they're non-negative. So, here, or with NMF, you get just two matrices, typically called W and H, that you're factoring into. Zoom in. Yeah, so again, we have the 27 works, and I've chosen 10 topics. With NMF, that's a parameter. You can say how many topics you want to calculate. Does anyone notice something that's kind of distinctive about this matrix? So, that's true. There are no negatives. It's sparse. Yeah, I heard several people say sparse, which means there are a lot of zeros. And this is, that's typically an additional constraint that's put on NMF matrices. And it also makes sense because kind of you don't want a value everywhere because you can't get negative. So, if you think of kind of like building up your original matrix, you don't want to have a positive place. You can't have it get too large because you're never going to have a negative to cancel that out. So, here, let's take, and then I'll show you H is over here. And again, this is also sparse and also non-negative. And so, this we can look at. So, Kathy shows up a lot in topic six. If we go back here, we would expect withering heights to have a lot of topic six, and it does. It has 0.79, which is kind of on the larger side. So, this is a, on the surface at least, this seems more interpretable, I think, of kind of finding large numbers, seeing what topics they line up with. What are some, are there any downsides to NMF? Something, oh, Kelsey? In the back, so it doesn't have a unique solution the way SVD does. Sure, it doesn't have a unique solution the way SVD does. That's a good one. Any others? This is kind of closely related. Yes? Isn't it difficult to solve? Yeah, the solution's ambiguous. It's also, so typically you have to add additional constraints. It is, yeah, our original approach, although SVD can also be slow to calculate depending on what you're doing, but yeah, we did run into speed problems on Thursday. Jeremy? I was going to say the lack of a diagonal is a problem because it's not as easy to see which topics are important, but then I was wondering, can you just look at the norm of each row or the norm of each column to get the same kind of idea? That's a really good point. I actually hadn't thought about that. I would say not having it ordered for you of what's most important, and I actually don't know if the norm gives you equivalent information. The only reason we needed the diagonal before is because of the orthonormal constraint. This doesn't have the constraint, it has the concept of how big each row and each column is, so maybe I was wrong and that's not a problem. I think it would not be straightforward just in that you've got the topic represented both in terms of how it intersects with the works in W and how it intersects with the vocabulary words in H. So you'd have to do some sort of normalization on the norm. So I would say it's not as straightforward to find the topic importance. Can you toss it to Tim who has a point? Did you say that singular value composition was unique? It's the singular values are unique. Oh yes, thank you. That's a great clarification. It's the singular values are unique, U and V are not unique. Why not? I think some of it you can also, so definitely you can multiply by negatives. This will show up like if you multiply U and V by negatives, and then particularly when you're doing the full version for the kind of fill in columns and whichever has the larger dimension, those part are not unique. Okay, so another kind of downside to NMF I was thinking about is that it's inexact. So SVD, if you do the full SVD, you can get something that fully reconstructs your matrix when you multiply back through. NMF is inexact though, because you're not even guaranteed that a non-negative W and H exist that perfectly multiply to get your original matrix. And the idea is you would multiply W and H together, and here you can see in blue, going row by column. Yes? Is a downside, like in SVD having orthogonal topics maybe becomes more interpretable because there's no overlap between them, whereas in this case there might be some overlap and that might be kind of weird? That's true, you could have overlap here, or you will have overlap since they're not not orthonormal. Alright, I have a general question. So in practice, I just see how the process goes, right? So suppose we do the SVD decomposition, and then say, it's exactly like we carry out every unique singular value. For example, here you only choose 10 out of maybe 30 topics. So for example, we have 30 eigenvalues, we construct the S matrix, and then we decide whether we want top 10 or top 20, and then once we finalize the number of topics, then we reconstruct the U and V. Is that how it works? So this is a great question. So I think you're asking, kind of, do you have to calculate the full SVD of getting all the topics and then just throw away the ones you don't want? And you do not, and we'll be talking about that in more detail with the new material today. It is possible to just calculate the topics you want for SVD, although that's kind of a newer approach and there's still a surprising number of materials that recommend you, or like, found a lot of algorithms online that will kind of be calculating the full SVD and then just throwing information away, even though that's much slower. But yeah, you don't have to do that. Oh, wait, grab the microphone. So I was just wondering, in PCA, do you have to still, because in that sense I guess you do have to calculate all the eigenvalues and vectors so that you can pick the top eigenvalues that corresponds to the eigenvectors, which are going to be your principal components. Is that the case? It actually depends. So some algorithms with, and we'll talk in a later lesson about algorithms for calculating eigenvalues, but some algorithms kind of pick out the largest eigenvalues first, particularly like iterative algorithms for finding them. And could you also please talk about the purpose of doing NMF? Because I remember in the advanced machine learning class, we kind of talked about a recommendation system where this thing can be helpful, but in this sort of top modeling area, why it's helpful? I think the main reason people argue for NMF is interpretability. And I tend to think interpretability is sometimes overblown, just in that you can get interpretability from any algorithm by altering your inputs that you put into it. So a lot of kind of supposedly black box algorithms are still interpretable. But that's, I would say, the main argument I hear for it. But it is definitely something that shows up kind of a fair amount. Jeremy? I don't know if this is interpreting, but I thought I heard something else in your question. Given that you have an algorithm that you can select how many columns, topics you want exists, do you have to run the whole thing ahead of time before knowing how many to pick off? So I know later on you're going to show us the little pictures, like here's the graph of how the singular values decrease, but do you have to run the whole thing to draw that picture and then run it again to pick off the number of columns? Okay. So this is kind of the question of do you know how your singular values are decreasing as you go, because that could let you know what a good stopping point is. And yes, you can kind of look at your singular values to have a sense of if you want to calculate more. You would have to calculate the whole thing first, in that case, to draw the picture, to then go back and decide how many to keep. That's true, but you can be calculating for a set number, see if you want to calculate more, as opposed to doing the whole thing. If we do robust PCA, we'll actually see an approach that works that way. Any other questions about this other view of AdamF and SVD? Can you talk about the pros and cons of storing a sparse matrix? So yeah, the pros and cons briefly, and we'll go into more detail of this later because we'll see kind of how SciPy handles this, and SciPy actually gives you three different ways to store sparse matrices, all of which have their own tradeoffs. But if you don't have that many non-zero entries, it's kind of like wasting all this memory to just be storing zero in lots of spaces and can take less memory to only store the sparse version. And this also comes up in algorithms of if you do a lot of computations with just zero, you might be doing wasted computations because you know anything times zero is zero, so that can be a way to save time as well. Yeah, I was actually thinking with this question about the usefulness of AdamF. Actually first, just a few pictures I wanted to show to kind of revisit what we talked about, maybe I'll start here, on Thursday, and I've kind of updated the notebook from Thursday, just some minor additions, if you want to grab that again. But I had showed that if you, actually I guess first I should show kind of using this perspective of matrix-matrix multiplication being about taking linear combinations of columns, in the case of AdamF with reconstructing someone's face, let me make this bigger, and you can see that here we've got facial features, and so this is kind of like the bridge of someone's nose and underneath their eyes, and another feature might be the tip of somebody's nose. And here a feature is somebody's brows, and you can think about taking these different features and wanting to add up a combination of them to make somebody's face. And so here, kind of each face is taken by, or made by doing a linear combination of these facial feature columns. So this is kind of a good example of this linear combination perspective. And so here the coefficients are coming from this column, they're telling you the importance of each different facial feature, and you take your linear combination, and then over here you have faces. So this is kind of an example of that. And coming back to kind of the motivation for AdamF, if you were doing something like SVD, you would be able to have negative values in the faces, which has perhaps less meaning of kind of, I mean I guess that's like canceling out some other facial feature, but kind of coming up with things that are only non-negative is more immediately interpretable. I think we should probably stop for our break soon. So let's meet back in seven minutes, and we will be continuing with some new material later in class. So I just wanted to briefly show a tweet that I saw this morning that references one of the concepts we talked about last week. Does anyone remember what temporaries are? It's like keeping the memory A plus B, but when you add C then you have to forget the result of A plus B. So it is that you're having to allocate memory to store A plus B, right? And if you had a longer computation, like we saw an example last week that, I forget, it was like A times B squared plus the natural log of C. NumPy was storing, or the old version of NumPy was storing the natural log of C in one place. It was having to store A squared, A squared times B, and allocate all this temporary memory that kind of takes up time and memory to do. And so I saw this. This is an announcement just from two weeks ago that the newest release of NumPy is allowing the reuse of temporaries. So it is still allocating temporaries, but it's allowing to reuse them. So I thought this was kind of a neat example of how this field is kind of always changing and it's interesting to keep up on it and kind of to see it in the news. So kind of before we move on, I wanted to revisit PyTorch and last time, and I just kind of found some material from a PyTorch AutoGrad introduction, but does anyone remember what AutoGrad is for PyTorch? Actually, okay, even before that, let me ask, do you remember why I decided to use PyTorch last time? Yes, I hear a lot of people saying GPU. I wanted to speed things up by running them on the GPU and PyTorch, one of its purposes is it's a deep learning framework, but another purpose is that it's a alternative to NumPy that runs on the GPU. And does anybody remember the method that we use to tell PyTorch to put things on the GPU? Yes, I heard a lot of people say dot CUDA. And so that, actually, let me go to that. There was a note that if you're not using a GPU, you want to delete the dot CUDA's that show up here. But that's where we're explicitly telling PyTorch to put things on a GPU. Okay, so AutoGrad. Does anybody remember what that is? Oh, yeah? I don't think I remember it completely, but isn't it like an optimization? So it is very useful for optimization, yes. And so, no, that was good. Definitely in the very right ballpark. So what do we need? We talked about stochastic gradient descent last time. What do we need to know to be able to do gradient descent or stochastic gradient descent? You need to know the derivatives. Exactly, we need derivatives. And so AutoGrad is automatic differentiation. Because if you don't know the derivative and don't want to have to calculate it, in some cases you may not even be able to calculate it, PyTorch's AutoGrad calculates it for you by letting variables keep track of how they were made. When I tell people this, they always assume that it must be doing it really slowly by calculating the function and then the function plus a little bit. Like even Terrence here at USF, I was telling him, he was like, oh, too slow. I kept saying to him, no, it actually calculates it really properly. It's so amazing that it's possible that he can come up and don't believe you. Thank you. This is calculating the derivative very fastly. I just wanted to show some basics of PyTorch again, because I think what we did last time was very quick. So here, creating a variable. This is just a two by two matrix. And we're saying requires grad equals true. And so that's letting it know, hey, we're going to want to be able to get derivatives with respect to this variable later on. And then I'm printing X and it tells me this is just, oh, and I've initialized it to torch.ones. NumPy has a very similar method, np.ones, that initializes a variable all to ones. There's also zeros that initializes all to zeros. So here we have a two by two tensor where the values are all one. And a tensor is just a generalization of a matrix. So you can think of it as a matrix, but it can have higher dimensions. And then X has its data, which in this case is all ones. And it also has a grad attribute, which is going to store the gradient, which right now is zero, because we haven't done anything. So then we do y equals x plus two. We print y. That's all threes. And we'll come back to this idea in lesson, kind of notebook three, but there's something called broadcasting. You'll notice here we're adding a scalar to a matrix. And what it's done is it's basically just kind of multiplied the scalar to be the right dimensions. So basically we added our matrix one, one, one, one, two, two, two, two, two to get all these threes. But this idea of broadcasting is pretty important and we'll see more of it. And then we do z equals y times y times three. The out equals the sum of z. So if we print out z and out, z is 27, 27, 27, 27. Out is 108. And then we do out dot backwards, which is kind of back propagation, and it's calculating the gradient. And let's take a moment. Let's see if this worked. Oh, yes, it did. OK, so remember. We'll write what z is. So y was x plus two squared times three. And we want to take, and then z is the sum of that. And really, kind of if we think of x being the individual entries now, the sum of that is going to be equal to four, kind of times an individual entry, since they're all the same. So that's 12x plus two squared. And if we take the derivative, so we're interested in d out dx. Hold on a moment. I'll come back. OK. OK. So, yeah, I'm not going to sum them yet. So you take the derivative. So you should remember, kind of with an exponent, you pull it down, subtract one off. So that would be six times x plus two to the one power, which is just itself, times the derivative of what's inside, which is just one. And so we're getting that the derivative is six times x plus two. And remember, we're interested in where x is equal to one. So the derivative is six times three equals 18, which is what it's telling us. So PyTorch has done this for us. Any questions about that? Do you remind us about the difference between a tensor and a variable? So tensors and variables have the same API. These are both kind of PyTorch notions, but variables have the AutoGrad option in that they keep track of how they were created. And so you have, if you want to use AutoGrad, you need to use variables. Other questions? So you can see it's really handy to have it calculate the derivative for us. OK, so I want to return to kind of where we left off now. Let's start back with comparing approaches. So Scikit learns NMF. It was fast. We didn't have to tune parameters. The people that created it used decades of academic research and it was pretty specific to NMF. And then this is a list of kind of some relatively recent research in NMF. So you can see it's kind of an active field. And this comes from a Python library called Nimfa that's specifically about NMF. So then we decided we wanted something that we could use or build ourselves. So we used PyTorch and stochastic gradient descent. Stochastic gradient descent is a very general purpose optimization algorithm. So we were just choosing to apply it to NMF, but it works for many, many different optimization problems. So it's a very good general purpose tool to have. It didn't take us that long to implement. We did have to do more fiddling with the parameters though. So remember we had a learning rate, keeping track of what size steps we wanted to take. And it was also not as fast. We initially tried it in NumPy and because it was so slow we had to switch to PyTorch. Jeremy? And we also found one benefit of it, another benefit of it, which was because we made the non-negativity like a penalty rather than a constraint. We had a few small negatives and that allowed us actually to have a more accurate decomposition which maybe created some better topics. That's true, yes. And that did give us though more parameters that we were having to tune of, you know, how much weight to give, wanting it to be non-negative versus wanting it to multiply to give us the right answer. So any questions about NMF or these different approaches? OK, so we're going to return to SVD. And we've kind of talked about this sum, this idea of truncated SVD. So for full SVD would be calculating kind of the full dimension of topics. But we've already seen that it's handy to just have kind of a limited number of topics because those are the most important ones. This is also how SVD is used in data compression where you are kind of choosing a smaller value because you want to kind of save space with your data. And so, yeah, this is truncated SVD. This is the picture from the Facebook blog post again here. Also, I realized after class Thursday that some of the examples I think were documents by words and some were words by documents. So I think I may have misspoke a few times on Thursday about which order they were because we do see both in here. This is saying the words are rows, the hashtags are columns. So words, this would basically be words by topics then, topics by topics, and then topics by hashtags. And so we're going to be looking at using a randomized algorithm to calculate the truncated SVD. And so this is going to be a quick method and this kind of gets back to April's question earlier about do you have to calculate the full SVD and then throw away information. We're going to use a randomized approach to just calculate, well, what we want plus a little bit more of buffer but to not have to calculate the full thing. So we talked about, you know, shortcomings of classical algorithms, just matrices are so large and often data is missing or inaccurate. So why spend the extra computation when your result is not going to be perfectly accurate. Another key theme of the course that data transfer is a major role in the time of algorithms. So it's not just about computational complexity. And then so we're going to be referencing this paper by Halco, let me pull it up, called Finding Structure with Randomness. And I think it's very well written. I particularly like the introduction so you might want to check it out. But this is where a lot of material in this lecture as well as I think some in the next lecture comes from. And they give different examples. So just know that that's out there. Okay, so Scikit-Learn has a randomized SVD built in and this is from the decomposition module. Actually I should probably not start running things now because I don't know what's been run. But you'll see here we can request how many components we want or how many singular values. So we're saying five and it's quite quick. So let me get back. And so just to remember the data set that we were using here was from newsgroups. So these are kind of discussion boards on the Internet on different topics. We had two thousand posts to newsgroups. We're saying that we just want to get five topics. And then there were twenty six thousand different vocabulary words used. And so the top five topics, remember we just requested things from four categories. And those categories were space, graphics, religion, and atheism. And the top five topics that we see are JPEG, image, EDU, file, graphics, images, GIF, data. So these all have to do with graphics. So does the second topic. This next one seems like a mix of space and religion. You do have graphics showing up. So there's some problems with these not being kind of fully separated out. But overall, they're pretty good. So I'm going to talk about the approach of kind of how this randomized SVD was calculated, because it is really handy to not have to calculate the full thing. And so the basic approach is that we want to find an approximation to the range of A. And does anyone remember what the range of the matrix is? It's kind of a linear algebra vocabulary question. Anyone? OK, yeah, go for it. Is it the space covered by the column basis? Exactly, yeah. Yeah, the space covered by the column basis. And that's actually, that's really great language because that, it's kind of getting back to this idea of thinking of the, you know, the taking linear combinations of the columns. But so one way to write it would be that the range of a matrix A is all y such that Ax equals y for some x. And so if you think about a lot of times, so this is going back now to the image of A being some transformation. But if A is kind of transforming, you know, taking vectors x and transforming them to y, the range is everything that you can hit over here by multiplying by A. But yeah, I really liked Kelsey's definition of thinking of the columns as a basis and saying, you know, what are the columns span? I have a good one off the top of my head. OK, so Jeremy's asking what a basis is. Can anyone answer what a basis is? You think of your basis as like the coordinate axes for your space? Yes. Yeah. So kind of depending on the space. So if we're talking about all the numbers in R2, then the coordinate axes are the standard basis. Yeah. Yeah. And so really kind of depending on your space, the basis are vectors that you can take linear combinations of to get to get any value in the space that you're talking about. I think it's like, just for R2, you can interpret it as like, what area can you tile over with parallelograms where the legs are? Yes. Yes. OK, I actually wasn't going to show this till later, but I really think this is like the perfect time probably to watch the three blue, one brown video about bases. So if you're not familiar with three blue, one brown, these are really fantastic videos about linear algebra. Well, he makes them about many topics. You're going to switch over. Let me just get to the video. But I highly recommend these. And this I've kind of specifically chosen. Actually, OK, so I was going to show the change of basis video, which I particularly like. But do you feel like you need to see the linear combinations span and basis intro video before you see the one on change of basis? How? OK, so no shame about your answers. Raise your hand if you feel really comfortable with the idea of bases and span. And then raise your hand if you feel like you need a refresher on the ideas of bases and span. OK, so it was about half and half. So I'm going to go with the review. Hopefully even for those of you that feel really comfortable with these ideas, I find three blue, one brown to just be a really new perspective that you don't see in a lot of linear algebra classes. So I think that you'll still kind of gain something from this. Jeremy, do I need to do anything else? Yes, I do. But there's another kind of interesting way to think about these coordinates, which is pretty central to linear algebra. When you have a pair of numbers that's meant to describe a vector, like 3, negative 2, I want you to think about each coordinate as a scalar, meaning think about how each one stretches or squishes vectors. In the x-y coordinate system, there are two very special vectors, the one pointing to the right with length 1, commonly called i-hat, or the unit vector in the x direction, and the one pointing straight up with length 1, commonly called j-hat, or the unit vector in the y direction. Now, think of the x coordinate of our vector as a scalar that scales i-hat, stretching it by a factor of 3, and the y coordinate as a scalar that scales j-hat, flipping it and stretching it by a factor of 2. In this sense, the vector that these coordinates describe is the sum of two scaled vectors. That's a surprisingly important concept, this idea of adding together two scaled vectors. Those two vectors, i-hat and j-hat, have a special name, by the way. Together, they're called the basis of a coordinate system. What this means, basically, is that when you think about coordinates as scalars, the basis vectors are what those scalars actually, you know, scale. There's also a more technical definition, but I'll get to that later. By framing our coordinate system in terms of these two special basis vectors, it raises a pretty interesting and subtle point. We could have chosen different basis vectors and gotten a completely reasonable new coordinate system. For example, think some vector pointing up and to the right, along with some other vector pointing down and to the right in some way. Take a moment to think about all the different vectors that you could get by choosing two scalars, using each one to scale one of the vectors, then adding together what you get. Which two-dimensional vectors can you reach by altering the choices of scalars? The answer is that you can reach every possible two-dimensional vector, and I think it's a good puzzle to contemplate why. A new pair of basis vectors like this still gives us a valid way to go back and forth between pairs of numbers and two-dimensional vectors. But the association is definitely different from the one that you get using the more standard basis of i-hat and j-hat. This is something I'll go into much more detail on later, describing the exact relationship between different coordinate systems. But for right now, I just want you to appreciate the fact that any time we describe vectors numerically, it depends on an implicit choice of what basis vectors we're using. So any time that you're scaling two vectors and adding them like this, it's called a linear combination of those two vectors. Where does this word linear come from? Why does this have anything to do with lines? Well, this isn't the etymology, but one way I like to think about it is that if you fix one of those scalars and let the other one change its value freely, the tip of the resulting vector draws a straight line. Now, if you let both scalars range freely and consider every possible vector that you can get, there are two things that can happen. For most pairs of vectors, you'll be able to reach every possible point in the plane. Every two-dimensional vector is within your grasp. However, in the unlucky case where your two original vectors happen to line up, the tip of the resulting vector is limited to just this single line passing through the origin. Actually, technically there's a third possibility too. Both your vectors could be zero, in which case you'd just be stuck at the origin. Here's some more terminology. The set of all possible vectors that you can reach with a linear combination of a given pair of vectors is called the span of those two vectors. So restating what we just saw in this lingo, the span of most pairs of 2D vectors is all vectors of 2D space, but when they line up, their span is all vectors whose tip sit on a certain line. Remember how I said that linear algebra revolves around vector addition and scalar multiplication? Well, the span of two vectors is basically a way of asking, what are all the possible vectors you can reach using only these two fundamental operations, vector addition and scalar multiplication? This is a good time to talk about how people commonly think about vectors as points. It gets really crowded to think about a whole collection of vectors sitting on a line, and more crowded still to think about all two-dimensional vectors all at once, filling up the plane. So when dealing with collections of vectors like this, it's common to represent each one with just a point in space, the point at the tip of that vector, where, as usual, they want you thinking about that vector with its tail on the origin. That way, if you want to think about every possible vector whose tip sits on a certain line, just think about the line itself. Likewise, to think about all possible two-dimensional vectors all at once, conceptualize each one as the point where its tip sits. In effect, what you'll be thinking about is the infinite flat sheet of two-dimensional space itself, leaving the arrows out of it. In general, if you're thinking about a vector on its own, think of it as an arrow, and if you're dealing with a collection of vectors, it's convenient to think of them all as points. So for a span example, the span of most pairs of vectors ends up being the entire infinite sheet of two-dimensional space, but if they line up, their span is just a line. The idea of span is a lot more interesting if we start thinking about vectors in three-dimensional space. For example, if you take two vectors in 3D space that are not pointing in the same direction, what does it mean to take their span? Well, their span is the collection of all possible linear combinations of those two vectors, meaning all possible vectors you get by scaling each of the two of them in some way, and then adding them together. You can kind of imagine turning two different knobs to change the two scalars defining the linear combination, adding the scaled vectors and following the tip of the resulting vector. That tip will trace out some kind of flat sheet cutting through the origin of three-dimensional space. This flat sheet is the span of the two vectors, or more precisely, the set of all possible vectors whose tips sit on that flat sheet is the span of your two vectors. Isn't that a beautiful mental image? So what happens if we add a third vector and consider the span of all three of those guys? A linear combination of three vectors is defined pretty much the same way as it is for two. You'll choose three different scalars, scale each of those vectors, and then add them all together. And again, the span of these vectors is the set of all possible linear combinations. Two different things could happen here. If your third vector happens to be sitting on the span of the first two, then the span doesn't change. You're sort of trapped on that same flat sheet. In other words, adding a scaled version of that third vector to the linear combination doesn't really give you access to any new vectors. But if you just randomly choose a third vector, it's almost certainly not sitting on the span of those first two. Then, since it's pointing in a separate direction, it unlocks access to every possible three-dimensional vector. One way I like to think about this is that as you scale that new third vector, it moves around that span sheet of the first two, sweeping it through all of space. Another way to think about it is that you're making full use of the three freely changing scalars that you have at your disposal to access the full three dimensions of space. Now, in the case where the third vector was already sitting on the span of the first two, or the case where two vectors happen to line up, we want some terminology to describe the fact that at least one of these vectors is redundant, not adding anything to our span. Whenever this happens, where you have multiple vectors and you could remove one without reducing the span, the relevant terminology is to say that they are linearly dependent. Another way of phrasing that would be to say that one of the vectors can be expressed as a linear combination of the others, since it's already in the span of the others. On the other hand, if each vector really does add another dimension to the span, they're said to be linearly independent. So with all of that terminology, and hopefully with some good mental images to go with it, let me leave you with a puzzle before we go. The typical definition of a basis of a space is a set of linearly independent vectors that span that space. Now, given how I described a basis earlier, and given your current understanding of the words span and linearly independent, think about why this definition would make sense. In the next video, I'll get into matrices and transform of space. See you then! Great. Is there anything that you found particularly noteworthy or interesting about this? It's okay if not. It can also just be a good review of the terminology. I also just want to highlight that he has some videos not related to linear algebra that are just really interesting and beautiful. I particularly recommend his one on the Towers of Hanoi, which is kind of this puzzle, combined with binary and with Sierpinski triangles. So there's some really surprising connections between those three topics that I did not know about, and also some really nice visualizations. I don't know this person at all, but I did just start supporting him through Patreon because I really love his videos. I just mentioned it because hopefully people will see this video in the future, and if they do, they should support the person who created it. Okay, so kind of going back to, and so we'll return another day to kind of talk about change of basis, but going back to this idea of the range of A. So as Kelsey said, that's the space spanned by the columns of A. So hopefully this kind of helps you get up to speed on those concepts of columns of A. We're taking all possible linear combinations of them, and that's the range of A. And so our goal, and just to kind of remind you what we're doing, our goal is to come up with an algorithm for the randomized SVD. And so to be, or for a truncated SVD using randomized values. So we want to be able to just calculate as many topics as we're interested in, not have to calculate them all. We're going to use randomization, and this is kind of outlining a path for us. So the first step is we want to find a matrix Q that has R orthonormal columns such that A is approximately Q times Q transpose times A. And so the thing to note here, let me write this down maybe, is that suppose A is M by N and Q is just going to be M by R. So there could be a lot of space savings there. And so Q times Q transpose, actually this is a question for you, is Q by Q transpose going to be the identity? You can just shout out your guesses. I see both nodding and shaking heads. Okay, who wants to vote for yes, it will be the identity? Who wants to vote no, it will not be the identity? Okay, does anyone want to say why they voted the way they did? Okay, I'll say. So if, oh, what? Because it's columns, but the row, actually the multiplication of the rows might not be orthogonal. Exactly, yes. Yeah, so this was kind of a tricky question, if both the columns and rows of Q had been orthonormal, then it would be the identity. And even, yeah, so if both the columns and the rows were orthonormal, we would get the identity. However, it's just the columns, and so we kind of have this tall, skinny matrix. So Q kind of looks like this, and we multiply Q by Q transpose, we're getting something, we want something that acts kind of like the identity for A, but it's not actually going to be the identity because we didn't have enough inputs kind of going into it, but we're hoping to kind of find something so that Q by Q transpose at least acts similar to the identity for A. And so we'll come back to the question of how do we actually find such a Q, but for now just know that that's what, that it is possible. So then we want to construct B equals Q transpose times A. Actually, I should... OK, I guess I have to pull this up here. I'll write that again, but I will not erase it this time. So remember A is M by N, Q is M by R. So then when we do Q transpose times A, that's something that's R by M times M by N, and we get that the product is R by N. So B is a lot smaller than A. So we can compute the SVD of B by standard methods, and this will be much quicker than it would have been to compute the SVD of A. And then plugging back in, so kind of here, that's the formula for S... Oh, that's actually a typo. But B is U times sigma times B transpose, typical formula for the SVD... Oh, no, it's not a typo. I called it S because it's a different one. We're going to have U later on. Then remember A is approximately Q times Q transpose times A. So we plug in for Q transpose times A, plug in this SVD for B, and we get A is approximately Q times S times sigma times B transpose. And we can set U equal to QS, and now we have a SVD for A, a low-rank SVD. So I have a question. Why is it okay to say U is QS? Because we want U to have orthonormal columns. Kelsey? I think U is just a rotation of S. Yeah, that's the key. Since S and Q are orthonormal, we'll get something else that's orthonormal exactly. Yeah, we'll talk about the topic of rotations later. But yeah, S and Q are orthonormal, so that works. So now we're going to return to these questions of, okay, how did we find Q? But are there any questions just about this plan of what we're going to do? If you happen to remember the computational complexity of SVD, so when we reduced the size of it, are we reducing its speed by linear or polynomial? I did not remember. Let me write that down. I'll talk about that next time. It might be squared in the number of columns. I would believe that, but I will check. Yeah, so general idea is kind of we're finding the special Q, then we find the SVD on a smaller matrix, Q transpose times A, and we're able to plug that back in to have our truncated SVD for A. Okay, so first question. How do we find Q? And so it turns out we can just take a bunch of random vectors, W, and look at the subspace formed by Aw for these different random vectors. We'll form a matrix W with the W's as its columns, and we take the QR decomposition of Aw equals QR. So we will be talking about the QR decomposition a lot. For now, all you need to know is that the QR decomposition exists for any matrix, and it's an orthonormal matrix times an upper triangular matrix. And so this is something I think nice about linear algebra is you kind of have these standard naming conventions, and so pretty much any time you see a matrix named Q, you can assume it's orthonormal, because that's just a commonly used convention. But the QR decomposition is all about getting an orthonormal matrix times an upper triangular matrix, and we will learn how to do that in a later lesson. So we'll take Aw, W's random, get the QR, and the Q, this is kind of a property from the QR decomposition, and the Q forms an orthonormal basis for Aw. And Aw is giving us the range of A, since it's kind of A times these different values. And since Aw has far more rows than columns, it turns out that it works in practice that these columns are approximately orthonormal. It's just really unlikely that you would get columns that were linearly dependent when you're choosing random values. Any questions about this? We'll come back to that under how we should choose R, I think. And yeah, this idea of the QR decomposition will show up in almost every lesson, so you will get to see a lot of the QR decomposition in this class. It's pretty foundational to numerical linear algebra. So yeah, for now you just need to know that Q consists of orthonormal columns, R is upper triangular. When I say upper triangular, that means that everything below the diagonal is zero. Trevathan says the QR decomposition is the most important idea in numerical linear algebra, so that's pretty high praise. And then this question, so remember, we chose Q to have R orthonormal columns, and then R is giving us what the dimension of B is going to be. So how do we want to choose R? So if we wanted, suppose we had a matrix with 100 columns and we wanted to get 5, so in the question of, going back to our literary example, we just want 5 topics. To be safe, we're going to choose something larger than 5, so you could just kind of as a rule of thumb say let's add 10 to what we're doing, so let's do it with 15. So you don't want to calculate exactly 5 because we've got this randomized component, so it's kind of safer to give yourself some buffer, but we don't need to calculate the full 100 topics. So our projection was only approximate, so we're making it a little bit bigger than we need. Now let me show you what this looks like in code. So we, above we used scikit-learn's implementation. Now I'm going to write my own, which is based off of the scikit-learn source code, only it takes into account fewer special cases. So this is a less robust version, but I think it's kind of clearer what's happening. First we want a randomized range finder, and so this is what just finds the Q that we saw above in step one when we said like, okay, we want Q such that A is approximately Q times Q transpose times, or yeah, Q times Q transpose times A. So what we'll do in here is just randomly initialize a matrix to our size, and here, yeah, in here size we're kind of telling it how many columns we want. And then for now let's imagine that the number of iterations were zero, so we can ignore this inner for loop, then we could just call the QR decomposition on A times Q, and we get back kind of Q and R, and we'll return our Q. And so that's giving us the Q that we want. Questions about the randomized range finder? So this is kind of just finding, finding that Q we want to kind of approximate the range of A. And then in the next lesson we'll be covering LU decomposition, but LU decomposition decomposes a matrix into a lower triangular matrix times an upper triangular matrix. And we can add some iterations of that here in the middle, that this will make our result more accurate, and it's basically kind of gives us this chance, so we kind of want to imprint A, like we're really interested in the range of A, so we want to imprint A again and again. And so it would be nice to just kind of keep multiplying by A, you know, because if you multiply by A a bunch of times you're getting something that's like super in the range of A. The issue with just doing that directly is that, you know, it could shrink to zero or it could explode. That's very unstable to kind of multiply by the same number again and again unless that number happens to be one. And so taking the LU decomposition is a way to kind of normalize it and take into account like, okay, we want something that's normalized so that it doesn't explode or vanish. So that's just kind of like a very kind of high-level intuitive idea of what this is doing. And then how that's used inside the randomized SVD is first the number of random columns we're going to create is the number of components we want plus the number of oversamples. So I mentioned since this is random we're going to give ourselves a buffer and kind of oversample, so that's defaulting to 10, but you could choose another number if you wanted. Then we'll call the randomized range finder with our matrix and with the sum of the number of components plus the number of oversamples. That's saying how many columns we want to find. Then we do Q transpose times M. Another way to think about that is projecting M to this K plus P dimensional space using the basis vectors. So Q is the, yeah, the K plus, or kind of giving us this basis of a K plus P dimensional space. Then we calculate our SVD on B. Remember B is a lot smaller than our original matrix. We get that back. Here we delete B to free up the memory and then we say U is equal to our Q times the U that came back for B and we'll just return the number of components. Remember we've calculated the number of components plus oversamples so we don't want to return everything. We'll chop off the last 10 columns and last 10 singular values and just return what else is there. So we can try this out, see that we get stuff back and this is, let me think, I think this is faster. Just marginally. And we're still getting topics that seem reasonable. So this is, this is working that even though we had 2000 posts by 25,000 words, if we were going to do a full SVD that would be finding 2000 topics. We've said we just want five. We're going to calculate 15 and we get back pretty good topics. I looked up the computational complexity and for an M by N matrix, SVDs form computational complexity is M squared N plus N cubed. Okay. Thank you, Jeremy. Yeah. So what Jeremy was saying is it's M squared N plus N cubed. So both, both of those terms are cubic, which is really slow. So it is awesome to be able to slice stuff off of there because, you know, before we would have been, and we are still, I guess our M is staying the same for this smaller matrix B, but N is changing. Yeah. And then we are almost out of time. Maybe we'll start here at the end. I just had a quick exercise for you. But we'll do that next time. Yeah. So next time we'll kind of finish up, talk about this a little bit more. And I think that'll be good to kind of return to it after, after two days. And then we'll be getting into background removal after that, which is exciting. I also wanted to remind you there's homework one is available on GitHub and that is due Thursday. Yes, Tim. That's right. I, I think either is fine. Yes, whatever. Maybe print it. But yeah, maybe print a PDF would be best. Yeah. So print a PDF. I'll say that in the Slack channel as well. But yeah, print a PDF for the homework due on Thursday. To email us. Yeah, it's good. Great. Thank you.

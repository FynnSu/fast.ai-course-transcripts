 All right, I'm going to go ahead and get started and I wanted to announce first that I just put in Slack a link to a mid-course feedback survey and this is just for me and it's anonymous but it's kind of helpful for me to kind of get feedback about how things are going and if there are things that it would be helpful to adjust. So please try to fill that out today. And also if you're just auditing the course, I'm still interested in your feedback so fill it out as well. Yeah, so I was going to start with some review. We're going to talk about the robust PCA algorithm we saw last time and kind of look at a little bit of some different information about it from how we covered it before. I wanted to start with just a reminder, what is the SVD? Do you have a microphone? Anyone? SVD? Single value, singular value decomposition. Yes. It's just a perfect decomposition of a matrix and the singular value, maybe that is a diagonal matrix. And what can you say about the U and V matrices? Well, I think they can represent different things but it's like topic over word. That's true for topic modeling, yeah. Was it the left singular and the right singular components of it? Yes, those are those names. I was looking for one other property, although everything you've said is true. They were orthogonal? Yes, yeah. Okay. And what is the truncated SVD? Truncated SVD. How do you get from the full SVD to the truncated SVD? Brad? And be sure to hold the microphone close to your mouth. Sure. So, a truncated SVD is simply, at least I think it's a lower rank approximation. And I think that in the context of what we were talking about last class, we're talking about a sort of random, I'm trying to think of the exact terminology, but using a smaller matrix and then factorizing that? Yes. Right. And then, or it could be something like a lower rank approximation or something like that. Yeah, yeah. So typically, you'll think of the truncated SVD as kind of taking the full SVD and chopping off however many kind of singular values, slash columns of U, slash rows of V to get something smaller, which is good for data compression. And then Brad brought up last time we were looking at this randomized approximation where we were kind of taking a different version of the same matrix by getting a random projection and it had fewer columns, but the same, or approximately the same column space. And then taking the full SVD of that smaller matrix and letting that be kind of the truncated SVD of the full matrix. One thought is worth mentioning that the particular randomized approach Rachel taught isn't the only way to calculate truncated SVD. So if you just did a full SVD and literally threw away a bunch of the last columns and rows, that's still truncated SVD. It's just an inefficient way to do it. There are other ways to calculate it, so the randomized approach is our way, but very good at calculating the truncated SVD. Yeah. Yeah. Oh, actually Brad's got another question or comment. So I know last week I sort of mentioned that in the approach where you actually calculate the full SVD and then you chop off the last n or so smallest singular values, I know that that is the best low rank approximation. How well does the random approximation compare to that? I know that obviously that's not an efficient way of doing it because you have to do the whole thing to start with, but the point of the random is that you don't have to do the whole thing. Right. So is there any sort of intuition of how well that does, how good of a job that does approximating? I think in general it does a very good job. I'd have to look back at the paper, because I think the paper does give guarantees on how good it is. There's actually a parameter, you can choose how good it is, which is that thing where you add more components than you need. I was kind of referring to that as buffer, but we have a different name for it. Like you say, I want 5, but you actually calculate it with 15 and then get rid of the last 10. So if you made that parameter big enough that it was the whole thing, it's perfect. If you make it so small that it's 0, it's perfect, so you can pick. But even with 10 extra, it's 10 to the negative, something really small. The paper recommends either 10 or 15 when you're doing this in practice. Okay, one more, oh wait, Jeremy, throw the microphone back please. And so just one last thing about the buffer. So is the intuition behind that, like if you only want the top 5 and you have these extra vectors, is that sort of like a place to dump what you don't, like if you only took 5, then you're kind of squeezing all the bad stuff into sort of the ones that you want. Whereas if you have a buffer, it's like if you want to have the best 5, you need somewhere to put the not good stuff. I'm kind of talking really vaguely, sort of get what I'm saying. I think I would say that, actually I got a full screen, what that number is referring to is how many columns you're taking in your randomized approximation of the column space. Jeremy, how do I exit full screen mode in Minimoji? Well, I want to get a new sheet here. What those numbers are referring to is, so like if, you know, this is like your wide matrix A that's too big to decompose, you're kind of deciding, okay, how many columns do I want to get over here? I see it as more kind of guaranteeing, like your goal is that you want the column space of A and the column space of B to be the same thing, but kind of by adding some extra ones on, I think that's kind of a more reassurance of like, okay, I'm really hitting this much of the column space of A. Because it's, I guess it's like you kind of just need that, need those column spaces to be the same up to however many singular values you're trying to get. Right. And then I guess what I was wondering is, are those top say five, if you want say the most important ones, then you necessarily need that buffer to sort of put the less important stuff, right? Whereas if you just have the five, then all the less important stuff is going to be kind of forced to be within that top five. Is that your way to think about it? So there is this element of randomness still. And so I think some of it's like if you were just getting five, if you knew that they were truly the best, I think you would be okay. But because it's random, you're kind of taking some extra, but you're right. Like it is having a, or Tim, did you have a, like a bunch of, I don't know, M&M's, like the best tasting ones are like at the top left hand corner. And you know, like there's five best tasting ones, but you can't just like pick five from the top left corner. You kind of pick like a handful more and then that increases the chance of you having the best five and that kind of handful. That's what the buff, that's how I saw the buff. Yeah. I like that. I like that analogy. Because you pick a little bit more so that you have the higher chance of having the best in that handful. Yes. Yeah. That's good. Thank you. Sam? Do you think we can go over what's happening when we multiply by a random matrix? Can we see why it's useful and like the different kind of contexts we can use it? Because you guys mentioned that it's useful all over the place. Sure. I was kind of thinking of like different applications of that. What does it do, first of all? What does it do and then why do it? I kind of get it, but I'd like to know a little more. Okay. Yeah, that might be something I'll return to next time. Let me write that down. Yeah, I want to think about if there's maybe like a good way we can visualize that as it's happening or something. So kind of more detail on what is a randomized projection, where else is it used. One key idea we mentioned is this idea that when you multiply by a random matrix, the result in columns, as long as there aren't too many, you're likely to be independent of each other, so you're likely to get orthogonal columns, which is kind of a good starting point for thinking about why it helps. Yeah, and that kind of plays into the idea of like by taking a buffer you're getting even more orthogonal columns, and so you are covering kind of more of your space. Sam, again? I guess the part in particular that I don't understand is when we multiply by the random columns, we're transforming the matrix, and if the columns and rows all correspond to like one particular feature for each one of our users, for instance, then when we transform it by multiplying by a bunch of random columns, at what point do we have to transform it back so that we have, so we actually have, like, So yeah, so the transforming it back is what happens if you'll remember we like take the SVD on the smaller matrix and then we multiply by Q again, which I can't remember if it's Q or Q transpose, to kind of that's what transforms it back since Q is orthonormal, it's its own inverse, and so that's kind of the transformation back. I don't remember that part, I can look through it. Okay. Great. So I think we've thoroughly hit the first three questions about SVD and truncated SVD, are there any other questions about it? Okay, and then I think something that I definitely wanted to say more clearly than I did last time is that classical PCA is finding the best rank K estimate L of M, so minimizing the norm of M minus L where L has rank K, and as Brad said, truncated SVD is giving you the best classical PCA. So that's the kind of best rank K estimate L is going to be if you did the SVD on M, your full matrix, and then cut off however many columns you needed. That's what gives you the best estimate. Is that under the Frobenius norm? Yes. Oh, that was under the Frobenius norm. And the Frobenius norm is where you're just squaring each of your elements element-wise and summing them. And this, yeah, kind of as we said before, you could get the truncated SVD by doing the full SVD and then throwing away the values you don't need, although that's slow. And so we saw the randomized approximation as an accurate substitute for that to kind of do it more quickly. So that's a classical PCA. Does anyone remember what robust PCA is? I'll give you a hint. We added in another matrix, so instead of decomposing just to be L, a single matrix, we decomposed it to be L plus another type of matrix. Sam? We're decomposing it into a dense sparse, which makes it more robust to outliers. Yeah, so we're getting a sparse one now. And so it's a low-rank matrix plus a sparse matrix. And I wanted to kind of highlight with, previously when we were looking at this background removal problem, we started off just doing the randomized SVD or just doing an SVD to get the background. And there we're really just trying to find this low-rank matrix, and that's the background. The difference with when we go to robust PCA is now we're trying to find the low-rank matrix and the sparse matrix, and so we're actually kind of actively looking for what is the foreground. So initially kind of when we just did SVD, we were like just looking for the background and then we were like, okay, whatever's left over must be foreground, whereas robust PCA we're taking this very different approach of, okay, we're actively looking for both the background, which is low-rank, and the foreground, which is sparse. And then what are some, what are, so we, oh, Tim? How does it actually do that decomposition into L plus S, like how do you actually do the decomposition? Okay, so we'll talk about that. That's a good question in a moment. And that, I'm going to kind of just highlight some key parts of the algorithm that's kind of part of, it's an alternating Lagrange method. And so we're not going to get into like the details of, all the details of the optimization, but yeah, I will go over kind of three, what I think three key parts of the algorithm are. Yeah, good question. Jeremy, at the top of the microphone fell off. So what are some other applications that we saw? So we did the background removal, yeah, facial recognition, and in particular we were seeing faces where there was a lot of noise. So at any time that your data is corrupted with noise, particularly large amounts of noise. So one of the weaknesses of PCA, classical PCA is classical PCA can handle noise that's small everywhere, but it's very brittle if there's even just like a few entries that are way off and have a huge amount of noise. And so robust PCA is good at handling that. And in that case, the low ranked data is kind of your, I guess, real data or accurate data and the sparse data is your noise, but those values can be anything. So when you have grossly corrupted data, which often happens, kind of particularly with online interactions, robust PCA is useful. Any other applications? Tim? All right, hold on a moment. Why would we not use robust PCA, like in what case would we just not, like why would we want to use classical PCA? It seems robust PCA is better. That's true. I mean, I guess the downside to robust PCA is that it is a little bit slower. But yeah, it is. Is it much slower? Like are there cases where it would just be much slower than classical PCA? Well, actually I'll say two. So it's slower. It's also more finicky to train. Like I felt like there were more kind of parameters to tune when programming it. The name robust PCA kind of makes it seem like you have to tune less. That's true. Yeah. And maybe that's like once you have it working, like some of this was like trying to get it working from the papers, but there was kind of like a parameter I had to change. I probably feel like, do you think it's slower? Because it's kind of using this randomization, it's another argument of maybe we always should use this. Yeah. It's kind of hard to find a good working version and you did a lot of work to get it to work properly. Now it does. Maybe this is the best place to start. Yeah, I think I would still, just because SVD is so easy to run when you're using someone else's implementation, I think I would probably run SVD on your problem just to make sure I want something better. But yeah, I mean, robust PCA has a lot of advantages and there are a lot of places it's applicable. Right. It's not like robust versus classical PCA. Oh, yeah. Why would anyone use classical PCA? I mean, since like classical PCA though is kind of a very basic SVD, that is something, I think I was even with the background removal problem and you shouldn't do this, but I actually did the robust PCA first and then was like, oh, let me see how I do on randomized SVD. And then was like, oh, randomized SVD was actually better than I expected. So that's up here, like this is what I was getting from, yeah, just like the randomized SVD, which is like a single line. But yeah, like I do think robust PCA is a great algorithm to use. Yeah. Good question. All right, yeah, let me maybe go back through some of the steps of the algorithm now. I'm going to kind of display some different information than I did last time, so hopefully that'll be helpful to get a different perspective. But yeah, just to remind you, so at the top we had kind of first went through kind of how, like what does our data even mean, how we're setting it up, you know, and turning this video into a single matrix where each column is one point in time. You know, we've kind of unwrapped the pixels to just make a straight column from each point in time. And we tried it with a rank, here's a rank one approximation just using a randomized SVD. And then we introduced the idea of robust PCA, we saw the faces. And then, yeah, I'll just show this again, this is, if you are really interested in optimization, these are Jupyter notebooks from Steven Boyd, who's at Stanford, has a convex optimization short course, which looks really interesting. When I tried to read Steven Boyd's book and watch his Stanford class, I found it kind of inaccessible. Okay, so this, and actually I guess before I get into the heart of the algorithm, I wanted to say, so the authors kind of define what they call a shrinkage method, and what that is, is basically they take any singular values that are less than tau and round them down to zero. And so you can think of that as another way of truncating your matrix, it's like kind of just ignoring the small singular values. So this shrinkage method will show up. I've got a question for you, you better find that Steven Boyd notebook. Oh, there's links to it in here, it's, let me find the section, so right above robust PCA, I have something that says if you want to learn more of the theory, which is. And some of these, I think I've mentioned this before, but I'm using, Jupyter notebooks has some extensions, and I'm using one that has the section folding, which is really handy. Oh, collapsible headings is what they're called for being able to fold these up and down. So that's definitely something you need to kind of install that to get it, but it makes it much easier, then you can kind of like open and close sections and navigate around pretty easily. And actually, let me just show maybe this high level of the algorithm this bigger. So what we're doing for our low rank approximation is taking, so here they called this D, curly D singular value thresholding, but that's basically where you just take the SVD, do the shrinkage operation on the singular values, so throw away the ones that are too little, multiply back together to reconstruct your matrix, and that's the approximation for the low rank matrix. The approximation for the sparse matrix is to take the, kind of just do the shrinkage operator of throwing away the small singular values. And this is the, sorry, I should say the alternating aspect is to estimate the low rank one, you're taking your full matrix minus the sparse one, kind of minus this error that you're keeping track of, and then you kind of alternate back to, okay, let's estimate the sparse one, that's the full rank matrix minus the low rank one, and you're kind of going back and forth between let me estimate the low rank one using the sparse one, and vice versa, let me estimate the sparse one using the low rank one. And so kind of how that looks in the code, so this PCP, which stands for principal component pursuit, and I should say principal component pursuit is just one algorithm for robust PCA, so robust PCA is kind of referring to this decomposition we want and their different algorithms to get there, and in this class we're using principal component pursuit. Kind of the heart of it is we've got this for loop, and so then I picked off a few of key things and added comments, and this has been updated on GitHub, but I said here, you know, we're updating our estimate of the sparse matrix by shrinking slash truncating the original, sorry, the original minus the low rank, so we're getting kind of the difference between that, and remember we're going for original equals low rank plus sparse, so whenever we do original minus low rank, that should be sparse. Then we update our estimate of the low rank matrix by doing the truncated SVD and reconstructing it, and then something that kind of makes this more intricate is we don't actually want to calculate the full SVD, so we need to say how many, so we're using a randomized SVD, but with randomized SVD we need to say how many singular values we want, so we're giving it a rank that we're interested in, and here that's called SV, and so SV is something that we're going to update each time, because note it would be, if we try to remember the dimensions of this matrix, I believe it was 4,800 by 11,300, we don't want to do the full rank for that, that would be a lot of values, so we're just going to do some of those, and the way it kind of tells how many to do is, so after you truncate, so when you're kind of dropping all the values that are less than, in this case it's one over mu, we're throwing away, if we find that we're getting more values than we need and are throwing away a lot of them because they're so small, then we just increase by one how many singular values we're getting, because we're basically doing fine, we're getting plenty since we're having to throw a bunch of them away, however if we're keeping all of them, that's a sign that we're not getting enough singular values, because they're all large enough, and so there might be more of a kind of large magnitude that we're not finding. So in that case we add 20% of the smaller dimension, which in this case is 240, to SV, and I think this, actually let me show this now, so I added some print statements that I didn't have last time to kind of say what rank it's using each time, so here when we use the algorithm, it actually starts with just calculating rank one, and then that was not enough, so we add 240, so that's 20% of the smaller dimension, which is 4800, so next time we've got rank 241, and we do that, and this drops down to 49, oh I think that's, so what it's resetting to is how many values you kept, so we did that and it only kept 49 of the values, although then, oh we kept 48 and added one, okay and then that time that wasn't, kind of wasn't enough, so we're going to add the 20% again, which is, and keep in mind each time we're doing this we're doing it on a slightly different matrix, you know, because we've been alternating back and forth, updating our estimates for LNS, so then we add 240 again to get to 289, and this whole time it's keeping track of this error, add another 240 and then our error is low enough and so the algorithm halts, but that's kind of a, gives a little different perspective, and let me go back to the algorithm because there's just a third piece, so we've got this aspect of, you know, we're updating how many singular values we want to calculate each time, and then here's where we're calculating the residual, so looking at X, which was our original matrix, minus our low rank matrix, minus our sparse matrix, and so that's everything that hasn't been factored into either the low rank or sparse matrix yet. And I also wanted to show, so this time, and this has been updated on GitHub as well, I made an examples list where I, at each step, and I just kind of randomly chose, I'm going to keep track of the picture from column 140, which is just a particular point in time, this is something that you wouldn't, what, row 140 because I've transposed, so there's a kind of check up here on the shape, and you, yeah, if M is less than N, then you transpose it, so yeah, but in the kind of pictures I showed before, it's column 140, so I'm just kind of keeping track of one point in time. This is something that makes it slower to kind of keep track of this list, but I just did that to get some visual output of how things are changing, and I think that can be a nice way of kind of seeing how it's working, and this shows up here. So now, actually, when I ran the algorithm, I got back my examples, and so you can see during the first iter-, or, at the end of the first iteration, this matrix on the left is the sparse matrix, it's not very good, it's just a black square, so I think it's maybe everything zero, and here's our matrix on the right, which is already pretty good because we've done an SVD, and so that's picking out the low rank part, but we really want to get the people as well, and so you can see next step, we're kind of starting to get some people, and then they're coming, coming more into view, and I actually don't see a huge amount of difference between the, the fourth and fifth iterations here, but I thought this was kind of nice to see at the end of each iteration, what is the low rank one look like, and what does the sparse one look like. And we kind of talked before about how finding the sparse one, I think, is more difficult than the low rank one, just in that, or actually, no, that's, that's not true, I was going to say with the, the people, I guess the sparse one, kind of as long as you have figures there, maybe you're not worried about all the edges, disregard, but so yeah, this is what happens kind of after each iteration. Any questions? Kelsey? How unique is the solution to this? Like, so in this case, maybe there's like a pretty clear local maximum. Yeah. If you have a situation where it's like less well represented by decomposition, does where you start affect where you end up? That's a good question. So there are some additional constraints on this problem. So like in general, you have to say that, or kind of this algorithm is forcing the low rank matrix to not also be sparse, I believe, like if you had a, yeah, if you had a low rank matrix that was also sparse, then it's kind of not, so the general statement I gave you of robust PCA is actually not fully, or is not uniquely defined, like there could be several different decompositions, but I think most algorithms kind of make additional assumptions to kind of constrain it further. But what happens if I were to give it like an image that didn't have, say a video where the background was changing constantly or something like that? Would you potentially end up with an unstable decomposition or something like that? Yeah. I mean, I don't know that that would even converge. Would it be more like on a map where you could possibly come up with any different solutions? I mean, I guess like the example of a video with the background changing to me sounds like there might be no solution in that, like, I don't know that that could be represented as a low rank matrix plus a sparse matrix. The other factor you have is how much of a penalty, so there's kind of like a parameter of kind of how much you want to penalize the low rank error versus the sparse error of those two matrices being off, and I think if that was adjusted that could give you a different, you know, like different results of like how bad, kind of having your sparse matrix not be fully sparses versus your low rank matrix not being fully low rank. I don't know if this makes sense, but I was just wondering like if it wasn't like constantly changing but more like in the real world you'd have more like switching scenes from time to time, would you end up with something where the low rank matrix is like a row for each scene, you know, and then the kind of thing that's multiplying by something on you at each time point, how much it represents that scene, would that work? I mean, obviously you could try it, I'm just trying to think if it makes sense, I'm sure if we... Yeah, that seems... They're almost like topics. Yeah, no, that does seem possible to me. Because we figured out we could recreate the original matrix if it didn't change the background with just a single... With rank one, yeah. Yeah, no, that does sound like it would work. I would love for you all to try running this code on different videos to see what you get. I suspect one where it's gradually panning might not work at all, but again, interesting to try. Yeah. Yeah, these are good questions. Any other questions? And again, we're not going into all the details of this algorithm, I just want you to kind of have a high level idea of kind of what we're doing with the alternating between getting the low rank and the sparse and with adjusting the number of singular values we're calculating for the low rank one kind of based on are we getting lots of little ones that we can throw away or are all of them meaningful? Yes. Yeah, just a quick question. So in terms of being grossly corrupted, how do you best define that being corrected? So, I mean, I would think of that as, I mean, you would have to set some sort of parameter, like I don't know if your error is less than tau, that small error, but I think of that as kind of something with where there are entries that are just completely wrong. It's not so much that they're close, but they're just off. For example, in this image, right, when we try to separate the background with the people, so in this case, how do you define the images being corrupted? Oh, so this image is not corrupted, yeah, that's a good question. So this image is very high quality, but the ones we saw above, let me go back up, okay, so these facial ones are grossly corrupted, so kind of like when you have the, yeah, like the pixels where it's like, you know, a lot of these white pixels are like just not even close to being the right color. They're damaged somehow. Right, got it. Yeah, and Netflix talked about having this problem, and so I would assume with Netflix that's where the, I don't know, someone has put in a movie rating that's just wrong. It's not how they felt about the movie, and perhaps, you know, they made a mistake or accidentally entered it or, you know, weren't trying to enter what they did. Yeah, and so it's kind of important to note here, so this problem with the faces is different from, and I mean this does, the original dataset here was showing pictures of the faces lit up from different angles, so you did have kind of the same face many times with different lighting sequences, but it's really different in that like kind of the faces, the low-rank part, which was the background, and then the sparse component is these pixels that are completely wrong. And so here, you know, we're only interested in the sparse component so we can get rid of it. You know, it's not like with the background removal where it's like, oh, we're interested in the people which are sparse, whereas here it's just like, okay, we want to know how to get rid of this sparse wrong stuff. And then I also wanted to highlight again, I think I have it on here, with topic modeling we could have used robust PCA, so we just used, you know, SVD or NMF on topic modeling in the first week, but here the low-rank part could be common words that are in all documents, and then the sparse part could be a few keywords from each document that kind of make that document different from the others. And I would actually, kind of going back to Tim's question of like, isn't robust PCA always going to be better, yeah, I would be really curious to see some topic modeling done with this, because it does sound really promising to me to kind of have kind of like the sparse words defining a document. Any other questions? Okay, so we're going to move on to the LU decomposition, let me just make sure I've done all the results. Yeah, so just again, here are, and I wrote a little kind of helper method plot images where you can enter your original matrix, the sparse and low-rank ones you got back, and then just however many points in time you want to see, since I think it's kind of most helpful to see, you know, them lined up next to each other from this is at time zero or kind of whatever corresponded to the zeroth column at a hundred and at a thousand. Yeah, let's start on LU decomposition. So above, we had used Facebook PCA, which is a randomized SVD, and then in the previous lesson we had kind of written our own randomized rangefinder, which was basically just a less robust version of scikit-learn's one, or I mean it did less error checking, so both of those use LU factorization, and so we're going to go into kind of how to do an LU factorization since we've kind of used some methods that use it. LU factorization factors a matrix into the product of a lower triangular matrix, that's the L, and an upper triangular matrix, which is the U. Then I wanted to check who remembers Gaussian elimination, and then who wants more review on Gaussian elimination. Okay, so let me, I'm going to go through one example, and we can even, if you want to see a second example, let me know, because I know it may have been a very long time since you've done Gaussian elimination, depending on when you took linear algebra last. Okay, so ignore the LU down here. We're just going to be, that'll come up in a moment, but we're just going to take this matrix A and want to do a Gaussian elimination on it. Actually, does anyone remember, so what's kind of the first thing you do with Gaussian elimination? Like, you're trying to reduce it to, yeah, well, I mean, there are a lot of different ways to talk about it. I was just going to say you kind of want to get rid of everything beneath one, turn those all into zeros first, although, yeah, like long range, I think you're doing other stuff. And this reminds me of, I think it was Trevathan that says at some point, like, so much of numerical linear algebra is just like inserting zeros into matrices, but here you kind of want to take this first row, and we want to get rid of the three in the second row, so we can, and down here, I'm going to put a matrix. So this matrix in the bottom corner, this is not something you would have done typically in numerical linear algebra, but we're going to keep track of what we multiply by. But here, I'm going to kind of multiply this first row by three and subtract it to zero this guy out. So first row is not going to change one, negative two, negative two, negative three. And our goal is to make that zero, and so we did that, yeah, we did that by multiplying by three and subtracting, so that makes the top one would become negative six, subtract that, so negative nine plus six is negative three, negative six and zero, oh, positive six is actually zero minus negative six, and then times three, those are going to cancel out because we ended up with negative nine minus negative nine. So that's kind of step one of Gaussian elimination. Questions about that? And initially- Why do we, so just to warn you, Gaussian elimination is a thing that made me never want to say linear algebra. It didn't come back to it for like 20 years, so I'm very lazy on this, but like why is it that we're allowed to like subtract rows from other rows, like what are we doing? That's a good question. So really, let me see if I, is there a way to add more space on top, Jeremy? Well, I wanted to be able to write more, but okay. So really what we're, often where these problems come up is when you're solving a system of equations, and so you might have, I don't know, maybe this was one x minus two y minus two z minus three w, and then we weren't dealing with a vector b, but often you would have some value over here, like I don't know, maybe this is five, and then you have another equation that's three x minus nine y, no z term, minus nine w equals 10 or something. And so one way to solve these, if you're just kind of thinking about them as systems of equations, is you could multiply the first equation by negative three and add those together. Because think, you know, if you had a solution to this, you know, values for x, y, z, and w, multiplying both sides by negative three, you know, that equation's still going to hold. You multiply by negative three on both sides, and then you can add those together as kind of a legitimate thing to do as well. And that's even kind of, I think, like a more intuitive, like if I gave you, let me go to a place of the page that I don't think we'll need, you know, if I gave you some problem and we're like, solve, actually, this might be too easy, okay, because you'd plug in the x, but you could also think of that as like, oh, you know, like, let me multiply each side by three and subtract those, and then I'll get the equation just in terms of y. So that's where this kind of idea of multiplying by rows by things and adding or subtracting them. Is that helpful, Jeremy? Yeah, I think so. I guess I'm still trying to make the connection to solving equations versus, like, is that equivalent to any decomposition or just this particular decomposition, or why can we, why is the fact that we can do this in simultaneous equations mean we can do this in matrices? Well, so the matrix is just representing the system of equations. And so... In this particular case, if we were to write the first line as 1x minus 2y minus 2z minus 3w, and what kind of the broader problem is you might be interested in solving Ax equals b, and you might be interested in solving that for a lot of different values of b, and you wouldn't want to have to kind of go through and solve it separately each time, and so finding this, this is kind of getting ahead, but finding a factorization of a is going to let us then have a quick way of solving it for a bunch of different b's. We kind of just factor a once, and then we could solve it for a bunch of b's. So this idea of thinking about the system of equations is specific to the idea of finding a factorization. That's where we can use this technique. Yes, yeah. So something, kind of to spoil the punchline, is we're going to see that Gaussian elimination is very closely linked to LU factorization, and they're actually kind of even talked about almost like interchangeably. So this kind of process, even though kind of in a linear algebra class you're usually not seeing that it's giving you a decomposition, it's the same method. Yeah, so going back to the decomposition, the first column with zeros beneath the diagonal. So we've gotten rid of this 3 by multiplying, oops, multiplying the top row by 3 and subtracting. How can we get rid of this negative 1? Can I say it louder? Yeah, so add the two columns together, or two rows, sorry, the first and third rows. We add those together. This is a zero, and I should note that adding is actually, yes, it's like multiplying by negative 1 and subtracting, which seems like a more roundabout way to talk about it, but we're going to put a negative 1 down here to keep track of how we got rid, how we got this entry to zero. So yeah, add those, go to zero there, which is nice. This becomes 2, and this becomes 4. And then for the last one, we can multiply by 3, or think of it as multiplying by negative 3 and subtracting. So let's put a negative 3 here to keep track of, and then here this, oops, this becomes zero, negative 3, negative 2, this will become zero, or does that, yeah. Oh, negative 12, so I reversed my signs, thanks. And then this one is 20, thanks. Okay, so then the next step is we want to fill up, oh, did I write down the last, yeah. Now we want to make everything below negative 3 have a zero. And so it's nice the, oops, row 3 has already been done for us, so I'll just put a zero here. I don't have to do anything. And again, the top row is not going to be changing. The second row is not going to be changing. Actually neither is the third row, so we're really just working on the last row at this point, so negative 3. And note that the reason, okay, so now that we're trying to get rid of this negative 12, we're going to use the second row, because we don't want to flip this zero back, the one in the corner, flip it back into being something that's non-zero. So we use the negative 3, so multiply that row by 4, and subtract. So let's keep track of that 4, we'll write it down here, we used a 4. That becomes zero, this becomes negative 4, and this one stays negative 7. This is totally reminding me of why I hated Gaussian elimination. Let's never do this again. The good news is, once we get familiar with it, then we can get a computer to do it. So that's the benefit of learning the LU decomposition, is we're going to get a computer to do this for us in the future. But I did want you to remember what the process was. Half the final assessment will be doing this by hand. Okay, so now we're getting rid of this negative 4 that's right here. So the good news is we've got the top three rows are in good shape in terms of moving to our upper triangular matrix that we want in Gaussian elimination. So really we just need to multiply the third row by 2, or I guess negative 2, and subtract. So let's put a negative 2 in here, keep track of that. And then, yeah, this becomes zero, I've got negative 7 plus 8 is 1. And so we're done with our Gaussian elimination. Sometimes at this point, actually I think this is where you would stop. And then, amazingly, the matrix here is actually what I had written right here. And so it's going to turn out that A is equal to L times U. So we're keeping track of what we were having to multiply each row by to add and cancel out the other row. We're just going to fill in ones along the diagonal, and then kind of zeros everywhere else. But yeah, this is L, so kind of keeping track of those coefficients, and what we got from our Gaussian elimination was U. First let me ask, are there any questions just kind of about the process of Gaussian elimination? And I checked, and Khan Academy has kind of several videos on this if you did want to watch more Gaussian elimination, Jeremy. Linda? So the Gaussian elimination is the process to calculate the L and the U? Yes. Yeah. And sometimes like trevithin I think almost kind of uses LU, like LU decomposition and Gaussian elimination interchangeably. Matthew? Why is it called Gaussian? Gauss was... Gauss discovered it. I feel like other people might have simultaneously discovered it. I'm not sure. I'll look up the history on this. The linear algebra is that old? Yes, yeah. Yeah, and I think like this question of solving linear systems of equations is one that shows up a lot and is useful. Yeah, that's a good question. Yeah, other questions? Vincent? Yeah. When we're calculating like the values in L, like I can get behind the lower diagonal values, but then the diagonal being one, it's like we did one of row one to row one, and we did one of row two to row two, and that doesn't like fit with the numbers, so I'm like curious about the intuition behind that. Let me think about that one. Yeah, that's a good question. It's just so that when you multiply L times U, you don't zero out or anything, so like probably like killing the first row or whatever. Right. Now let me check, let me just check the time. Okay, so this would be a good time to stop for a break, so let's meet back in seven minutes at 12.08 or 12.09, and if you're bored, this is a good time to take the mid-course survey. The link is in Slack. Thanks. All right, let's get started again. Okay, so we left off by doing the LU decomposition, and we had this matrix A, and we got L and U here. I'm going to say, and Trephathan goes into more detail about this, but you can think about Gaussian elimination as kind of being this series of multiplying by matrices where each matrix is kind of just doing like one of the operations and kind of putting those together to get your final decomposition, but as Jeremy pointed out, Gaussian elimination is a bit tedious to do by hand, so we want to automate that and have a computer do that for us. So this is the basic LU decomposition, so that's where you think about each operation you do as a single matrix, multiplication. So here the product of all of those would end up giving you, maybe in this case, the inverse of L. If you want to talk about this later, feel free to leave it for now, but why is this decomposition interesting other than the fact that it appears in something you've seen already, or is it basically just useful as a component of other algorithms? It's useful on its own for the use case I mentioned before, if you're solving a linear system of equations multiple times, there are other ways to do that as well, but it's one way of solving a linear system of equations, particularly when you're going to do it for multiple vectors b on your right hand side of the equation. It's helpful to store these, because that'll speed up getting your x's kind of once you have the L and U. Yeah, so I wanted to go through how, kind of how we have the computer doing this. So we're making a copy of A that's going to be our U, and then remember U is what, actually I should probably go back to this, U is what we ended up with at the end of our Gaussian elimination. So we're going to kind of be starting off with A, we copy that into U, and then are going through these steps to get it in this form. And then L is where we're going to store kind of what we're multiplying by. So what we do is loop through K, and then, so K is in range n minus 1, since we don't need to do anything to the first row, J, or sorry, to the last column, J is going through K plus 1 to n, so that's kind of how when we're working on a particular row we need to do everything beneath it. And we'll set Ljk equals the ratio of Ujk to Ukk, so that's kind of seeing how, you know, whatever spot we're at compares to the diagonal, which is Ukk, and then we subtract off that times Uk comma to Kn is giving us the Kth row, kind of the remaining columns. So this is just the process that we did before, kind of put into code. And then we return L and U at the end. There are questions about this? And actually maybe it would be helpful, let me add a step, this might be too much, but I'm going to try printing L and U each time. I may not be connected to the kernel, let me check, okay so run this, oh this is not aligned well, but you can, actually let me just print U, because that will be more interesting, U will give us kind of the matrix A that we were reducing with the Gaussian elimination. And so you can see here, first we're introducing a zero here, and then a zero beneath it, and then at the third inner loop we've completely zeroed out the columns, the spaces in the first column. Then we go into the next one and we start zeroing out everything below the diagonal in the second column, and so on. Any questions? I don't recall, but this algorithm looks slow, like we're having to do a separate operation for everyone at the diagonal, so that's already n by m, and then each time you're having to do it on a whole row. That's right here, so the big O for this is 2 times 1 third n cubed. So what you can think of it is it's kind of like a pyramid, the amount of work it's having to do, in that you've got your outer loop, which is approximately n iterations, but then you're just going from that point up to n, so that's kind of like forming a triangle, and then within each of those you're just doing work on the kind of entries from k to n. And everything's dependent on everything else, so this doesn't look very paralyzable at all. That's true, yeah. I'll make a note to think about ways to speed it up, because now I am curious. We're going to go in a different direction today, Jeremy, which is kind of thinking more about the stability of this, but yeah, speed is interesting too, but yeah, so that's how we get kind of the big O of n cubed, and the 1 third n cubed is coming from this idea of kind of like, yeah, it's pyramid-like, how much work you're having to do on each row, and typically with big O you don't talk about the coefficients, but I have them in there just in case. So yeah, but before we get to that, I just wanted to highlight that the LU factorization is useful. Solving ax equals b becomes lux equals b, then you can solve ly equals b and ux equals y, and what might be nice about solving, so solving kind of steps two and three, those are both triangular systems. Can anyone say why those would be a better thing to solve than your full system? Kelsey? Because you could sort of even like backwards substitute, just do one equation at a time. Exactly, yeah. So kind of take like the tip of your triangle where you just have one variable, solve that one, back substitute that into the equation that just has two variables and so on. So it is always nice when you can get stuff into a triangular system. As for the question of memory, so above we created two new matrices, L and U, however they can actually be stored in matrix A by overwriting the original matrix, and remember with L, since the diagonal is always ones, you don't actually have to explicitly store that. And so this is called doing something in place, and it's a really common technique in numerical linear algebra. If you ever wanted to use A again in the future, you wouldn't want to do that because you are you know writing over it, but one of the homework questions, and I'll probably put this homework up on, I might put it up this afternoon or Thursday, it won't be due till next Thursday, actually like a week from Thursday, is to modify LU to do the algorithm in place instead of kind of creating this separate L and U. Yeah, any questions about kind of worker memory for LU decomposition? Okay, so now we're going to consider another matrix A, which is 10 to the negative 20th, 1, 1, and 1. Actually I want you to take a moment just on paper to use Gaussian elimination to calculate L and U. Okay, thank you. Raise your hand if you want a little bit more time. Raise your hand if you're ready now. Okay, I'll give you another minute. All right, does someone want to say what they got for Al? Okay, Vincent, Jeremy. So row by row, I've got 1, 0, and 10 to the 20th, 1? Yes, excellent, and then this. Yeah, so 1, 0, 10 to the 20th, 1, and then u is 10 to the negative 20th, 1, 0, 1 minus 10 to the 20th, which would be approximately negative 10 to the 20th since that's so much bigger than 1. And so if we enter these as matrices, I'm going to call them L1 and U1, here they are. Actually, first I should ask, are there questions about getting this Gaussian elimination or the LU for A? And you're probably feeling a little bit nervous because we had this 10 to the 20th and 10 to the negative 20th, which is really different from 1. So we might run into some problems. So we do an LU decomposition on A, and this is using the LU that was written above, so kind of just our own implementation. And what we get is, is about right. Oh, I redefined everything, I shouldn't have done that. Okay, sorry, hold on a moment. Okay, got A being what we want, so we can check. L1 is close to L2, so remember L1 is the one we calculated by hand. L2 is the one we got from our algorithm, or got from our implementation of LU. U1 is close to U2, so that's good. And now if we check that L2 times U2 is close to A, we get false. It's not. So this is kind of interesting that like each component, L and U, was close to the answer, but L times U is not close to A. And so this is an example. So the LU factorization is stable, but it's not backward stable. And I'm going to kind of define what those mean in a moment, but I think it's helpful to kind of keep this picture in mind. So an algorithm f-hat for a problem f, so you'll think of f-hat as kind of, yeah, kind of like how you're implementing this, whereas, you know, f is like the true problem you're interested in. And we say it's stable if for every x, f-hat of x minus f of y, norm of that over the norm of f of y is less than machine epsilon, where y minus x over x is order of machine epsilon. And so the way Trevathan says this I really like is that a stable algorithm gives nearly the right answer to nearly the right question. And so kind of translating that here, so x is kind of the right question that we're truly interested in, and we're just saying that there's some y that's close to that. So we've got, you know, nearly the right questions, that's y. It's, you know, it's almost x, where this is how we're defining almost. And then the right answer is f, but we're just getting nearly the right answer, which is f-hat, so the right answer to nearly the right question. So f-hat is like the thing you've written, and f is like some theoretical real version, and then what's the difference between x and y? So x is the true problem you're interested in, and y is the problem close to that. So like- Why is it f-hat of the true problem minus f of the not true problem? That's a good question, and that's because we're saying that the- you're getting nearly the right answer to nearly the right question. So here, x would be- I feel like it's helpful to kind of like unpack his statement in reverse order, but like nearly the right question is y. So x is the right question, nearly the right question is y. The right answer is f, so we're looking at- the right answer to nearly the right question would be f of y. So again, like what was truly the correct answer to your representation y, that's not quite the true question x, but it's close. That's what's giving you the f of y. But then we're just saying you're getting something that's close to that. Don't wait for me to understand. Okay. It's- I liked- I liked this interpretation because I think it's- I mean, this is like a math definition of stability, and I appreciated him kind of giving a way to like think about it in more colloquial terms. But yeah, it's okay if you don't- don't fully get it. Yeah, it's kind of thinking about like how we formalize these ideas of stability, that we have like a general notion to of, you know, like we want the- the answer to be close to what we want, but kind of how do you formalize that. So fortunately, backward stability is actually simpler than stability, and it's also stronger. So that means that anything that's backward stable will be stable, but not the reverse, as we just saw with the LU factorization, because that's stable but not backwards stable. And so an algorithm f-hat, or problem f, is backward stable if for each x there's a y close to it, such that f-hat of x equals f of y, and so Trevathan says that's a backward stable algorithm gives exactly the right answer to nearly the right question. So that's why we're taking yeah, f-hat of x equals f of y. And so, um, yeah. Well, we'll probably talk about this more next time. So now let's look at the matrix 1, 1, 10 to the negative 20th, 1. We'll call that a-hat, and I want you to take another moment and use Gaussian elimination to calculate what L and U are for this matrix a-hat, which is similar but not identical to our a from before. All right, and who needs more time? All right, does anyone want to say what they got for L? Okay, Kelsey? Thank you. So before it was, um, uh, yeah, before it was 10 to the 20th, now it's 10 to the negative 20th. Oh, right, sorry. Yeah, which, yeah, is subtle. Yeah, and then before we had 1 minus 10 to the 20th in our U, and it's 1 minus 10 to the negative 20th. But yeah, so how was, um, well actually I guess first I should show the punchline, which is that we take our L, U, D composition of A and check if it's, um, if A is all close to L and U, and this time it is. So that's, that's an improvement. Vincent? Jeremy, can you throw the microphone? Shouldn't the top row in U still be 1 and 1? Um, let me check. Yes, it should be. Thank you. Catch. Okay, and so then, um, how does A compare to A hat? Look back at A. Okay, so Tim is doing the gesture to indicate that two rows were switched. So really this A hat was just like A, but we switched the rows. And so this is, um, and first I want to say like switching the rows, um, if you think about this so often, you know, uh, each row in a matrix might be a different, um, data point. Switching the rows should be totally fine if those are different data points. If, um, you know, like different samples and some, like, measurements you've taken. If these were systems of equations, switching the rows is fine. And actually no matter what because you can, uh, um, think of this as just multiplying by a permutation matrix P. So here we did 0, 1, 1, 0 is P. Multiply that by our A to get A hat. Um, apply Gaussian elimination to P times A. And it actually turns out that it's okay to kind of permute them because at the end you could permute them back to, um, what you had originally. And so this is what, um, pivoting is. Um, is to, yeah, switch the rows around. And basically, and this is called partial pivoting, and that's where at each step you want to choose the largest value in column K and move that row to be, um, row K. Because the, the problem we were getting into before was when we were dividing by a really small number, uh, which we don't want to do. And so kind of choosing a large number, um, is going to be more stable. Yeah, and so that's going to be a homework problem of kind of adding partial pivoting to the LU factorization. And then we can see here, uh, this is just going through, this is the, no this is a different A than we had before. Um, you'll get back, uh, kind of LU and then a matrix of what your pivot, or your permutation matrix P. So that's letting you know how you permuted the rows. And then you can check that the, this is an example from Trevathon. We get the, the same answers. Um, there is something called complete pivoting which permutes the rows and the columns. Um, we're not going to get into that into detail. Um, it's, it's so time consuming that it's rarely used in practice. I think it's basically never used in practice. Any questions about pivoting? Why would you want to permute the columns? Um, so permuting the columns, you can actually kind of run into the the same issue of, um, yeah, I guess having like numbers that are too small. And so by like permuting the columns, you're able to get the, the largest value which is going to be more stable in a particular place. But yeah, it's just a lot to keep track of. So this is more for something that's used, like this is more for making the computations more accurate. It's not like something we would do. I guess like all of this is to make the, make our actual like computer algorithm return a more accurate answer or is it like analytical? Is there an analytical reason for doing this? Um, I mean, so I guess, yeah, so it's, yeah, it's to get your computer to return a more accurate and like part of what was going on with this, you know, example where the 10 to the negative 20th failed. I mean, that's less than machine epsilon, but it's something that like, you know, that's causing us to get something that's not even close as a result. I will, I guess, kind of jump to the punch line perhaps, which is that even with partial pivoting, it's going to turn out that this algorithm is technically not stable. Complete pivoting is? Yes. Well, actually, let me confirm that. I think that it is. I'll look it up and confirm. The issue with partial pivoting is that the matrices that would, so it's easy, we'll see it in a moment, we're going to construct a matrix that is unstable. However, those matrices are so rare in kind of nature or in practice that you don't actually get them. Yeah, good questions. Yeah, and I'll confirm about a complete pivoting, which yeah, I believe is stable, but too slow to actually use. So now we're going to look at a system of equations of this form. So here we've got a matrix where we've got ones on the diagonal, ones in the last column, negative ones below the diagonal, and we're saying that times x equals this matrix of ones with a two in the second to last entry. And so this is a very kind of particular type of matrix that we've constructed here. And so I wrote a function, makeMatrix, that will generate that for any size n to kind of have this form. And it's just, yeah, setting the last column to ones, starting off with the identity, setting the last columns to ones, and then setting everything below the diagonal to negative one. We're making this vector over here, which is going to be all ones except a two in the negative second position. I was going to have you do Gaussian elimination on the five by five system, but you might be. I see heads shaking, yeah. Let's not do that also in the interest of time, and you may have had enough Gaussian elimination. No, it wasn't just you. Enough Gaussian elimination practice. No, this is this is why we have computers. But here we're going to use the scipy.lenouge has an LU solve, and into that, actually let me pull up the, whoops, pull this up, you're going to pass a matrix and then the vector that you're trying to, you know, this is going to solve ax equals b, and you're giving it a and b, and it'll return x for you. And so I'm running this for matrices of size 10, 20, 30, 40, 50, 60 to see what happens. And then I'm going to plot what the the last five values in the solution are. Keep in mind, since the system is getting bigger, you know, the first one has 10, and then 20, 30, 40, 50, 60, but to have a way to compare them. And here we're printing them out, and if you look at the last five values, you might notice, okay, the last value for the first one, the 10 by 10 system, is pretty close to one, and we've got something pretty close to a half, negative a fourth, negative an eighth. You might see a pattern, and that seems to continue. Okay, one, this is in scientific notations. You kind of have to pay attention. Okay, this is really 0.5, negative 0.25, negative 0.125. So we've got this pattern for 10, 20, 30, 40, and so on. And then notice what happens when we get to 60, our answer. So this should kind of raise a flag for you that it seems to not be in keeping with where we were. And so here where I've plotted them, it's a little bit hard to maybe see because you basically, so I've made these dashed lines of different colors, and basically these are completely on top of each other. So this like thick multi-color line, that's the solution for the last five rows for 10, 20, 30, 40, 50, and then we jump to this when we hit 60. So do they all want to say what they think is happening when n equals 60? Any guesses? And actually I'll give you a hint. Let me find it. Actually okay, I'll just, so I had written out the Gaussian elimination. I won't make you go through it, but what you end up with for this system is, this is the 5x5 version. This is your U, so kind of all zeros in there. You've got a diagonal of ones, and then the last column, this is kind of pathological, it's doubled, you know, in every entry from what we were doing because we had all those ones. And then we're saying, oh that's equal, and so over here something very similar was happening, 1, 2, 4, and then 9 and 17, and those are off a little bit since we had that two entry amongst our ones, otherwise it would have been kind of perfectly their doubling. Actually just first questions on what I've shown. So I just took the 5x5 of that system, I did Gaussian elimination at home, wrote it out, and this is what you get. And now if we were gonna go back and backwards solve that, and I haven't written all the zeros in over here, but these would all be zeros. So let me make, so often when people write that they'll kind of just have like a big zero to show, like okay, those are all zeros in there, where it doesn't say otherwise. Actually this is a good time to point out U is a sparse matrix in this case, it has lots of zeros. So if we were going to start solving this now, you know, we'd start with the last row, 16 times X sub 5 equals 17, and we're gonna divide through by 16. So now any kind of guesses, maybe what was going on when we hit n equals 60? Sam? I guess 2 to the, the value at the bottom right of our upper triangular is going to be 2 to the 60. Yes. The plus 1 is lost, so it's just going to be 60 plus 60. Exactly, yeah, yeah. So we've gotten so large that we're kind of going to lose that distinction of having, and I think, I think it might be 2 to the 59, but yeah, exact idea of kind of, yeah, like you've got 2 to this huge exponent, and so then that can't keep track of this like little plus 1, it kind of overflows, and so we're getting, getting the wrong answer. And so this is a, you know, this case where Gaussian elimination or LU factorization, even with partial pivoting fails, and I think there, yes, there if we had had complete pivoting and we're pivoting on our columns, or you know, permuting our columns in addition to our rows, we wouldn't get this giant number that had added up to 2 to the 60th, and so that would have solved this, but since this case basically never arises, and complete pivoting is so slow, we don't use it in practice. And here this is kind of saying that in math more formally that it has this idea of a growth factor, rho, which is the maximum u value over the maximum a value, and that in, that times machine epsilon is part of the kind of error that you can see, and so yeah, it's a bad thing when rho is huge. But yeah, to get back to Tim's earlier question, question, complete pivoting would have solved that because we would have been making those big numbers while we were pivoting on. And so LU, because many numerical linear algebra classes kind of teach LU first, although Trevathan kind of points out that it's not, it's not a typical case even though it's, you know, a very widely used algorithm, but this idea that kind of Gaussian elimination with partial pivoting, so this is a quote from Trevathan, is utterly stable in practice. In 50 years of computing, no matrix problems that excite an explosive instability are known to have arisen under natural circumstances, even though, you know, we saw one today that was very contrived. So it's, yeah, I think it is kind of interesting, and it's also something that it's, the definition for stability, it's okay that we have rho, rho in there, but however, you know, if, yeah, if rho is huge, then this kind of breaks down. Any questions about this? So actually you say that when we have n equals 60, this lin-alk.au solve doesn't give us the proper answer, right? Right. Yeah, but if we don't do this LU factor and we just try to solve it on matrix and vector, it still gives the proper answer. So why would we care about this LU factorization? Sorry, if you just try to solve it, how it gives the proper... Yeah, so if we just do like lin-alk.solve without LU and we don't apply this LU factorization, it gives us the proper answer. That's a good question. So it looks like that this authorization just doesn't agree. Wait, did someone else? Tim? Oh, no. Okay. Oh, yes, that's a good, good suggestion. Trying to take this out. Space or no space. This is something, what? This is a good question. I will answer this next time. I want to look into more what scipy-lin-alk.solve is calling. You can see from the docs at the end it says it's calling lap-pack, which Rachel mentioned earlier. I mean it also says that it's called, oh, those are just for the Hermitian symmetric. Okay, so let me check that. So this is how you would look up lap-pack, the name of a particular method. I want to look into that more because I would have guessed that they were using LU solver. Actually, can you try it just real quick with a higher power, like do like n equals 80 or something? Oh, it also factorizes it into a permutation matrix. Yeah, but typically that's just a partial permutation matrix. Oh, and it also calculates the condition number. Oh, so it computes it with error balance. It uses iterative refinement, so it does more than... Okay, yeah, because I feel very certain that it's not doing complete pivoting because that's really slow. Okay, yeah, so this is doing something much fancier. Iterative refinement is applied to improve the computed solution matrix. Yeah, that's a really interesting find that that is working here. And this is often the advantage of using LAPAC, something that's been really, really well optimized. So we're at time, but I'll return to this question next time. And I think that's um, otherwise most of what I had to say about LU, yeah, LU factorization. And this is covered in chapters 20 to 22 of Trevathan.

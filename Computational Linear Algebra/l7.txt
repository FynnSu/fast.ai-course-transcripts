 Beginning of our end of last class, I introduced just kind of the start of the start of lesson four on compressed sensing of CT scans. And I just wanted, we talked about a few really useful concepts at the beginning and I wanted to review those. And so one of those was broadcasting and NumPy and that's how NumPy deals with arrays of different shapes and kind of the rules for whether or not you can do arithmetic on them. Can anyone remind us what the kind of two rules for broadcasting are in NumPy? Connor? Yes, exactly. And then for lining them up, do you start at the kind of beginning or end? Wait, Jeremy, throw it back. Yes, exactly. Yeah, thank you. Yeah, those are the rules for broadcasting. Start with the trailing dimensions, which is the far right-hand side, and then two dimensions are compatible if they're either equal or one of them is one. And so then I wrote a few problems as review. So suppose we have, actually is the font large enough on this? Is that better? Let me, okay, so we've got V, which is 3, so 3, just a tuple. M is 3x3 and A is 2x3x3. And so kind of before we actually run these, which of the following operations will work and which won't? So take a moment to think about these and then you can do like a thumbs up, thumbs down, maybe. Okay, does anyone want more time? Okay, so for the first one, A plus B. Thumbs up if you think that'll work, thumbs down if you think that it won't work. Thanks for everyone that's participating. Yeah, I see mostly thumbs up and that's correct, that works. So, and yeah, V may be a little bit confusing since it's kind of just got this single dimension showing up. But yeah, that works. The threes line up and then we've got the two and three before. And so what's happening with that is that, it actually might be helpful to put A here so you can see one has been added kind of to this whole first column along both matrices, if you think of this as being like an array of two matrices. And then we've got two was added along this whole second column and three was added here. Let's go on to the next one. How about A plus M? And I should leave the dimensions up so you can see. Thumbs up if you think A plus M will work, thumbs down if not. Excellent. So yeah, so all thumbs up and that works because the three by three lines up with the three by three at the end of two by three by three. So we can do that. And there matrix M is kind of being added to, you know, this first matrix in A and to the second matrix in A. And how about A dot transpose, or A transpose plus M? Thumbs up or thumbs down? Great. Yeah, I see a lot of thumbs down. And this is nice, we get the error, operands could not be broadcast together with shapes three three two and three three. Now you understand what that error means if you get it. Any questions about broadcasting? Kelsey? So that, and actually it's probably helpful for me to to show it. So that's reversing the, let me also put, let me change this to A so you can have kind of like the visualization. So here we're taking 5, 10, 15. So basically we end up taking kind of the the first column of this matrix and the first column of this one and putting those together to get this new first matrix. And then kind of second column here, 10, 20, 30, and negative 2, negative 4, negative 6, and putting those together. And then 15, 30, 45, negative 3, negative 6, negative 9, and putting those together. Let me do this again. I think it'll be clearer since we've got three showing up as a dimension twice. Let me do something with completely different dimensions. So you're, I think you're reversing the order of the dimensions. Yeah. Okay, but now I've got A234, so I think this will be kind of a nice example. So when we reverse the, completely reverse the order of dimensions it becomes 432. And then this is what A transpose looks like. And I definitely also recommend kind of looking at small test cases of these because I think often it's helpful to kind of visualize how they're changing. Yeah, good question. Any other questions about broadcasting? Okay, so next up we talked about how SciPy stores sparse matrices and that there were a few different matrix storage formats. Let's see, can anyone describe how, how coordinate-wise storage works? Matthew? Exactly, yeah. So you're keeping track of the index for the row, the index for the column, and the value that you want there. Great. And what's a, what's an advantage of this, this approach? Actually, advantage or disadvantage? So less memory than a dense matrix. Yes, yes. Right, so it's harder to get an entire row or an entire column. Exactly, yes, yes. So if you wanted to find everything in a particular row you would have to go through your array of row indices and pick out those kind of corresponding values. Alright, and so then kind of this was an example we saw last time of, and they've used this nice color coding to show like 75 is in row six, column four. Over here we have value 75, row six, column four, and that's how it's being stored. Next step is, oh and then this is also kind of illustrating for things like matrix multiplication, you can just, you know, look up what you have in your value array. You're not having to go through all the spaces where you have zeros. Alright, so compressed sparse row data structure. Can anyone describe what's going on with that picture? Sam? So to save space, the row indices are not repeated. It just says which of the n observations is the first that has that row in the order by rows? Exactly, yes. So this uses even less space than the coordinate sparse storage because it just keeps track of a row pointer, which as Sam said, it's just kind of telling you the first place that, or the first value that that's on that row. So for row one, the third value is the first to show up there, which is 22. And so we see that's the first value on row one. And for, let's see what else they color-coded, row three, the ninth value is the first there. And so you can see the ninth value is 42, and that's the first thing on row three. So what are some benefits and or pros and cons of this approach? I think when we are trying to look for non-zero elements of that matrix, you don't have to go through all the non-zero elements, you can just skip row by row until you get to the row which you need. So if you're looking up a particular row, yes, it's very quick to do that. Yeah, fast lookup by rows. And then can you think of a disadvantage? By columns it will be just the same complexity of n in the worst case. Yeah, so column lookup is painful, or yeah, more painful because you're kind of having to go backwards to calculate. Thank you. Wait, throw the microphone. It's for the video. So the row pointer, great question, is telling you, so there the index is telling you kind of where that row starts. So row zero, the first one is in value zero. So how index is being used is different for value and row pointer. Actually it's probably easier to look at. So the first three values are in row zero. So you really just need to store a zero and then this three lets you know, okay, now we've moved on to row one. And the kind of starting with index zero, the third value 22, so if you go over here, the third value is 22, that's the first thing in row one. And then we've got three values here and then value six is the first thing in row two. So down here we store six for the row pointer. So it's that saying row two starts with value six, which is 31. So the for the row pointer, yeah, the index is telling you the row number. And it's telling you that, though, in terms of what the index for the value is. Tim's got his hand up. Yeah, that's a great way of putting it. Like you can kind of think about like you've turned, you've turned this matrix into an array of values 11, 12, 14, 22, 23, 25, 31. And you, yeah, you want to keep track of where you're switching from one row to another. And so you're just kind of keeping track of, okay, we switch, or you know, we started a row here at 11, then another row started at 22, another row started at 31, another row started at 42, and so we just need a pointer to those spots where a new row is starting. Okay, let me go to, you will like this table better, Terrence. So this table, okay, you're welcome. Any other questions about compressed storage or sparse arrays? And so we didn't go into detail on the compressed column storage, but it's the exact same idea as compressed row, only you're keeping track of column pointers. And it's fast to look up a particular column, but in that case it's slow to look up a row. Kelsey? That's a good question. I would have to look that up. Let me write that down. Something also to remember is that converting between these types is linear time, so I think generally they kind of recommend converting if you're going to, yeah, be looking up a lot of rows or looking up a lot of columns. But yeah, I'll tell you the exact times next time. Any other questions? I have this question. So like, once we know that we have sparse matrix and we try to decide which format to use, so like can we just look up for every format, like this format is better for matrix multiplication, this form is better for like trying to get the inverse, this form is better for like to get the transpose matrix, or maybe this form is better for multiplying matrix by columns or stuff like that. Yeah, so yeah, different operations, and I don't know if it, I think some of the things you listed will probably be equally good in both, but yeah, it's very dependent on what algorithm you're using. And so particularly like looking at your code as well of if, kind of if you're accessing things by row or by column in a loop, that should be a factor. And I've copied a little bit of, yeah, so here like the SciPy documentation says that for multiplication or inversion it recommends either CSC or CSR, so I think that those are probably equally good. Yeah, if you're doing something else where you are, you know, looping through your rows or looping through your columns, you'd want to think about that. You're welcome, good question. And then it also says COO I think is a good format for constructing your matrices since you don't want to have to be calculating the row or column pointers yourself typically, and it's efficient to convert then, so kind of use COO to make your, pass your matrix into SciPy. Okay, yeah, so we'll be using both broadcasting and sparse storage today. I'm going back to this problem of CT scans, and so I showed last time in this article that I think is very nicely written. It starts with can math really save your life? Of course it can, and talking about kind of how CT scans work. I want to try introducing this in a different way today. So if you were actually kind of working on CT scan data, what would happen is you would be receiving some data that might like look like this on the left-hand side, and you're trying to reconstruct this full picture. And the idea, and so this is in real life this is usually 3D, we're just gonna be looking at a 2D picture. And here are the dimensions of this. This is L width for both of them, but then this is just a seventh as tall, what you started with. And the idea is, you know, it seems clear if you did a ton of x-rays of someone, you know, from every angle and got all this data that you'd be able to reconstruct the original, but you want to limit how much radiation people are experiencing. And so the idea is that we're kind of getting, you know, like much much smaller amount of data and want to reconstruct this picture that includes a larger amount of data. So it's kind of like the opposite of data compression and that you're kind of starting with the smaller amount of data and need to uncompress it to get the full picture. And then it's also you're having to find kind of the meaning of yeah kind of what these measurements, so these would be the measurements from different x-rays shot at different angles and different locations and then getting back to okay what would that mean for the picture. There are questions about kind of the setup. And so this problem, since we're using, we're gonna use data that we generate for the problem, like we're gonna be building this picture, it's different from a kind of real CT scan in that we first will have to make our data, then get what the measurements would be, and then we're seeing if we can reconstruct the original just from these measurements. Yeah, so I'm gonna show some pictures but I kind of what it'll look like is that an x-ray shot at a particular angle and a particular location, you're just gonna get a single number from that. So you know here we have a line intersecting with our picture and that would result in just one number. So kind of one pixel in this left-hand side of our measurements. And then this is a kind of brief review. So from the background removal lesson, that was background removal using robust PCA was written as this optimization problem. And do you remember what's special about the L1 norm? Shout it out. Yeah, it shrinks things to zero resulting in a sparse solution, right? So it induces sparsity because yeah, it's kind of pushing many of the coefficients to zero. And so you'll notice this picture that we'll be trying to construct is sparse. So we'll be using the L1 norm today. Okay, so the first part of this is just generating the data and I think this uses some interesting NumPy methods that some of them I hadn't even seen before. So I think it's an interesting process. Again, if you were really working with CT scans, you're not building your own fake data set. And so here we're choosing 128 for the dimension. So this is 128 by 128. And I'm gonna kind of walk through what this code is doing. So this is the whole method for generating synthetic data. This will return kind of one square like this, but we'll walk through that more slowly. And to walk through it, I'm gonna go through kind of a smaller example of just creating an 8x8 picture that has five points. And we'll see what the points mean in a moment, but that'll give you kind of the density of the data set. Of those white circles. So there's a NumPy method called ogrid that returns something as a column and as a row. And so here we're getting the numbers 0 to 7 as a column and as a row back. And then this is actually going to be very handy in a moment. We're gonna use broadcasting with this. So we want to end up getting something that's 7 by 7 or sorry 8 by 8. So we're kind of getting this row and column and we can do some operations and together with broadcasting that's gonna go from you know having something that is 8 by 1 and 1 by 8 to something that's 8 by 8. It's kind of a handy trick. And so this is basically kind of like the equation for a circle. We're saying X minus the center point squared plus Y minus the center point squared. And we get and then kind of combining that with broadcasting so the X part of the equation is still just going to be you know this 8 by 1 column. The Y part is this 1 by 8 row. And when we add those together we get an 8 by 8 grid. And this kind of has a density of being 0 at the center and then getting larger as you go further away since we're using the circular equation. Questions about this? Jeremy? Oh yes that's a great idea. And here since 0 is the first entry of both you can kind of see the original X in the first column and the original Y here. Okay so now we're constructing something called mask outer and basically that's because we're just going to be kind of interested in this center in the kind of the center of our picture. And I think this is kind of fitting with the idea of you know if you thought about doing a CT scan of someone's you know someone's torso that you're kind of getting this like circular cross-section is what we're getting here. So we're just adding an inequality so kind of taking you know our XY grid and then seeing when it's less than this radius squared that gives us this outer mask which is kind of us you know a ball of trues surrounded with false at the border. And this is also important I should say I guess because the the x-ray will be coming at different angles like this kind of ensures that it's going through the same amount of you know person being x-rayed kind of from each angle the x-rays traveling a similar distance. So we take that mask oh that's our outer mask and then we're constructing something called mask where we just randomly generate some points in a 2d grid. And so that was a parameter earlier kind of how many points we wanted to generate and in this case it's five so we've still got something eight by eight and it's all zeros with five ones and we can plot what that looks like so here we're just plotting kind of each each point is a pixel zero or one and we have five five white boxes. We'll use a a Gaussian filter then to kind of blur that so that's kind of going around these places but making it a bit more kind of continuous. And this is I mean some of this is uh you know this is specific to kind of like we're trying to get these like globby looking shapes in our in our data. Here we combine seeing when so now we've kind of gotten the Gaussian filter version we're just gonna take the parts of that that are greater than the mean for it and then also when when those points lie inside that inner circle. And so that's what this is. And then actually I should um maybe pull up ND image well I'll come back to that in a moment we're kind of using this additional library here is where we get this binary erosion feature from and this is just picking out the middle so kind of this was our whole thing we want to separate that into what was the middle and what was the exterior. And so this is how we make yeah kind of make those globs that you saw and this is a more pixelated version since it's just eight by eight whereas our original was much larger. But this actually let me scroll back up so kind of this circular shape or not circular globby shape here that's how we would get kind of the globby shapes here. Jeremy oh yes yeah the caret sign is XOR which is exclusive or actually first let me just see where ND image okay so that is a sci-fi library I'll go back to that yeah so Jeremy pointed out the the binary erosion gives us the the inside really what we want is what's left over and so we're taking an exclusive or with kind of between this picture the result that's everything exclusive or with this interior just leaves us with the exterior. And then I just wanted to pull up because I think this is an interesting library we're just using a little bit of functionality from it but sci-fi has an ND image for multi-dimensional image processing kind of as a library so if you're interested in it I think that would be something to check out with yeah with different types of filters. Other questions kind of so far what we're doing to create these globs? Can I have a question about this Gaussian filter? Is this something like the EM model? Remind me what the EM model is. So we assume that there are like a few Gaussian distributions and then we assign probabilities to every pixel that was derived like from white distribution or from black distribution by looking for the maximum likelihood. I think this is more like putting a Gaussian centered at each point and kind of looking at what shape that creates. At each point, at each pixel, right? Yeah, so we have like the five pixels that we've made white and we kind of like are putting Gaussians centered at each of those and looking at the distribution that creates. Let me pull up the documentation though. Oh, there it goes. Yeah, I guess it is like Jeremy saying that he thinks it's like a convolution. Yeah, I think that's accurate. Like what this, um, I'm just going to grab my stylus. What this looks like in 1D, I guess kind of in 1D, would be like taking the points you have and if you put like a Gaussian around each of those and then you're kind of looking at a, looking at the method that covers all of these, you kind of hear about this in kernel methods sometimes, kind of like looking at where your data is and then kind of using Gaussians to construct a distribution kind of based around that data. Yeah, good question. Any other questions? Matthew, on the Gaussian? Yeah. So what that's doing, yeah, so I think Jeremy yeah, pointing out it's a convolution is helpful. It's kind of like blurring the points around wherever we had these white pixels. So we were kind of starting with this picture and so you could think of that as kind of doing this averaging method to kind of even out the squares around this and so that's why we kind of have it very white in the corner, but you know if we take a point here and get the average of the stuff around it, you know that's gonna be gray whereas a point here an average of everything around it's still gonna be black and that's a way that kind of like smooths it out. Hi, you're welcome. Okay. And we can always come back to this later too. So this is, yeah, so that's kind of just to getting that kind of picture with the globs that in the end it's gonna be what we're trying to recreate. The next step is to generate the projections and so this will be the kind of the single value that we get from shooting an x-ray at a particular angle through our picture. So again I'm gonna walk walk through this and this is something where you don't have to understand every line of the code. But more I want you to kind of get the idea of like what the steps are. I should also just while it's on the screen want to highlight and here we're using sparse dot COO matrix in this line for our operator. Yeah, so here we're kind of feeding in what the dimensions we want it to be and we have L as well as L divided by 7 and again the idea is that we're using less radiation than kind of you know taking it at every single angle and every single location. And so this creates, we'll kind of come back to what this what this matrix looks like, but we've got a sparse matrix in coordinate format and then there is a dense method which converts sparse matrices to dense and it's very useful and that'll be helpful when we're wanting to plot what we're doing. We need to have it dense for for creating these images. But the idea is that the so the first the first dimension is going to tell you the angle and we only have L over 7 angles and then we have L positions. So that's kind of every single pixel height and then each image is L by L. So we're gonna be getting something that's L over 7 by L by L by L. So just to kind of get a feel for this matrix, this is an angle indexed with three and we're kind of have it starting from zero or just kind of position zero and then you'll notice the positions moving and basically getting lower each time. So then this is position one which is really from this distance almost looks identical, but the line is slightly lower than the previous picture. You can go on two and it becomes more obvious if you say go to position 40. So now we can see that okay we've definitely been lowering this angle, kind of keeping same angle in this case the angle with index three and going to a different position. So now we're lower kind of down to 40 and then we can look at other other angles at vertical location 40. So now I'm changing the first coordinate to be a 5 and this is a different angle. Actually let me kind of show like if you did 4 comma 40 that's giving you something in between angles 3 and angle 5. And so what we have is a kind of a matrix that's keeping track of all these different angles and locations. Here we can see this is a completely different angle the one indexed with number 15 and at location 40 still. This is angle index with 17 at location 40. So we're kind of getting all these different angles at every possible location. Where here the location basically corresponds to the height within the picture. Questions questions about this? This is a little bit tricky because projection T is it's a matrix in four dimensions which can be hard to think about. So here we're kind of just looking at these two-dimensional pictures from it by indexing the first and second dimensions and then we're seeing everything that's in the third and fourth dimension. Matthew? So the projection and we'll kind of get to that in a moment. The projection is gonna talk about how we're projecting into one dimension which is weird to think about but it's gonna be kind of taking this you know this just line at a certain place together with our whole image and just getting a single number from that. Yes, so oh and so that's that's representing what the CT scan gets because the CT scan you know you've got this well the cross-section you know two-dimensional cross-section of a person and you're sending an x-ray at a particular angle and even though those could be represented as you know like a 2d picture of the x-ray and the 2d picture of the cross-section of the person you just get a single number from that which is like the reading that the you know the CT scan or MRI machine is picking up. So kind of getting that down to a single number. Yeah so typically so projections are kind of take something you know that goes from a higher dimension to a lower dimension and I think this is somewhat unusual because I feel like people often don't talk about projections into 1d. Any other questions? Okay so yeah now I'm just kind of showing kind of you can transpose you know x-ray kind of going through these points we're just gonna get a single single number from that. Oh here I've also showed just how much intersection there is but any idea here is that the the x-ray kind of going through different materials of different densities that's gonna affect the reading at the end so it's got a sense of kind of what what yeah kind of like the density of what it's passed through based on kind of what the the measurement is. And so here I wanted to illustrate that and so we get this from so here Proj stores the projection this is at the very beginning when I showed that little L by 7 L over 7 by L matrix this is that the readings and so here where there was a lot of intersection like this this x-ray was having to pass through a lot of the a lot of the glob globules and we get a kind of larger number 6.4 and then here you know we have a line kind of going off off through this side part it's not really passing through much and we'll get a lower number. Yeah just 2.1 so this is kind of how we're capturing information of how much the line has passed through. And then we're also adding we add some noise to that so kind of assuming that this would would involve some noise in our measurements and then this is this is what we're referring to is kind of the projection this this matrix but that's just all the measurements so it's a bunch of 1D projections coming from these lines at different angles. Tim? What's the star project shape inside that argument? That's a good question. So star in Python is used to unpack list so there's something called star args and star star and actually let me pull this up because this is a really useful Python concept but it's useful when you want to be able to pass yeah like a list or dictionary of argument so the dictionary you use two stars. Is that a Python 3 thing? No that's in Python 2 as well, yeah. Let's see. Yeah and this this can be useful if you kind of like have a bunch of parameters and don't know how many you'll have. I actually find this really weird and annoying because like in some things in parts of NumPy they expect a shape as a tuple and sometimes they expect the dimensions as arguments. So in this case the thing that Rachel is using is the dimensions as arguments so you have to put the star there. So that's true about this issue with shape but I do want to say star args shows up other places that could be useful. Yeah actually let me show an application I really like of star. Okay yeah I'm gonna be careful about that. And are you are you all familiar with zip? Zip, yes. Wait that's not. I didn't know that. Put list at the front. Okay so what zip does is you can pass it to list or tuples and it'll of the same length and it will go through and kind of pair them together. So here we're picking off you know one and four, two and five, three and six. But something that's kind of fun is that I'll call this C. You can undo zip then by using zip again together with star. So in this case we wanted to pass to zip three things one, four, two, five, three, six, and we can do that by unpacking this list into kind of the the three tuples using star. So that's a way that lets you get your your original two back. Okay other questions about this figure? And that was yeah a great question about star args. Matthew? So you have the value that you're getting back. You're reading your highlighting in the matrix. Say that again, the values that you're reading. Yes, yes, yeah. And then what are the x and y axis again? For this matrix the, great question, the x-axes are the different angles and then the y-axes corresponds to the different kind of vertical position. So when we showed how like you know there's a line here and then lower yeah that's what the the y-axis is corresponding to here. Thank you. You're welcome. Alright so now that, so here the hard part was creating this data set and it's actually going to be pretty quick to get back to our answer. We're going to use use regression to try to recover the data. We'll start out with the L2 penalization. This is called a ridge regression. And so here we're just using scikit-learns, linear model ridge, fitting it to our projection operator and projection. And this is this is what we get which is not not very good. And so this was using least squares error. Now if we use L1 error which is called a lasso regression, this is what we get which is really close to our original picture. So this is a case where we had you know very kind of like perfectly sparse data set and so L2 regression did really poorly and L1 regression did a lot better. There are questions about this? Tim? Oh okay, Tim and then Terrence can go next. Okay, okay let me think about that one. Let's hear Terrence's question. Where are you getting polynomials from? Well, I use polynomial but it could be a line. Yeah, yeah. We're basically adding what we see. Each pixel is the summation of what is visible from a bunch of different angles. Yes, yeah that's a good way of putting it. Yeah, so yeah each pixel yeah it's gonna be a sum of what was seen from a bunch of different angles. So you're combining the lines, if you will, from these x-rays and at a particular point you know how much is there based upon the angle and the height of the electron at that time. I think that's basically what Tim's question was that you just answered. What? Oh okay, Jeremy's voting for a break. I just yeah, well let me say one more thing but then yeah then we'll take a break and come back to this. Actually yeah let's take a break and we will discuss this in more detail. So let's be back in seven minutes so that would be 12.05. I'm gonna start back up. Yeah so let's talk about what's going on with this regression. So just kind of to restate the problem. So when you have a CT scan and you get the kind of like the picture that looks like the insides, actually there's one up here, you know it looks like a picture of organs or tumors. That's not actually what the CT scans producing directly but rather you know it's taking these measurements and then you know this field of compressed sensing is around how do you get from those measurements back to or not back to but you know create a reconstruction of what the kind of the person's organs look like. So here, so Tim asked a great question about what's going on with this regression. So we have something called the projection operator and that's basically kind of like if you think of linear regression being AX equals B and you're trying to find X. In that case, so the projection operator is A. The original picture, which is this, is like our X and then B is the measurements we got, go back up here, this. And it's hard to think about the projection operator because it's a four-dimensional matrix or four-dimensional tensor and so that's what we were trying to kind of get at up here with all these plots of Proj T. So this is just basically kind of like a part of A. So A is this four-dimensional matrix and we can look at slices of it but A represents the lines coming from different angles at different locations. So it's kind of like all all the angles and locations that the CT scans taking and it like what those look like in 2D. So let me also, actually I think I can show that down here. So Proj operator dot shape. Oh and that is also, it's been distorted. So this kind of was the four-dimensional thing. We reshaped it so it was easier to look at but it's, this is coming from having 128 over 7 which is 18, something that was 18 by 128 by 128 by 128 and we've reshaped it here just to make it two-dimensional. But the the idea was four-dimensional that we kind of had angles by vertical location of where the x-ray was shooting then by x-coordinate by y-coordinate of the cross-section. So that's our matrix A and then X which we're trying to solve is this 128 by 128 image and so we're kind of thinking about like okay we put all these x-rays from different angles are kind of effectively being multiplied by this image and then we get out you know the smaller matrix of measurements and so we're trying to go backwards. Okay we've got the smaller matrix of measurements, we've got all the x-ray angles, how can we get the the picture of what what those x-rays were passing through. There are more questions about this. And I should say that here yeah like with the CT scan it kind of makes more sense to think about okay what is what is this x-ray passing through. What we're doing though with the fact that it's multiplying is just kind of taking advantage of the fact that we have a lot of zeros and ones and basically every place where either the person's organ is zero you know we've got a black spot or where the x-ray is not passing those are all zero and so those will zero out and so the only thing that gets picked up is where both of them are nonzero and those are the intersections. So you can kind of see this is a different one where they're intersecting. So if we were kind of multiplying the picture of the organs times the picture of the x-ray that's just gonna pick up the intersections because everything else zeros out and gives us kind of a measurement of how many and how many locations were both of those nonzero. I have a question. So I'm using down here prog dot ravel. Does anyone know what dot ravel does? Is that a hand? Sam? Exactly yes. Yeah and I actually feel like unravel would make more sense but it's yeah kind of flattening it into a one-dimensional array and that's handy here because we're basically converting what would be a four dimensional matrix times a two-dimensional matrix equals a two-dimensional matrix. This kind of how I think the problem makes sense of thinking with like the CT scans but we've converted that into a you know just typical matrix by a vector equals a vector. And so that's why we're having to unravel these readings just to be a you know one-dimensional array. A lot of people look very puzzled so I'd love to get more questions. Sam? Yes yeah so the four dimensions and let me go back up. The four dimensions are coming from our projection operator and so those are the angle that it's at and we have a total of L over 7 angles which in this case is 18 angles is 18 in this case yeah so we've got 18 distinct angles L positions and that return so this is an L by L matrix and that's just kind of each vertical possible height is the position and then the image for each is L by L and so what we're displaying here is we're kind of indexing on those first two we're picking an angle on a position and then showing the image and so this is our way of trying to look at little pieces of this four-dimensional array is we're just kind of you know indexing on the first two and then viewing a 2d picture. So really we're multiplying the last two dimensions by, we're multiplying the line in the last two dimensions by X and that's giving us a single value that's like how much it's intersecting. Yes. But we have first two dimensions simply because we have so many different lines at different angles and the index corresponds to which of those. Exactly. So that again have three dimensions with the yes. Yeah yeah yes that makes sense yeah yeah so another yeah that's a great way of thinking about it. Another like what's happening here with the dimensions is we're actually kind of reshaping this four-dimensional thing go down to where I have it. So the projection operator kind of was just write it 128 by oh no this one's 18. 18 by 128 by 128 by 128. But 18 times 128 is 2304 and 128 by 128 is 16,384. And so what's that what this is doing is you can kind of think of you know we've turned our picture with the cellular globs or organs or however we want to think about those. That was two-dimensional. We've kind of unraveled that into like a single vector and then we're gonna multiply that two thousand and three hundred two thousand three hundred and four times. Basically like that's getting multiplied with this x-ray for we have two thousand three hundred and four different x-rays you could think of it since we wanted to take an x-ray reading from 18 different angles at 128 positions for each angle. So that might be helpful to kind of think of you have 2300 kind of separate yeah separate readings. The this one yeah so this one is actually let me write it out. This is 18 by 128 but we have unraveled this into just one thing that's 2304 long. So kind of for each of those 2304 multiplications we're doing we're getting back one pixel here or one spot in this matrix. So this this is the result of doing 2304 multiplications. Yes yeah so this is what the the CT scanner is actually measuring. But then it knows the angles and locations that it's shot the x-ray to get each of these points. Yes yes good questions. Are there questions about this? Matthew? So this is B in the regression. You can think of this is like the column on the right-hand side and then A is... I need to fold some of these up. A is the collection of all these matrices. Yes yeah yeah so it's um like I think it yeah by the time we do the regression it is 2d. Yes yeah yeah yeah I guess actually yeah so here we've just reshaped it as 4d to get the kind of meaning for the different points. And in its 2d form this picture would be like a single row. So we've kind of taken this whole picture and made this this is one row and this picture is another row and so on. Because we've just kind of like unraveled that picture and so each row is a single angle and single location but then all the XY coordinates for that angle. Well there are 18 angles but there are 128 locations. Yeah so they're 2,000 rows. All right so um yeah so these I should say these pictures here these are all part of the matrix A. The picture at the bottom that's the vector B. So this is this is the vector B when it's unraveled. No it's more like each each each single location in this like each pixel here is a reading from one of those like entire pictures above. So kind of taking the angle at a particular location that just gives you a single value. So that would just be one entry here. Oh you're welcome. Oh. Matthew? Oh. Let me think about that. Are you thinking that the what would be the kind of the background you're trying to remove? Yeah I mean the tough thing is you don't actually have like you don't have the picture of the line going through the through the globs it's you just have like a single number for each of those. Yeah yeah so that's kind of a yeah like the point you're trying to get to is the yeah the picture of the globs. Yeah. Any other questions? Okay we'll probably review this at the beginning of next class. So definitely continue to think about it. Email me if you think of kind of other other questions because it is it's just I think a very different way of thinking kind of like understanding what all the pieces are here. But it's a really interesting application of regression. Alright so let's start on these. Oh wait another question. Yes yeah so this kind of even 18 angles is enough to get yeah like a really great reconstruction. Yeah where it's kind of like a naive guess might be like oh maybe I need 128 angles in order to you know be capturing data of the same dimensionality of what I'm trying to recreate. Yeah and there's it there's an entire field that kind of focuses on this called compressed sensing or compressive sensing. This is a topic David Umansky knows a lot about. Yes yeah. Alright so the next next lesson continuing with the theme of linear regression. We'll be looking we'll be using polynomial features but so different linear regression problem. We're going to use a data set from patients with diabetes. This is kind of a classic data set from a famous paper and it's included in scikit-learn which is nice. I think I mentioned in an earlier lesson but let me pull this up again that scikit-learn has a number of data sets that that come with it both kind of where the data is already kind of present with scikit-learn or where they give you data kind of loading utilities that you can pull extra data in. So I think that's a nice nice feature. So we'll be using this data diabetes data set so we just call load diabetes to get the data and then we know what the feature names are just from looking at the documentation. And so this data is going to have age, sex, BMI, blood pressure, and then various blood serum levels. We're going to use a trained test split which is kind of scikit-learn gives you one there. And in here you're probably familiar, actually I'll ask you, can someone explain why splitting into a training and test set is important in machine learning? Kelsey? Exactly. You don't want to overfit to your particular data set. You're trying to come with a model that will be general enough to handle new data and so you want to hold out some of your data as a test set and not look at it until the end. So we're gonna hold out 20% of our data for the test set and that'll allow us to evaluate how well our model does on new data. So the problem of linear regression is AX equals B and typically A has more rows than columns. So this would be when you have more data samples than variables. And yeah, we're trying to find an X that minimizes the L2 error which is the sum of the squares of AX minus B. And so this is a kind of assuming that our data that there's this linear relationship between the things we're measuring and what we're trying to predict. And so in this case that would be a linear relationship between age, sex, BMI, different blood serum levels, and the kind of long-term health outcome of the patient. So we can use, and we're using linear model from Scikit-Learn has linear regression for us. So we kind of create a regression, fit it to our training set with the Y training data, and then make a prediction on the test set. And then here written just a little helper method regression metrics that returns the mean squared error as well as the mean absolute error kind of between the actual data and between the prediction. And so we run that and we get 75 for the mean squared error and 60 for mean absolute error. Your questions kind of on the setup or on this is kind of a kind of a basic linear regression so far. All right, so now we want to try to improve this. One way we can do that is by adding more features. And so we're going to try adding polynomial features. And the idea here is that instead of just having it, you know, our outcome depend linearly on these variables, we can also look at things like age squared or age times sex, age times BMI, and so those are the interactions between the different terms. And this is kind of letting us look for more complex relationships. And so we're just kind of going up to the the squared version, but so that gets how each term interacts with each other term as well as their squared versions. And so now, and actually I should have probably put the dimensions before we start this. So we were starting with 353 rows, so that's probably 353 different patients, and 10 variables. If we add these polynomial features, now we've got 65 different variables that we're looking at. And so do a linear regression again. And we've just added, we kind of created these training features and now using those to fit our regression, and that improves our error. So we, before I think had 75 and 60, now we've got 55 and 42. Unfortunately time is squared in the number of features, and so this is this is going to slow us down a lot. And we can look, let's go back up here. Here's the time. So before it was 535 microseconds, and I think we've seen this before, but using percent time it in a Jupyter notebook runs, so here this is doing seven runs, a thousand loops each, kind of runs it multiple times and then gives you this average of how long it takes. So it's a really useful way to compare the times of things. Okay so that was 535 microseconds, so now we're up to 635 microseconds. So it's not that much worse in this case, but it's something that's going to, on larger data sets, be an issue. Or was there also look at the plus-minus. What? Oh yes, I'm sorry. So yeah, the one above was to do the linear regression. This is just to pull. Okay, so this is just to create the features, sorry to create these polynomial features. Sorry about that. And so that's, you're having to take all the data points and multiply them together, you know, all possible pairs and doing that calculation just to even create this set of polynomial data is taking even longer than it took just for us to do the linear regression, but we're still gonna have to do a linear regression on this this new data set. So this is this is kind of a time concern. So it's something that has, yeah, improved improved our error, but it's gonna slow us down. So we're gonna look at a way to speed up the feature generation. First I should stop. Are there any questions on how the polynomial features allow us to kind of create a better model than the plain linear linear regression, but then they take longer to calculate? Okay, so we're gonna use Numba today, which is a Python library that compiles directly to C. Jake Vander Plas has some nice tutorials on them, and I like in this tutorial he says that a lot of people ask him isn't Python pretty slow, and he's kind of got an answer of why he likes, and Jake Vander Plas I should say has a PhD in astrophysics and works at the University of Washington, has like a Center for Data Science, and he he's a contributor to a lot of Python scientific packages and does a lot of speaking, and also I think helps run like University of Washington's kind of like at their Data Science Institute. They have like, you know, data science for social good, summer programs and things, but he goes through kind of reasons why he likes having a Python implementation as opposed to just, you know, doing the whole thing in Fortran or C, and it's, you know, Python code is easier to read, understand, and contribute to. Pure Python packages are easier to install than Python wrapped C or Fortran code. It's easy to use at scale, so I think these are kind of some good arguments for why you would want to maybe compile some of your Python code to C as opposed to just switching to C or Fortran completely, and Numba is one way, one way to do that. So from here, from Numba, we're importing a number of methods and decorators and types that I'll kind of talk about as we use them. So we're going to step back from the problem of this linear regression or polynomial feature generation and just talk about what Numba is and how to use it and kind of go through a sample problem. So Numba is going to allow us to kind of avoid memory allocations and copies and give us better locality. So we're going to start with kind of this toy example. So this is something just in plain Python and NumPy where we have some number of observations that we're looping through. We've got two different arrays, X and Y, and we kind of want to take each each value in X and Y, do some computations on them, and then store them in array Z. So this is just a process in Python. Notice since we're using NumPy we can give ZZ a type. This says, or I'll ask you, do X and Y have types here? And actually this is confusing because I named these variables X and Y. Do X and Y inside this inner loop have types? Does anyone want to make a guess? So Malton is suggesting float 32. So for our arrays X and Y that we're passing in we have float 32 and ditto for ZZ where we're going to put our result we have float 32. But here inside the loop we have these kind of just, you know, temporary variables X and Y we're using and those don't have specific types. So even though they're actually all, we're always going to be putting float 32s in them, Python doesn't know that. And that's kind of one of the things that, so Python is not a typed language but in language type languages like C or C++ you have to say, you know, what type of variable it is when you declare it and that, you know, lets it know how much memory to set aside. And so it's kind of better for optimization although some people prefer that, you know, Python is flexible and you can give it different types and not have to declare it. So here we're timing this time to just kind of take in our two arrays, go through each of them, do all these operations, and create an output and that's taking 49 milliseconds. So this was a kind of the worst version. What would be a way, before we even get to Numba, a way to improve this? Just speed it up. All right, Jeremy, through the microphone. We can try to avoid the for loop. Yes. Hoping that the hardware will make it in parallel. Yes. Yeah, so this for loop seems like a bad idea because it's like we're individually going through, you know, each pair of elements in the arrays. We're doing the exact same thing but NumPy lets us vectorize that. So we can here take out the for loop all together and kind of just do the operations on the entire matrices thanks to NumPy. And so if we compare, this was 49 milliseconds, this is 35 microseconds. So notice that's over a thousand times better. The units have changed from milliseconds to microseconds. So that's a big speed up. So that's some really great that NumPy lets us vectorize. Does anyone remember the other kind of technical term or buzzword that we often say along with vectorize? It's actually an acronym. Yeah, SIMD. So single instruction multiple data. So that's what NumPy is letting us do here because we are doing the same instruction on all these different entries in X and Y. So now Numba has something called a just-in-time compiler decorator. And in Python decorators are kind of these methods that are applied to functions to kind of alter how your function works that show up with an at sign above the above the method. So this is pretty simple. So we imported JIT from Numba above, and again JIT stands for just-in-time compiler, and we just add add JIT on top here, apply it to, so we've got our for loop back so this is kind of or this is the the code from the top original Python process. And what this does is so we're no longer vectorizing but we do have better better locality here. And now when we run it we're down to six microseconds. So the version with NumPy was almost 36 microseconds. So this is six times quicker. And then the one in plain Python with the for loop was really slow. That was like 50 milliseconds. Does anyone want to say what does it mean to have better better locality? Kind of learned about this with Brad? Yes, yeah so it's well it's um you're kind of you're accessing data that's close to each other in memory. Yeah I mean so it's often I mean you can talk about time locality or space locality but it's kind of using things that you access kind of like when you access or when you access something use it multiple times before you write it back to slow memory. So kind of like while you have it in fast memory like do all the operations you need with it as opposed to writing something you know or carrying something from slow memory to fast using it taking it back to slow bringing it to fast again using it again trying to consolidate all those uses and then yeah I'm using things next to each other. Great, thank you. And so here I think it can seem to be kind of counterintuitive that we've put the for loop back in but the idea is that we're kind of you know we're picking up X and Y which we need and then we do a bunch of operations with X and Y and this is you know this is kind of a toy toy example but you could have a more complicated algorithm where you are you know using them multiple times. What would happen what would happen if we were doing this in numpy without the for loop? So kind of this example up here has potentially has poor locality. Why is that? Sam? Yes yeah so the issue with the the numpy array is if you want to do something on all of X maybe all of X doesn't fit in cache and so you're pulling in part of it you know doing the operation on that part then you have to pull in the next part of X do the operation on that part pull in another part of X do the operation on that part and then you go on to the next line and you're like oh I need the first part of X again and this is you know if X is a really long array. Thank you. There questions about this? These are kind of getting at the concepts we saw back in week one with the highlight video. Okay so let's go on to make this even a little bit better. Numba also has a vectorized decorator so here we're using numba instead instead of numpy so at this one we use the just-in-time compiler decorator and kept the for loop now we're taking out the for loop and just have the vectorized decorator and it's actually like I mean it's almost equivalent to the using the just-in-time compiler which was 6.4 microseconds this is 5.8 but it's kind of good to know these two different options. Tim? What's the difference between numba's vectorized and numpy's vectorized? So this is still compiling to C and it's still optimizing what happens. So it's not doing like with numpy where it's doing this like you know having to use all of X so it's you know might pull the first part of X into cache and then pull in the second part and then pull in the third part and then the next line it has to use X again so it starts again with the first part of X. Numba has optimized that away so it's not gonna yeah do that sort of thing and the the reason numba's able to do this is because because it's compiling to C so Python like when it's running dynamically it you know it like it doesn't know that oh I'm gonna use X again in the next line and so this is kind of wasteful that I'm pulling each section into cache and then putting them back when I'm gonna have to do that again one line from now whereas C is able to optimize things like that and so that's kind of what numba's getting you is this kind of big picture optimization. Numpy does use C yes that's true but that's kind of on an operation scale whereas like yes yeah yeah whereas here even though numpy yeah is using C on an operation scale you still kind of it doesn't have this knowledge between lines of like hey I'm using X in this line but I'm about to use X again in this very next line yeah I'm actually not sure about that I think they recommend just using one but all I can look at that what Oh using but thanks good questions okay yeah and actually I guess in the interest of time should probably stop here so next time we'll be applying this to going back to our problem of wanting these polynomial features but wanting to be able to create them more quickly I also wanted to remind you that Thursday homework two is due and so is the draft of your writing assignment Tim um so I mean the better shape your draft is in I think the better feedback you'll get which will make your final better but it should be it's like reasonably complete all right any other questions all right I'll see you on Thursday

WEBVTT

00:00.000 --> 00:08.000
 Yeah kind of a topic for tonight, privacy and surveillance. It's in the news a lot.

00:08.000 --> 00:11.360
 Thanks, yeah, thanks for kind of discussing this and you've already

00:11.360 --> 00:16.040
 brought up several points that will come up again kind of throughout

00:16.040 --> 00:20.680
 this evening. My goal is to talk a little bit just about kind of some facts about

00:20.680 --> 00:24.600
 the current state of things, how things are surveillance technologies are being

00:24.600 --> 00:31.960
 used, some of the risks of what can go wrong, my rebuttals to a few common of

00:31.960 --> 00:36.480
 really these are kind of like rebuttals to rebuttals that I hear, and then some

00:36.480 --> 00:42.440
 steps towards solutions. So one of the readings was the New York Times article,

00:42.440 --> 00:47.400
 your app knows where you were last night and they're not keeping it a secret, and

00:47.400 --> 00:50.920
 this was from a collection of data that I think came from over 200 million phones

00:50.920 --> 00:55.840
 in the United States. I thought it was kind of helpful how they visually broke

00:55.840 --> 00:59.560
 it out, kind of showing the stories of individual people. So here's a woman that

00:59.560 --> 01:04.880
 the New York Times identified and kind of tracked her going to and from her job,

01:04.880 --> 01:09.720
 visiting her ex-boyfriend, going hiking. At one point she goes to a doctor's

01:09.720 --> 01:14.080
 office, they keep track of like how long she's there, kind of these very, you know,

01:14.080 --> 01:19.960
 potentially intimate details. Another, yeah, her going from her home to

01:19.960 --> 01:22.720
 Weight Watchers and kind of looking at this over time. And it was something

01:22.720 --> 01:28.280
 where they had, or it says here, her location was recorded over 8,600 times

01:28.280 --> 01:36.600
 in four months. So a lot of very granular data. And the New York Times has kind of

01:36.600 --> 01:41.400
 done a series of articles on related, and I think even like a few separate data

01:41.400 --> 01:46.000
 sets, but all kind of in the same idea of kind of just how much data is being

01:46.000 --> 01:50.400
 collected from people. Are there any thoughts on this article or reading it?

01:50.400 --> 01:56.960
 I just heard this talk today actually about political campaigns using this

01:56.960 --> 02:00.340
 data from all these sort of nefarious companies and merging with the voter

02:00.340 --> 02:05.720
 records in this way that was like pretty shocking to me. And also I think the

02:05.720 --> 02:10.800
 thing that was sort of shocking about it is that these campaign cycles are so short

02:10.800 --> 02:14.800
 there actually can't be a lot of safeguards and how the data persists

02:14.800 --> 02:19.360
 long-term and sort of there's no central responsibility for this stuff. So each of

02:19.360 --> 02:24.440
 the campaigns is in some version of it in this way that was pretty frightening to me.

02:24.440 --> 02:28.120
 Yeah, and that raises kind of another issue also of how data sources can be

02:28.120 --> 02:30.920
 combined. And so sometimes when you're just looking at a single data source,

02:30.920 --> 02:34.880
 while this, you know, seems revealing enough, it could be even significantly

02:34.880 --> 02:39.480
 more revealing when combined with other data sets. Just while it's going back there,

02:39.480 --> 02:44.280
 Erin raised the point about also how data can be more revealing in aggregate

02:44.280 --> 02:48.360
 as well. And so there was a case with Strava a few years ago, you'll remember

02:48.360 --> 02:54.060
 when they released this data set publicly, that ended up revealing not

02:54.060 --> 02:57.480
 just kind of the location of several foreign military bases, but even kind of

02:57.480 --> 03:02.320
 a guess at the inside architecture. And that was something that, you know,

03:02.320 --> 03:05.520
 releasing the data for any one person wouldn't have revealed that, but

03:05.520 --> 03:10.880
 altogether it did. And another kind of image from the New York Times article

03:10.880 --> 03:14.080
 of here's someone coming to a Planned Parenthood, they know what entrance

03:14.080 --> 03:17.920
 they're using, what time, what time they leave, how long they're there. This is

03:17.920 --> 03:26.280
 kind of very intimate data being sold and distributed. There's another article

03:26.280 --> 03:33.640
 on Rinder and OkCupid. OkCupid asks, you know, questions about drug use

03:33.640 --> 03:39.640
 and sexual preferences. Again, kind of very personal and intimate data. And so

03:39.640 --> 03:44.520
 and it's hard because they're also these like layers of intermediaries that this

03:44.520 --> 03:49.600
 data often gets passed through. So Grindr's app includes software from MoPub,

03:49.600 --> 03:55.880
 which is Twitter's ad service. MoPub shares with more than 180 partner

03:55.880 --> 04:00.720
 companies. One of those partner companies is AT&T, which shares more with more than

04:00.720 --> 04:05.920
 a thousand third-party providers. And so it can be even hard to kind of, I think,

04:05.920 --> 04:09.600
 think about the scope of this, of kind of how many intermediaries this data gets

04:09.600 --> 04:14.160
 passed through and it's not something that's kind of really regulated or

04:14.160 --> 04:19.560
 tracked in a systematic way to even understand the scope. But again, this is

04:19.560 --> 04:24.000
 kind of very, very personal data being sold. And something the New York Times

04:24.000 --> 04:28.360
 article talks about is even, you know, even when data is aggregated it can be,

04:28.360 --> 04:33.040
 many people talked about, it can, you can de-anonymize data and kind of

04:33.040 --> 04:44.080
 disaggregate and identify individuals in many cases. So, oh, yes? Can you pass a...

04:44.080 --> 05:01.360
 Wait, so I don't know if this is the same study that Ben talks about. I think it was Grindr that had to be bought out by the U.S. State Department because it was, I can't remember if it was Grindr or Scruff, it was one of those dating apps.

05:01.360 --> 05:24.320
 But there were U.S. military personnel who were on the app, and then the app was bought out by some foreign coding command with the China, and then the U.S. State Department had to step in because it was putting their own military personnel in danger. It was one of the wilder stories.

05:24.320 --> 05:32.320
 Wow, okay, I missed that. That is, yeah, that is significant, yes, yeah. And that's another issue of kind of how...

05:32.320 --> 05:54.320
 I think Grindr got in a lot of trouble too because a lot of the ways it was sharing data even on itself did not have great security protocols, and they asked a lot of questions about HIV status, and it was problematic in some countries.

05:54.320 --> 06:08.320
 Yeah.

06:08.320 --> 06:17.320
 Wow. Thank you for those additional details. This is, yeah.

06:17.320 --> 06:21.320
 Very, very significant stuff.

06:21.320 --> 06:39.320
 And another topic, so that's kind of some of the phone, data from apps on our phones, is kind of police use of facial recognition. This is an article about New York Police Department, which has been putting children as young as 11 into a facial recognition database.

06:39.320 --> 06:59.320
 And it's important to remember that the technology wasn't even developed on children typically, so you've got higher error rates, even if you were okay with this sort of use, but also this is kind of then taking someone at a very young age and kind of adding them to this database.

06:59.320 --> 07:18.320
 The Georgetown Law Center for Privacy and Technology does a lot of great work. They did a study, Garbage In, Garbage Out, looking at how kind of facial recognition was used by police in practice, and they found a lot of kind of horrifying examples.

07:18.320 --> 07:35.320
 One is of police had a suspect, it returned no matches, and they said, well, this guy kind of looks like Woody Harrelson. So the actors, they Googled his picture and then entered that into the kind of mugshot database and then use that to surface suspects.

07:35.320 --> 07:59.320
 So that was covered, but it was like a series of police kind of photoshopping faces, like there was one where like the eyes were closed, so they photoshopped open eyes onto the face, also using kind of, you know, sketches of the, based on a description of the suspect and trying to use facial recognition technology on those.

07:59.320 --> 08:14.320
 And in many cases, you know, this is kind of surfacing a list of like the closest matches, but that doesn't necessarily mean they're good matches at all. So it's very kind of even more concerning to hear how this is being used in practice.

08:14.320 --> 08:33.320
 And then kind of another example. So this was early on in the Hong Kong protests last summer. A lot of people were circulating kind of pictures of, so typically people in Hong Kong are using kind of a cashless system with cards that automatically scan.

08:33.320 --> 08:53.320
 They didn't want to be tracked that they were attending the protest and so kind of having these long lines of people paying for paper tickets. But this is, I think, a common point where some of these technologies can be very convenient when things are going well, but it's also kind of important to think about other scenarios.

08:53.320 --> 09:05.320
 So next I wanted to talk about the talk by Alvaro Bedoya, and Alvaro Bedoya is director of the Georgetown Law Center for Privacy and Technology.

09:05.320 --> 09:11.320
 I thought this was a kind of interesting some history on kind of the use of surveillance.

09:11.320 --> 09:25.320
 So he goes back to Queen Elizabeth I building a network of spies and informants to root out perceived threats and particularly targeting Catholics and Puritans, so kind of religious minorities.

09:25.320 --> 09:50.320
 In the 1880s in the US, 2,300 Mormons were prosecuted for polygamy and this was facilitated kind of through surveillance and I thought it was interesting to note that kind of the anti-Mormons were saying that Mormons were not white and were actually Muslims as a way of kind of justifying the surveillance and prosecution.

09:50.320 --> 10:05.320
 The FBI amassed a 1500 page file on Cesar Chavez, who led a lot of the farm worker protests that kind of resulted in kind of more more rights and protections and

10:05.320 --> 10:29.320
 Professor Bedoya writes that Cesar Chavez was a saint and they actually didn't dig anything up on him, but that very, very few of us would be able to have a FBI file of 1500 pages about our kind of every doing and what all our kind of friends and neighbors and people we interact with do and not have something very embarrassing or

10:29.320 --> 10:39.320
 even, you know, like illegal show up in it and that that's kind of a very unreasonable bar to gather this much information on somebody.

10:39.320 --> 11:05.320
 The FBI also tracked Martin Luther King Jr., compiled recordings of his affairs, they threatened to blackmail him and encouraged him to kill himself with this, so there's kind of a, and there are even more examples in the essay, but kind of a long history of surveillance being used in particular to target certain people.

11:05.320 --> 11:23.320
 And as Professor Bedoya wrote, the pattern is we watch those who are quote, considered less than, will you spy on your superior or will you spy on the poor man, the person of color, the immigrant, the heretic, we watch those who are other.

11:23.320 --> 11:42.320
 And he identifies this pattern when those others organize, mobilize that watching is redoubled, surveillance becomes a tool to stop marginalized people from achieving power.

11:42.320 --> 11:54.320
 The key reaction I had to this was that notion of, especially the second pattern of thinking about, or really both patterns, was how immediate the standard is clear when you apply it the other direction.

11:54.320 --> 12:05.320
 If you even think of like fiction and anybody who is casing a bank or stalking a president, the threat is immediate and obvious, but once you turn it the other way, it's more intrigue.

12:05.320 --> 12:19.320
 And I think we have that natural narrative reaction that reinforces the observation here, is that we know it when we see it, but we don't recognize it as such, but we don't feel threatened by the action that we observe.

12:19.320 --> 12:26.320
 Your perspective matters a lot in this, yeah, kind of what you see as a threat.

12:26.320 --> 12:42.320
 Yeah, I think what I'm thinking about here is that like basically this is happening now, in particular, like I believe I recently read an article where ICE was using location-based data to identify immigrants who they wanted to target for arrest.

12:42.320 --> 12:45.320
 Yeah, well, yeah, that'll come up later.

12:45.320 --> 12:48.320
 But yeah, that's a great, great point.

12:48.320 --> 12:58.320
 And Colin, and then I'll bring a point. There was this book about the Great Far Wall of China, because I don't remember the exact, I think that was the name, I don't remember the subtitle or the author.

12:58.320 --> 13:07.320
 But they kind of bring up because censorship and surveillance are really kind of two sides of the same coin a lot of the time, because when you're trying to like look into it, you have to see what speech is there so you know what to censor.

13:07.320 --> 13:14.320
 And they kind of made a point that the number one thing they kind of censor for is actually just like solidarity or people kind of working together.

13:14.320 --> 13:24.320
 And there was this really interesting example where there was this comedy website, and they were very careful, there were no kind of like offensive or seditious jokes, but people from the website were meeting each other in real life.

13:24.320 --> 13:29.320
 And then that started getting censored because they just had these signals, they could kind of like put their hands up to each other.

13:29.320 --> 13:33.320
 And so it's like, oh, you're from this site, and we can talk offline.

13:33.320 --> 13:47.320
 And so a lot of times I think that it's not just about their surveillance, but it's also there's kind of censorship and pressure to move you from unsurveilled channels of communication to only be able to use surveilled channels of communication.

13:47.320 --> 13:51.320
 So that's kind of just like the other side of this pattern to watch for pushing the call.

13:51.320 --> 13:55.320
 Yeah, that's great. Thank you.

13:55.320 --> 14:01.320
 I'm going to move on, but I'll take more questions later. And then another quote from this.

14:01.320 --> 14:09.320
 I contend that we are a nation of dissenters, and this was the first Hispanic senator in the US denouncing McCarthyism in 1950.

14:09.320 --> 14:19.320
 And this was a piece of history I did not know about before reading this, but apparently after Dennis Chavez spoke up about this, that other people then started feeling like they could speak up as well.

14:19.320 --> 14:32.320
 And he said our nation was created by dissenters, our ancestors were not satisfied, kind of emphasizing kind of the value and importance of dissent.

14:32.320 --> 14:50.320
 And then an example kind of more recently is in 2015 in Baltimore during the protest over Freddie Gray's death, a black man killed wrongly in police custody. Police used facial recognition to identify protesters.

14:50.320 --> 15:08.320
 And this was, they said, to identify protesters who had existing warrants. However, you can have warrants for relatively minor things. And this kind of came to light through some of the company that built this technology, Geofeedia, used this as a positive case study.

15:08.320 --> 15:26.320
 And so, and they quote someone saying, every person we got off the streets before they hurt someone was a win, which I just thought was such a weird quote because it's before they hurt someone, they didn't hurt someone, you know, what did they do?

15:26.320 --> 15:33.320
 But I think this is kind of very, very concerning to surveil protesters.

15:33.320 --> 15:42.320
 Then I wanted to include kind of a few, a few things to know about how the government uses technology in practice in many cases.

15:42.320 --> 16:11.320
 It's often opaque. And then this is a kind of very U.S. specific. In the U.S., they're restrictive NDAs. It's typically not evidence based. And then there's often a lot of dysfunction kind of through the whole pipeline of how contractors bid on proposals, who get selected, the asymmetry in terms of the technical knowledge that the companies have versus the people working in government trying to evaluate these proposals.

16:11.320 --> 16:22.320
 And I'll say a little bit about that, but it is not a kind of efficient or well-designed process in many ways.

16:22.320 --> 16:45.320
 So government use of tech is often opaque. An article, this was a year or two ago, came out that Palantir had a project in New Orleans to test predictive policing. It had been going on for seven years and city council members didn't know about it. When they were asked for comment, they were like, we didn't even know that we had this program.

16:45.320 --> 16:53.320
 And it ended up was canceled kind of a few weeks after The Verge broke the story about it.

16:53.320 --> 17:08.320
 New York City started an automated decision system task force to evaluate how the city was using automated decisions, automated decision systems, which I was super excited when they announced that they were doing this.

17:08.320 --> 17:20.320
 They got a lot of fantastic people on the committee and then they did not give them any information about what automated decision systems New York uses because they claimed it was proprietary and that they couldn't tell them.

17:20.320 --> 17:30.320
 So Meredith Whitaker, one of the founders of the AI Now Institute, was on the committee and at the end she said it was a waste and kind of sets a terrible precedent.

17:30.320 --> 17:38.320
 A large portion of the committee did release a kind of shadow report dissenting with the official report.

17:38.320 --> 17:52.320
 But so here, you know, is even a system where you had something that was voted on, we should do this, and then not giving the people involved information they needed to even evaluate.

17:52.320 --> 18:08.320
 There's also this idea of something called parallel construction, and that's when police will use a technology to kind of gain some knowledge, not want to reveal that they use that technology and have to come up with kind of an alternative case.

18:08.320 --> 18:28.320
 And so an example is stingrays or cell site simulators, which can kind of identify people's cell phones, and the company that produces these has incredibly restrictive NDAs that actually encouraged police that it was better to drop a case than to reveal that they were using this technology.

18:28.320 --> 18:40.320
 And Elizabeth Jo, who's fantastic, she's a law scholar at UC Davis, she spoke about this at the tech policy workshop here at USF at Cade.

18:40.320 --> 19:00.320
 And she highlighted that kind of the law develops through cases, like that's kind of how, you know, the edges of what things mean get refined, and when even the existence of a technology is hidden, there's no way for the law even to kind of like adjudicate or for us to kind of develop what should our approach on this technology be.

19:00.320 --> 19:06.320
 And so that's kind of a pretty extreme case, but it often operates that way.

19:06.320 --> 19:19.320
 So another point about government use of technology is that it is often not evidence based, which can be really startling given the amount of money that's often involved.

19:19.320 --> 19:27.320
 So, face recognition is being used in many schools that many people have been warning for a while that it won't stop mass shootings.

19:27.320 --> 19:37.320
 One key reason is that many shootings are committed by current students, they are not people that would be on a watch list, many of them don't have any sort of history.

19:37.320 --> 19:44.320
 So even just like the premise is kind of off of, even if you can recognize people you don't know who who the shooters are.

19:44.320 --> 20:03.320
 And there was an article more recently that even some of the companies producing this technology are starting to acknowledge that it won't stop shootings that are kind of pivoting to other reasons they still still want to sell.

20:03.320 --> 20:15.320
 I guess I was reading a couple of research papers that Facebook published maybe about two years ago on their technology, how they were having, they were wondering about like open sourcing it.

20:15.320 --> 20:27.320
 And particularly this model that helps them to detect depression, anxiety, and you know the behavioral expressions of people that are borderline suicidal and may likely have an attempt.

20:27.320 --> 20:38.320
 But the problem with this was that like that could be used for nefarious purposes and like it also like stated, like it'll calcify somebody's mental health record through like the years to come.

20:38.320 --> 20:55.320
 And so I think the follow up question that I had been reading that paper and tying it into this is like how do you identify somebody that has a severe track record of mental health or psychological problems and you can use facial data, like you tie it with a track record and what if they don't

20:55.320 --> 21:08.320
 I do want to highlight that mass shooters are not correlated with mental health issues, they are correlated with domestic violence though and kind of being domestic abusers.

21:08.320 --> 21:25.320
 I think it's, it's also kind of on this question of identifying mental health issues. It's very important though to recognize like what are the resources we can offer people I think in many cases, people with mental health issues who are not getting treatment

21:25.320 --> 21:44.320
 because they can't afford it because there are not enough available affordable therapist and it's kind of often I think more structural and systemic issues as opposed to I don't think that identification is like the kind of the primary

21:44.320 --> 21:55.320
 pain point there that needs to needs to be solved, but that is it. Yeah, good, good question. Yeah, and that work does raise a lot of ethical issues to discuss.

21:55.320 --> 22:05.320
 Actually, let's do John over on that side and then Aaron and then I will continue.

22:05.320 --> 22:32.320
 In the military case, we talked about military industrial complex, is it possible to talk about the tech industrial complex in this case, meaning everything in the tech industry comes up with, while the government use it, even if it's not always efficient

22:32.320 --> 22:43.320
 to stop machining, no one knows.

22:43.320 --> 23:07.320
 Yeah. Are you getting at how a lot of this technology is produced by private companies and marketed or sold to government agencies or to the military. Yeah, and I think it results in kind of a very opaque system and often a lack of accountability and I think, I think many people feel more comfortable

23:07.320 --> 23:32.320
 with, you know, I've seen people that are, you know, very concerned about surveillance in China because they see it as, you know, there it's the government that's kind of developing and deploying the technologies whereas in the US it's private corporations, but which are kind of have all sorts of financial partnerships and deals with the government and so that does give it a kind of different

23:32.320 --> 23:42.320
 character in some ways, but still raises a lot of issues and even raises, I think, kind of different issues often around the lack of transparency and lack of accountability.

23:42.320 --> 24:09.320
 And then one more example I wanted to give was San Diego was using, law enforcement in San Diego compiled over 65,000 face scans during a seven year long project that was just finally ended of facial recognition. It's almost completely unclear how effective the initiative was with one spokesperson saying they're unaware of a single arrest or prosecution that stemmed from the program.

24:09.320 --> 24:24.320
 And so that's just kind of a remarkable amount of data to collect with, without even having kind of any evidence that it was accomplishing its, its purported goals.

24:24.320 --> 24:30.320
 Another case study comes from Amazon's Ring doorbell, his background Amazon.

24:30.320 --> 24:38.320
 The Ring has partnerships with over 800 police departments in many cities. This is subsidized with taxpayer money.

24:38.320 --> 24:48.320
 Amazon can stream 911 calls in real time, and Amazon requires police departments to read pre approved scripts when they talk about the program.

24:48.320 --> 25:00.320
 So I think this is kind of a remarkable amount of control for a private corporation to be having on something that is, you know, ostensibly kind of like a public service.

25:00.320 --> 25:20.320
 However, NBC News did an investigation which came out just last week. They talked to officials at over 40 different police departments in the US, which all of which have been using Ring for at least six months, and found kind of no evidence that Ring was effective at fighting crimes.

25:20.320 --> 25:36.320
 And this was a mix of many, many cities said that they don't actually track anything related to telling if it was effective. There are a lot of examples, though, of people saying, well, like, we've probably caught someone but I can't think of anything.

25:36.320 --> 26:00.320
 One interesting quote was from, I think, from an official in Houston saying that evidence isn't their limiting factor. He said we already have kind of more evidence than we're able to investigate or act on and so kind of gathering more potential evidence doesn't doesn't necessarily help us in any way.

26:00.320 --> 26:09.320
 I mean, they also, many police complain they're just getting sent like videos of raccoons in people's yards and kind of irrelevant.

26:09.320 --> 26:18.320
 But this is, I think, for kind of such a massive program, the fact that there's really kind of no evidence around it.

26:18.320 --> 26:34.320
 Amazon does cite statistics, that they came up with themselves on how they reduce crime, but they would not share any details or data on how they calculated that.

26:34.320 --> 26:39.320
 Yes?

26:39.320 --> 26:56.320
 How do these programs get started? I've dealt with government RFPs before. It's like a whole thing. How are there so many of these pilot programs? And I guess on the flip side, are they not ending because people have just forgotten about them?

26:56.320 --> 27:12.320
 I don't know. I mean, I think many of the decisions, I mean, I'm sure there's some of that, but I think in many cases, the decisions are not necessarily evidence based or even particularly rational. I think technology can seem promising to people.

27:12.320 --> 27:26.320
 Ring is something where I, and this issue in general is something where I feel like I know a lot of people that I really disagree with. There are people I know and agree with on other issues and we talk about Ring and they're like, that sounds great. I want one.

27:26.320 --> 27:34.320
 So I think some of it's also kind of the appeal. I mean, in general, I do think people largely operate on stories.

27:34.320 --> 27:50.320
 You know, we kind of all find stories very compelling and so you can certainly kind of bring up a few individual stories of, you know, here's someone that was captured. You don't have, you know, the comparison of would this person been arrested otherwise?

27:50.320 --> 28:02.320
 How did this affect, you know, overall arrest rates? But I do think we kind of operate on stories and kind of what sounds interesting or exciting.

28:02.320 --> 28:13.320
 Yeah, that's a good question. And then actually, we're past time for a break. So let's take a break now and meet back here in seven minutes.

28:13.320 --> 28:36.320
 Okay, let's get started again. And then one other point that came up after the break started is that the business model for many of these companies is cloud storage. And so the companies are often thinking about kind of long term profits and are willing to sell at a discount or give a free year initially, which can kind of get departments hooked.

28:36.320 --> 28:47.320
 So kind of in conclusion, just about understanding kind of how government uses technology. It's opaque. They're often very restrictive NDAs.

28:47.320 --> 29:00.320
 It's typically not evidence based. And then I mentioned kind of some of these issues of often people in government may not have the technical know how to evaluate the claims that they're hearing from tech companies.

29:00.320 --> 29:07.320
 Tech companies are often out of touch with kind of what the actual needs and processes of how government operates.

29:07.320 --> 29:21.320
 There's also an issue I think most kind of governments still require a waterfall approach to software development because they're really kind of thinking about these, you know, projects as a whole, whereas the tech industry is much more in an agile approach.

29:21.320 --> 29:36.320
 So there's really just a lot of disconnect. I've heard people from the US Digital Service talk about this, that, yeah, there's a lot that could use improvement, significant, significant structural improvement.

29:36.320 --> 29:59.320
 And then the, you know, the process sometimes of bidding on contracts is kind of so complex and specific that like the major tech companies have giant departments that are just focused on the kind of how you bid, but not necessarily on some of the more substantive parts of the technology.

29:59.320 --> 30:17.320
 So then I did want to acknowledge, and someone pointed this out about my syllabus, several people pointed this out about my syllabus even kind of prior to the class starting when I was showing it to friends, that I realized my viewpoint is very clear here and that I'm not necessarily representing the other side.

30:17.320 --> 30:38.320
 I did ask on Twitter looking for kind of what were the most thoughtful, well-reasoned arguments kind of in favor, and I got a lot of discussion on that one. And something that I thought was interesting is someone kind of made the meta point, you know, the responses to this threat are interesting because for many of the examples,

30:38.320 --> 30:48.320
 I've seen those used as reasons why facial recognition is bad, that you kind of had people using the same examples and arguing that this is a reason for it and this is a reason against it.

30:48.320 --> 30:55.320
 So that was interesting. And I will try to highlight what I think some of the differences in underlying values are.

30:55.320 --> 31:15.320
 But I think my hope with talking about kind of the rest of this lesson is that even for kind of proponents of facial recognition, I hope that you are aware of what the risks are and are kind of weighing those.

31:15.320 --> 31:27.320
 But just the main arguments I've heard for these technologies are public safety, the idea of catching criminals, and convenience and efficiency.

31:27.320 --> 31:36.320
 So now I want to talk about what are the risks, why does this worry me, and many others.

31:36.320 --> 31:49.320
 So I'm going to kind of talk about some worst case scenarios. So in Western China, the Uyghurs, which are a Muslim minority, over 2 million or up to 2 million people have been placed in internment camps.

31:49.320 --> 32:02.320
 And this has been facilitated through a lot of biometric data including scans of people's faces, audio of their voices, blood samples, huge, huge amounts of data have been gathered.

32:02.320 --> 32:12.320
 This is an article from Wired that I think one of the best articles I've read on the issue because they interviewed a number of Uyghur refugees in Turkey.

32:12.320 --> 32:22.320
 So there are a number, so disproportionately it's men being put into the internment camps and so there are a number of wives that have escaped.

32:22.320 --> 32:38.320
 But they kind of talked about, you know, having to be required to go to the police station and like make faces to show different emotions. One woman had a deeper voice and they said they didn't believe her initially when they were taking the audio samples and kind of kept making her repeat them.

32:38.320 --> 32:43.320
 But they've kind of very invasively gathered quite a lot of data.

32:43.320 --> 32:49.320
 And this is terrible.

32:49.320 --> 32:58.320
 There's an article on the engine room called Dangerous Data, the Role of Data Collection in Genocide that covers several, several examples.

32:58.320 --> 33:06.320
 And so one is the work of IBM with the Nazis during kind of World War II and during the Holocaust.

33:06.320 --> 33:20.320
 So IBM's computers were used to track people, kind of whether they were Jewish, if they were Gypsies, how they were executed.

33:20.320 --> 33:43.320
 And this is something to keep in mind is that it was less where, you know, now where someone can just buy a computer and then go use it for several years on their own. These machines required a lot of maintenance and so people were having to go service them regularly and kind of a much closer relationship to maintain the computers than you would need kind of with a modern computer.

33:43.320 --> 33:54.320
 As Swiss judge ruled, it does not thus seem unreasonable to deduce that IBM's technical assistance facilitated the task of the Nazis and the commission of their crimes against humanity.

33:54.320 --> 34:02.320
 Acts also involving accountancy and classification by IBM machines and utilized in the concentration camps themselves.

34:02.320 --> 34:13.320
 And this picture is of Adolf Hitler meeting with Tom Watson Sr., the CEO of IBM. This was in 1937.

34:13.320 --> 34:25.320
 IBM did later break ties with the Nazis, but it was most people say kind of far too late and many, many companies kind of broke ties far, far sooner than IBM.

34:25.320 --> 34:37.320
 So this is kind of just thinking about kind of the worst case scenarios of what data collection can facilitate, particularly kind of gathering sensitive, sensitive data.

34:37.320 --> 34:59.320
 And there is an example from France where someone broke the punch card machine so that the column recording if someone was Jewish no longer functioned and fewer people were killed in that region. So that's kind of like an example of how, or evidence that this was playing a role.

34:59.320 --> 35:18.320
 IBM also helped. I mean this was in early 30s. Germany did a much, much more detailed census than it had done previously and was able to identify kind of far more Jewish people than they had previously recognized as living in Germany.

35:18.320 --> 35:38.320
 So those are kind of some worst case scenarios. So concerns about the risk with surveillance are it kind of disproportionately harms already marginalized groups, data collected for one reason will be used for other reasons later.

35:38.320 --> 35:48.320
 There are errors in data. Internal threats are often the biggest threats and it stops dissent which is crucial for social progress.

35:48.320 --> 36:02.320
 So kind of already shared this pattern earlier from the Alvaro Bedoya reading about how we watch those who are considered quote less than.

36:02.320 --> 36:23.320
 A really concerning article was about, so India has implemented a much more comprehensive biometric system and a number of HIV patients have stopped their treatment of antiretrovirals because they are scared that they will be outed as HIV positive.

36:23.320 --> 36:43.320
 Some of them are gay or sex workers and are worried about being outed for those characteristics. And so yeah, this article kind of had interviews with a number of people that were successfully receiving treatment and then stopped it with the implementation of having to give more biometric data.

36:43.320 --> 37:00.320
 And this I think kind of came up in the earlier, not this example in particular, but kind of this issue of how already marginalized groups may stay away from a place when they feel like they're being tracked.

37:00.320 --> 37:14.320
 And then police are illegally giving vehicle location data to ICE, documents suggest, and using kind of massive license plate scanning schemes to share data.

37:14.320 --> 37:20.320
 So these are kind of already marginalized groups being further harmed.

37:20.320 --> 37:32.320
 It's an article, trading privacy for survival is another tax on the poor that covers, well as I say, they were talking about in the US, but this is a global pattern I think.

37:32.320 --> 37:45.320
 Public benefits programs ask applicants extremely detailed and personal questions and sometimes mandate home visits, drug tests, fingerprinting, and collection of biometric info.

37:45.320 --> 37:58.320
 Prisons across the US are quietly building databases of incarcerated people's voice prints in many states for prisoners to be allowed to make phone calls. They have to consent to giving their voice data.

37:58.320 --> 38:12.320
 And voice is another form of biometric data, which I use the word consent, that is not meaningful consent when someone is in prison and this is their only option to make a phone call.

38:12.320 --> 38:37.320
 And then in refugee camps, eye scans are being used, iris scans, it's often being linked to people's kind of food benefits. And so this is something, and I chose this from a paper, this has been covered by other, I found news articles, but they often framed it as, oh, like isn't this so convenient, iris scans are being used to help get these refugees their food.

38:37.320 --> 38:46.320
 But it's important to notice, again, I don't think there's a meaningful notion of consent for if this is kind of a refugees only option to receive food.

38:46.320 --> 39:02.320
 This is also another situation where it's a private company that has quote, volunteered, that they are offering the service for free, but their kind of payoff is that they're getting to collect all these iris scans.

39:02.320 --> 39:13.320
 And so I think these are really disturbing examples.

39:13.320 --> 39:24.320
 And then this I learned about from a Frank Pascual article. I definitely recommend following Frank Pascual. He does a lot of great work. He's the author of the Black Box Society.

39:24.320 --> 39:37.320
 But he highlighted that at least one Indian fintech app uses social media to predict people's credit rating, and it reduces the score of people who are engaged in political activity.

39:37.320 --> 39:45.320
 That's kind of being a bigger credit risk. So that's very concerning.

39:45.320 --> 40:02.320
 And then here are some stats that were highlighted by Tawana Petty of the Detroit Community Tech Project. And so she's been organizing in Detroit against the use of facial recognition there, which is very pervasive, and I'll say a little bit more about that in a moment.

40:02.320 --> 40:17.320
 But she pointed out the demographics of facial recognition bans. So the first table is cities that have banned facial recognition, and I have just included white and black for kind of simplicity.

40:17.320 --> 40:44.320
 And then in Detroit at the bottom, there is no ban despite kind of very extensive organizing. Detroit is 79% black, which is much higher than these other cities. And so this is kind of an example showing even how much harder it can be to to regulate the use of these technologies for black people, specifically more broadly for groups that are already marginalized.

40:44.320 --> 41:01.320
 So this is a chart about Detroit's, it's called Project Green Light. And so it puts these cameras with green lights outside of businesses. And it is, check, it's already at 256 locations.

41:01.320 --> 41:11.320
 Oh no, it's expected to expand to over 480 locations by the end of 2019. This graphic is kind of I think from last summer.

41:11.320 --> 41:33.320
 And I've highlighted an article by Chris Gilliard, who talked about the difference between quote luxury surveillance and forced surveillance. And so, you know, an example of luxury surveillance, we're talking about Strava before kind of an Apple Watch or something that kind of people are using as a luxury item.

41:33.320 --> 41:45.320
 They, you know, like the data that they're getting versus kind of forced surveillance here. A lot of people complain they have these bright green lights. In some cases, this is, you know, can be shining, you know, across your street into your home.

41:45.320 --> 41:54.320
 It's really disruptive and kind of ugly. And he kind of talks about some of the contrast between forced surveillance and luxury surveillance.

41:54.320 --> 42:05.320
 Something else about Project Green Light is when it was first rolled out, they tested it on and they had like the small group of maybe 10 businesses initially.

42:05.320 --> 42:16.320
 And what they did is they prioritized police calls from those businesses. And so then they said, wow, like this really reduced crime having the surveillance cameras.

42:16.320 --> 42:30.320
 But actually it was they had prioritized this very small number of businesses. So I mean, of course, that had a kind of positive impact. So it was a very misleading, misleading statistic that's been used to to justify kind of the further rollout.

42:30.320 --> 42:43.320
 Another reason that massive data collection and surveillance and biometric data collection worry me is that data collected for one purpose will be used for others.

42:43.320 --> 42:54.320
 So it was only confirmed in 2007 that the U.S. Census Bureau gave up the names of Japanese Americans in World War Two so that they could be placed in internment camps.

42:54.320 --> 43:01.320
 And this is something that at the time that the census was taken in 1940, it was illegal for the data to be shared this way.

43:01.320 --> 43:09.320
 And there were guarantees of it will not be shared this way. It's illegal. And then they changed the law and then also hid that they did it.

43:09.320 --> 43:14.320
 Although many people suspected were like, this must be what happened.

43:14.320 --> 43:27.320
 And so that's something that's difficult to kind of once data is collected, you don't necessarily know how the laws are going to change in the future, how it will be used differently.

43:27.320 --> 43:40.320
 Tim Wu wrote a good article about this opinion piece in The New York Times, and he says, one hard truth is that data and surveillance networks created for one purpose can and will be used for others.

43:40.320 --> 43:49.320
 You must assume that any personal data that Facebook or Android keeps are data that governments around the world will try to get or that thieves will try to steal.

43:49.320 --> 44:06.320
 So also this idea of even if you did have a lot of trust in the company or group gathering the data that others will try to get that data and that they will use it for for other purposes.

44:06.320 --> 44:27.320
 I am curious about the census question in particular, because just from the reading that I've done, or like the circles that I wander in, I hear that census data is really important for folks to get the right benefits for federal funding to be allocated in ways that make sense for especially marginalized.

44:27.320 --> 44:31.320
 Yes. So I'm wondering how you think about that balance.

44:31.320 --> 44:43.320
 Yeah, this is something I don't feel like I have a satisfying answer to. There is a tension that yes, that data is really important for distributing resources and for people being counted.

44:43.320 --> 44:58.320
 Another example. So I should also say there were identification cards that were used in the Rwandan genocide. And so it is considered kind of a best practice to not record people's ethnic data or something that could be sensitive.

44:58.320 --> 45:10.320
 But in the case of the Rohingya refugees in Bangladesh, they are actually demanding cards that list that the Rohingya because Myanmar has really tried to erase them as a people.

45:10.320 --> 45:20.320
 And so that kind of there's another study on how kind of these a number of countries have biometric kind of identification projects in the works.

45:20.320 --> 45:34.320
 But that was something that came up through kind of their study and interview with a lot of people that and I don't have a clear answer on this that, you know, in that case, you know, this is people saying like, you know, like our ethnicity has kind of been denied and erased like we want it recorded.

45:34.320 --> 45:40.320
 And so I don't have a good answer on this.

45:40.320 --> 45:46.320
 There are other thoughts or does anyone have a way to balance that tension?

45:46.320 --> 46:02.320
 All right. I do think it's good to at least kind of be aware that it exists and kind of like that these risks exist. Although you're right, there can be kind of like when resources are being allocated based on that data.

46:02.320 --> 46:07.320
 Yeah, it's important.

46:07.320 --> 46:21.320
 This is a big thing kind of like public health data because you collect a lot of data to combat specific things and like I think one thing some people suggested and I kind of agree with is that data should also be able to expire.

46:21.320 --> 46:47.320
 As I mentioned earlier, and so it does make sense for somebody to have the agency to decide what data is theirs and also what data they're called by, right? But then at what point, like with the Japanese internment camps, if that census data has been allowed to expire in some way, possibly, you know, or not being put on to, it might have prevented something.

46:47.320 --> 46:58.320
 Yeah, no, I like that idea in general of data expiring because I only see this also with kind of more prosaic uses by tech companies of kind of once they have your data, they have it forever.

46:58.320 --> 47:00.320
 Thank you.

47:00.320 --> 47:04.320
 Hey, sorry, I just wanted to talk about the census.

47:04.320 --> 47:15.320
 Yeah, this is something that the census blog writes a lot about the US census. Yeah. And I think it is pretty interesting.

47:15.320 --> 47:32.320
 Just, I don't know what, if they've written anything recently, but generally, they have thought a lot longer about the problem of collecting individual data and then aggregating it, anonymizing it, de-identifying it than a lot of tech companies have.

47:32.320 --> 47:50.320
 Yes. So some of the things that they talk about are pretty interesting and you don't really see that in de-anonymization literature that comes out of tech and some of it is non-automatable or like it makes the data pretty useless in some ways.

47:50.320 --> 48:04.320
 But I do think there are lots of interesting aspects of it, like for census, if you're just using it for allocation, you don't really need where the people came from, so you could mix up the data intentionally.

48:04.320 --> 48:17.320
 So it's another form of deleting or expiring the data by just mixing up certain aspects so that you can never get to the truth of certain aspects of the collection.

48:17.320 --> 48:39.320
 Great. Thank you. Yeah. And I would also highlight that I think the census is, you know, it's a very, in some ways, kind of unique example in that, you know, like, I think for like your average tech company, they probably shouldn't, that they should not be collecting as much data as the census and they have very different, very different needs.

48:39.320 --> 48:51.320
 And so that in many use cases, I think the question of kind of collecting less data is an important one to be asking, although I do think you can make a stronger case for the census on some of the data.

48:51.320 --> 48:56.320
 But thank you for the recommendation to the census blog.

48:56.320 --> 49:03.320
 Well, then another example of data collected for one purpose being used for others.

49:03.320 --> 49:18.320
 So there are over 10 states that allow undocumented immigrants to obtain driver's licenses. And so this is something where states were, you know, encouraging and saying, you know, it's safe to get a driver's license and we would prefer that you have a driver's license than not.

49:18.320 --> 49:25.320
 And then ICE agents have been accessing those databases.

49:25.320 --> 49:36.320
 There is also kind of an article about states that are, who are not sharing their records with ICE.

49:36.320 --> 49:55.320
 DHS is trying to get other states to get those records for them through data sharing agreements. So typically, you know, many states will kind of share data with each other and so as a way to kind of get around states that are trying to protect their their data from ICE.

49:55.320 --> 50:10.320
 And this also kind of gets into kind of how data can be passed around that because these other states were, you know, we're not going to say like, hey, I'm trying to get this data for ICE. So it's just like, can we share our driver's license data?

50:10.320 --> 50:15.320
 Alright, so that's kind of how how data can end up being used for different purposes.

50:15.320 --> 50:21.320
 Another area of concern is that data contains errors.

50:21.320 --> 50:44.320
 And so there is a database that primarily LAPD, but some other Southern California law enforcement agencies contribute to and an audit found that 42 babies under the age of one were added as gang members and 28 of those were recorded as having admitted to being gang members.

50:44.320 --> 51:00.320
 And then kind of even worse, this database, there's really no process in place to correct errors or to update people's records. And so I think the, going back to this idea of expiration dates could also be significant here.

51:00.320 --> 51:12.320
 It said even for people that have had kind of no contact with police for five years, I think maybe they're supposed to be purged from the database, but instead they were set to not expire for 100 years.

51:12.320 --> 51:37.320
 And so it's kind of once you're in this database, you're in it, which is really concerning. Another article that I just read this weekend said that LAPD are evaluated, and we're going to talk more about metrics in Lesson 5 and how metrics can encourage gaming, but LAPD are evaluated on 16 metrics daily to measure their productivity.

51:37.320 --> 52:02.320
 And one of those metrics is how many gang members they've interviewed. And so you can probably guess what this is encouraged, but many, many people, over 20 officers are under investigation for kind of forging these documents about people saying that people had evidence they were gang members or admitted to being gang members when they didn't.

52:02.320 --> 52:10.320
 So this is kind of a risk of this approach.

52:10.320 --> 52:35.320
 A study, the FTC did a study of credit reports in 2012 and found that 26% had at least one mistake in their files and 5% had errors that could be devastating. And so this article, how the careless errors of credit reporting agencies are ruining people's lives, was written by a reporter who was going to rent an apartment.

52:35.320 --> 52:46.320
 And the landlord contacted him afterwards and was like, you failed the background check for having felony firearms convictions.

52:46.320 --> 53:09.320
 But I'm like surprised because you seemed like this mild mannered. He's a reporter for public radio. Most landlords would not have called and followed up. And so he looked into it and he found the error, but the three credit bureaus are TransUnion and Equifax, Experian.

53:09.320 --> 53:26.320
 I don't remember which one it was, but one of them. So he called and was like, I've identified this mistake. He talked to someone at the Tennessee courthouse about it. And then he said he had to make over a dozen calls and they're just like, oh, we're investigating your case, but they wouldn't update it until he said, I'm a reporter and I'm writing about this.

53:26.320 --> 53:37.320
 But that's something that most of us would not be able to do. Many of people, he's also white, many people would not have gotten the benefit of the doubt from the landlord and letting them know what had happened with the background check.

53:37.320 --> 53:43.320
 And so this can, this can really impact people.

53:43.320 --> 53:50.320
 That kind of related point along those lines is that surveillance often operates without accountability.

53:50.320 --> 54:08.320
 And so even if there's, you know, this is supposed to be the process in place, that's often there's no, no accountability about it. And this is, I think, very much relates to power and kind of often it's if it's powerful people surveilling less powerful, that helps explain why there's not accountability.

54:08.320 --> 54:28.320
 So Amazon, so after the research by Joy Balamwini came out of, you know, Amazon having higher error rate, actually, sorry, Amazon contested that research, even though kind of all major researchers were like, no, this is accurate and reproducible.

54:28.320 --> 54:40.320
 But they also contested the ACLU study, which misidentified 28 members of Congress with criminal mug shots. I actually don't remember which of those this article is about.

54:40.320 --> 54:55.320
 But they, Amazon, part of their defense was like, oh, you're not using the software properly. But then it turns out that their only known police client wasn't using the software in the way that Amazon said was proper because that's not what the defaults were and they didn't provide any training.

54:55.320 --> 55:07.320
 And so this is kind of an example of how how things get used in practice and the way that Amazon was kind of trying to evade accountability.

55:07.320 --> 55:18.320
 And then we kind of talked about some of these, these article, articles or issues before with, you know, Palantir using, or New Orleans using Palantir's technology and people not even knowing about it.

55:18.320 --> 55:25.320
 So, of course, there is kind of no insider accountability to how it's being used.

55:25.320 --> 55:46.320
 Then Zeynep Tufekci has said that internal threats are often the biggest threats. And she said this in context of this article on how two operatives for Saudi Arabia were working at Twitter and looked up very sensitive information about Saudi Arabian dissidents.

55:46.320 --> 56:07.320
 And many people have said this probably led to the imprisonment and torture of people because they, and this kind of goes back to an earlier comment about, you know, who has access to data and tech companies of people were kind of really able to look up this data.

56:07.320 --> 56:34.320
 And this occurred in 2015, but a lot more details have just come out with the FBI has a case, a case around it, but kind of very, very sobering to read that one of these people looked up information on kind of like a thousand people within kind of activists that were using, using Twitter.

56:34.320 --> 56:43.320
 Another issue, so this was an Associated Press story about how police officers abuse confidential databases.

56:43.320 --> 56:50.320
 They found incidences of database misuse by police from all 50 states and 36 large police departments.

56:50.320 --> 57:05.320
 Over two years, there were over 650 cases where an employee or police officer was fired or suspended for database misuse. And they say this is a definite underestimate of the problem because this is just people that got caught and faced repercussions.

57:05.320 --> 57:21.320
 And two kind of big categories of who was targeted are kind of ex romantic partners who end up being stalked or abused, as well as people that have protested or spoken out against police.

57:21.320 --> 57:30.320
 And again, kind of police have access to all this sensitive data that they're looking up kind of for their own purposes.

57:30.320 --> 57:45.320
 And I think, I think this is one of the points where kind of how people weigh the, the internal risk really can change how people feel about these technologies.

57:45.320 --> 57:53.320
 I think that people that are kind of more focused on, you know, we need to catch criminals or I'm worried about these external risk.

57:53.320 --> 58:12.320
 Maybe more positive about surveillance technologies, whereas I think kind of putting weight on these internal threats and misuse of data can lead you to be much more cautious and skeptical of these risks or of surveillance technologies.

58:12.320 --> 58:18.320
 Any thoughts on these?

58:18.320 --> 58:31.320
 So these are just kind of several attributes that I think it's important to know about surveillance. So kind of how it's disproportionately impacting people that are already marginalized, the way that data can be used for other purposes later.

58:31.320 --> 58:52.320
 That it often even has contains errors in it, that there's often not accountability about how how it's being used, and that the threats can be internal. And so that, you know, that's something where even if a company or group has, you know, very strong protections against like exterior hackers, if someone internally is misusing it.

58:52.320 --> 58:57.320
 Well, that's, that's no good.

58:57.320 --> 59:03.320
 So then I'm going to respond to a few things I commonly hear and disagree with.

59:03.320 --> 59:13.320
 And so the first is, don't people know what they're getting into when they use these services? And I hear this a lot, even from people will be like, yeah, this is bad, but you know, people knew what they were doing.

59:13.320 --> 59:19.320
 And Lindsay, Lindsay Barrett wrote a great article, our collective privacy problem is not your fault.

59:19.320 --> 59:31.320
 But a few things to keep in mind. So we talked last week how it would take the average American 40 minutes a day to read every privacy policy they saw. We know that people don't have that much time.

59:31.320 --> 59:40.320
 Also, these privacy policies are typically written at a college reading level, whereas the average American is at an eighth grade reading level.

59:40.320 --> 59:52.320
 Companies rely on manipulative design tricks to get you to spend more time and more money and share more data. So the design is often really pushing you in a certain, certain direction.

59:52.320 --> 1:00:03.320
 Facebook keeps shadow profiles of people who don't use it. So even if you don't use Facebook and never have, they can still have a profile on you.

1:00:03.320 --> 1:00:17.320
 Many employers require the use of certain technologies. And so for many people kind of saying, I'm not going to use Gmail anymore, may genuinely not be an option because their employer may use it or require it.

1:00:17.320 --> 1:00:30.320
 And then kind of more practically, in many cases, people would need to be opting out of modern life. And that could really impact their time, their ability to interact with various social circles and groups.

1:00:30.320 --> 1:00:38.320
 And so these are, and then also there's no granular control. This is something about typically terms of service, you know, it's this all or nothing.

1:00:38.320 --> 1:00:44.320
 Do you accept it? Lauren? And can you pass the?

1:00:44.320 --> 1:00:53.320
 This is kind of in the vein of like things that you like would need to opt out of modern life for but it's like you wouldn't even have a choice.

1:00:53.320 --> 1:01:07.320
 Something that always, I always wonder about is how a lot of people seem completely convinced that their devices are listening to them constantly and their advertising is being targeted based on what they're saying out loud to people.

1:01:07.320 --> 1:01:21.320
 Which based on like my experience in the tech industry. I just think that sounds completely implausible the way that things work right now and it's my guess is that it's all just confirmation bias but I was wondering if you have any thoughts on that.

1:01:21.320 --> 1:01:32.320
 Yeah, I mean my understanding is that that is confirmation bias or that I mean so much other data is being collected that that's how these predictions are made.

1:01:32.320 --> 1:01:48.320
 Yeah, I guess I meant more on the subject of like the fact that people, that people think this but they still use their phone. Yeah, well just the whole psychology around being like angry about this thing that you're convinced is happening, but probably isn't better.

1:01:48.320 --> 1:01:59.320
 Was that a hand? Oh, here past the catch box.

1:01:59.320 --> 1:02:10.320
 Yeah, I think it kind of says a lot that we trust a lot of these companies so little that they're like that story like kind of circulates and everybody's like, yeah, I believe it.

1:02:10.320 --> 1:02:22.320
 I think like I mentioned at one point that like when I was doing fieldwork talking to Uber drivers they mentioned that like Uber had suspended a bunch of people that have like met in a physical place to like talk about unionizing back in 2015.

1:02:22.320 --> 1:02:33.320
 And like, I had never met anybody that had actually happened to like who could say that their account was suspended, but like, it was a thing that you just believe, because it was like of course Uber would track where you are off the clock.

1:02:33.320 --> 1:02:37.320
 Of course, like, I just, I accept that as fact.

1:02:37.320 --> 1:02:45.320
 So it's like, it's sort of a sad state that, like, when somebody is like, Oh yeah, Facebook's like listening to our conversations and so that's how it has all these ads and stuff.

1:02:45.320 --> 1:02:54.320
 Yeah, I believe it. But I think, yeah, like Rachel's, Rachel's relaying that like Facebook and all these other companies have said, we don't, we can't do that.

1:02:54.320 --> 1:03:05.320
 I mean, technically, it wouldn't even work out, but like they just collect so much data from so many other avenues that they can piece together things that are uncannily and scarily representing the stuff that we've experienced and said and done.

1:03:05.320 --> 1:03:10.320
 Sure.

1:03:10.320 --> 1:03:20.320
 I also think too, like, when, when people like believe that they can't do anything about being surveilled, it also creates kind of a brain powerlessness.

1:03:20.320 --> 1:03:35.320
 The things that we can actually fight against, we don't. So I think one really cool thing about the CCPA is that there are now abilities for people that reside in California that actually conduct contesting the data being collected.

1:03:35.320 --> 1:03:53.320
 A really interesting story from a friend of mine, she works on the Cypriot border and Cypriot border on a bicameral project, and their employer wanted to have everybody put their fingerprint in so they could like measure cardience.

1:03:53.320 --> 1:04:09.320
 The people on the Greek Cypriot side were able to contest that face-to-face APR, while her co-workers on the Northern Cypriot or Turkish side are not part of the EU, so are not subject or have the rights of GDPR, and their biometric data was collected.

1:04:09.320 --> 1:04:17.320
 So like I think, I don't know, I think about those two things, like the CCPA works because we use it.

1:04:17.320 --> 1:04:23.320
 And we're able to push for that. And other people, you know, in other states don't have that right now.

1:04:23.320 --> 1:04:26.320
 Great, thank you. Now that was a great example too.

1:04:26.320 --> 1:04:39.320
 Yeah, and I will say, and we'll kind of get to this in the final section which we'll cover next week, but that these are really collective problems that need to be addressed collectively.

1:04:39.320 --> 1:04:51.320
 And so I think it's understandable that it can feel incredibly discouraging as an individual because it is something that you don't solve as an individual by opting out, but that we're going to need kind of collective solutions to.

1:04:51.320 --> 1:04:54.320
 We'll talk more about that that next week.

1:04:54.320 --> 1:04:59.320
 But yeah, thank you. Thank you for that example.

1:04:59.320 --> 1:05:09.320
 And let me briefly start on this kind of other common objection that I hear as well. If you've done nothing wrong, you should have, you should have nothing to hide.

1:05:09.320 --> 1:05:22.320
 And I see eyes rolling and people laughing so I think you realize this, but you know some groups face much higher scrutiny, particularly black people, Muslims, queer people, immigrants, and many others.

1:05:22.320 --> 1:05:32.320
 This is supported by research. Black people in California are stopped far often more by police even though they're not found to be breaking the law at higher rates.

1:05:32.320 --> 1:05:53.320
 The perpetual lineup, another study from Georgetown Law Center for Privacy and Technology found that the photos of black people are disproportionately likely to appear in these kind of databases that are being used by law enforcement with no regulation and no oversight.

1:05:53.320 --> 1:06:10.320
 We also see this with kind of arrest rates for, you know, like marijuana possession or these minor offenses where research shows black and white people use marijuana at the same rates, yet the arrest rates are very different.

1:06:10.320 --> 1:06:37.320
 And a point that kind of came up in a few different articles is that with surveillance you can kind of get somebody on anything. If you collect enough, enough data about people, you in most cases will be able to get someone if you're trying, if your goal is to target that person and certain groups face kind of much, much higher risk of this.

1:06:37.320 --> 1:06:59.320
 And then I guess so. Maybe we'll discuss this next time. So this is one of the readings last week that we didn't get to, but I thought it was actually particularly relevant this week of this idea of do artifacts or particular technologies kind of lend themselves towards certain uses in certain kind of directions of power flow.

1:06:59.320 --> 1:07:22.320
 And just in my final minute, kind of an example that I wanted to share this week. So on Twitter, Joe Redmond, who's a computer vision researcher who wrote the YOLO, You Only Look Once paper, kind of really widely used and popular work said, I stopped doing computer vision research because I saw the impact my work was having.

1:07:22.320 --> 1:07:40.320
 I love the work with the military applications and privacy concerns eventually became impossible to ignore. And so this is kind of really significant and here Joe's identifying that computer vision seems to be kind of overwhelmingly used in this way.

1:07:40.320 --> 1:08:00.320
 And that was cause for him to stop. There's a lot of discussion and debate about this that you can find on Twitter. And then Timnit Gebru, who also has a PhD in computer vision, says that she thinks she is headed towards that direction and has been talking about it constantly.

1:08:00.320 --> 1:08:15.320
 And so that's kind of a very relevant, just happened in the last few days, but here are kind of prominent computer vision researchers kind of weighing does this technology kind of point in that direction.

1:08:15.320 --> 1:08:26.320
 And then we're at eight o'clock. I'm sorry to cut it off. We will pick back up here though and we'll discuss this more next time. I'm also sorry to leave it on a bit of a downer of a class.

1:08:26.320 --> 1:08:33.320
 Next time I will talk more about kind of some what I think the directions towards solutions and trying to address this are.


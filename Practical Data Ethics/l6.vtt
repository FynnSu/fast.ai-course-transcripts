WEBVTT

00:00.000 --> 00:04.300
 All right, so algorithmic colonialism, and actually week six is gonna have two separate

00:04.300 --> 00:05.300
 parts.

00:05.300 --> 00:09.200
 So we'll talk some about colonialism, and then I want to talk about kind of next steps

00:09.200 --> 00:12.220
 and where do we go from here after that.

00:12.220 --> 00:23.820
 So I had some articles about Facebook's internet.org, which was later rebranded as Free Basics.

00:23.820 --> 00:29.100
 And this is something like I definitely remember years ago when I first heard about internet.org,

00:29.100 --> 00:33.120
 assumed it was a nonprofit, it was not.

00:33.120 --> 00:40.600
 And so in this, the Atlantic article by Adrienne LaFrance, Facebook and the New Colonialism,

00:40.600 --> 00:49.580
 she cites this kind of blow up on Twitter in 2016, where Marc Andreessen, kind of famous

00:49.580 --> 00:55.740
 investor said, anti-colonialism has been economically catastrophic for the Indian people for decades.

00:55.740 --> 00:56.740
 Why stop now?

00:56.740 --> 01:02.400
 So kind of very offensive remark, he later deleted this tweet, and this was in response

01:02.400 --> 01:11.140
 to India at the time had voted to not allow Facebook's Free Basics, since it was a violation

01:11.140 --> 01:12.920
 of net neutrality.

01:12.920 --> 01:19.960
 And the idea of Facebook's Free Basics is kind of creating this walled garden of people

01:19.960 --> 01:23.680
 could access the internet for free.

01:23.680 --> 01:29.240
 That's all in quotation marks because it was they could access Facebook and a select list

01:29.240 --> 01:34.580
 of kind of whitelisted sites in which US corporations were overrepresented, but they would have

01:34.580 --> 01:36.480
 to pay to access other sites.

01:36.480 --> 01:42.720
 And so for people that couldn't afford data, you know, this is really kind of steering

01:42.720 --> 01:45.780
 them into what websites they will visit.

01:45.780 --> 01:51.920
 And many of them were, you know, accessing the web through Facebook as their portal.

01:51.920 --> 01:57.880
 And actually, I forgot to include this, but there have been studies in kind of in many

01:57.880 --> 02:04.320
 many countries in the global south of people use Facebook and the internet interchangeably,

02:04.320 --> 02:07.420
 because when they're accessing the internet, it is through Facebook.

02:07.420 --> 02:13.840
 But that means there are a lot of kind of design decisions and priorities from Facebook

02:13.840 --> 02:18.840
 that influence kind of what and how they access sites.

02:18.840 --> 02:22.400
 And this was actually part of Facebook's business development group.

02:22.400 --> 02:26.800
 So the idea was kind of like, oh, this potentially will lead us to more customers later.

02:26.800 --> 02:31.440
 But it was pitched as, you know, we're doing something nice that we can probably profit

02:31.440 --> 02:32.440
 off of later.

02:32.440 --> 02:39.460
 I mean, I guess another note that comes up a lot is that in practice, it I think often

02:39.460 --> 02:43.200
 is not necessarily well, it varies from country to country, but in many cases, the people

02:43.200 --> 02:48.680
 that are using it are kind of paying for internet soon after.

02:48.680 --> 02:57.360
 So it isn't necessarily as much of a gateway as it kind of first suggested.

02:57.360 --> 03:03.160
 So in the article, they quote an English professor at Emory saying, I'm loath to toss around

03:03.160 --> 03:09.200
 words like colonialism, but it's hard to ignore the family resemblances.

03:09.200 --> 03:17.440
 And they listed some similarities about using words like equality, democracy, basic rights,

03:17.440 --> 03:23.240
 masking the long term profit motive, justifying the logic of partial dissemination is better

03:23.240 --> 03:24.240
 than nothing.

03:24.240 --> 03:27.940
 So here, you know, even though it's not giving people access to the full internet, and it's

03:27.940 --> 03:33.580
 just this shorter list of sites, isn't that better than nothing?

03:33.580 --> 03:42.960
 Partner with local elites, invested interest, and accuse critics of ingratitude.

03:42.960 --> 03:51.040
 And then kind of one case study is, so Free Basics was, or is present in the Philippines,

03:51.040 --> 03:58.400
 and Facebook played kind of a very crucial role to the election of Duarte there.

03:58.400 --> 04:02.280
 And this article, this is a pretty kind of thorough article on it.

04:02.280 --> 04:06.240
 The platform is a leading provider of news and information, and it was a key engine behind

04:06.240 --> 04:10.880
 the wave of populist anger that carried Duarte all the way to the presidency, where he's

04:10.880 --> 04:19.160
 been kind of committing a number of extrajudicial killings in the name of kind of a drug war.

04:19.160 --> 04:24.000
 Some Filipinos say Facebook treats the Philippines as an absentee landlord might.

04:24.000 --> 04:28.720
 And so it's been, and again, there was, you know, Facebook, and I think they did this

04:28.720 --> 04:33.640
 for many political campaigns where, you know, they offered kind of help with the campaign,

04:33.640 --> 04:39.160
 but on the whole has not been very involved in some of the negative issues or disinformation

04:39.160 --> 04:43.160
 campaigns in addressing them.

04:43.160 --> 04:48.960
 Any questions or thoughts?

04:48.960 --> 04:57.440
 Another, and this is a keynote that Sarita Amrut, who's a director of research at Data

04:57.440 --> 05:07.280
 and Society, gave on tech colonialism, kind of giving another definition of kind of how

05:07.280 --> 05:16.000
 colonialism is defined and saying that a colonial relationship ultimately is hierarchical, extractive

05:16.000 --> 05:17.000
 and exploitative.

05:17.000 --> 05:21.720
 And I thought those three adjectives were very helpful to think about, and in thinking

05:21.720 --> 05:28.360
 about tech and kind of what is the relationship between tech companies and the people confusing

05:28.360 --> 05:39.120
 technology and in which cases is it hierarchical, extractive, and or exploitative.

05:39.120 --> 05:54.200
 All right, another assigned reading was this one by Abebe Berhane, The Algorithmic Colonization

05:54.200 --> 05:55.820
 of Africa.

05:55.820 --> 06:01.280
 And she writes, the discourse around data mining and a data-rich continent, common language

06:01.280 --> 06:06.280
 within Africa's tech scene, shows the extent to which the individual behind each data point

06:06.280 --> 06:10.600
 remains inconsequential from their perspective.

06:10.600 --> 06:18.300
 And so kind of focusing, you know, mining this process that's usually about, about kind

06:18.300 --> 06:24.080
 of hard materials, not about people.

06:24.080 --> 06:28.260
 The discourse of mining people for data is reminiscent of the colonizer attitude that

06:28.260 --> 06:38.560
 declares humans as raw material for the taking.

06:38.560 --> 06:45.240
 There was an article on Huawei technicians helping African governments spy on political

06:45.240 --> 06:46.640
 opponents.

06:46.640 --> 06:54.700
 So in Uganda and Zambia, the government was able to contact Huawei to intercept encrypted

06:54.700 --> 06:57.600
 communications to track opponents.

06:57.600 --> 07:03.120
 There was a Wall Street Journal investigation on this, and I think this is incredibly concerning.

07:03.120 --> 07:10.120
 And this was a map from it of kind of how many, how many countries have surveillance

07:10.120 --> 07:11.120
 projects.

07:11.120 --> 07:20.680
 And again, you're kind of, I think, getting here this weird corporate public partnership

07:20.680 --> 07:28.320
 that often kind of this is not well, not well defined to include accountability.

07:28.320 --> 07:34.840
 Raman Chowdhury talked about this in her talk on algorithmic colonialism.

07:34.840 --> 07:40.360
 She said, a lot of this technology lends itself towards a centralization of power.

07:40.360 --> 07:46.520
 Efficiency seems to dominate everything, but efficiency actually pushes you towards centralization.

07:46.520 --> 07:52.960
 And this reminded me a bit of the Politics of Artifacts article again, I was at a hand

07:52.960 --> 08:01.420
 or Politics of Artifacts article of, yeah, this idea of are there technologies that lend

08:01.420 --> 08:05.960
 themselves more to kind of centralization versus, versus distribution.

08:05.960 --> 08:16.400
 But right now there is, I think, a lot of focus on efficiency, which has, has some downsides.

08:16.400 --> 08:21.520
 Another quote from her talk, and this talk is available kind of completely as a transcript

08:21.520 --> 08:25.160
 and I believe I linked to it in the optional reading.

08:25.160 --> 08:30.360
 Good governance means systems of accountability, the privatization of civic digital infrastructure

08:30.360 --> 08:35.120
 is dangerous because we don't have the same level of redress with private companies that

08:35.120 --> 08:43.080
 we do with government.

08:43.080 --> 08:50.680
 Okay.

08:50.680 --> 09:00.040
 So one question was, are there good counter examples for tech colonialism?

09:00.040 --> 09:12.160
 And there was in the Samrita Amrut keynote at the end, she does cite some examples, I

09:12.160 --> 09:18.520
 think kind of positive examples of local communities building technology.

09:18.520 --> 09:22.180
 And I'll even maybe I'll pull that up during the break as kind of yeah, as a positive counter

09:22.180 --> 09:23.180
 example.

09:23.180 --> 09:26.240
 That's a good question, Amalia.

09:26.240 --> 09:35.000
 And then we have a comment, oh, yeah, a great comment from Aaron, that regarding the Deregite

09:35.000 --> 09:39.400
 presidency, we also have to remember that in the historical context, the Philippines

09:39.400 --> 09:41.440
 was an American colony.

09:41.440 --> 09:46.320
 And that's, I think, true of many of these that, you know, we're talking about kind of

09:46.320 --> 09:52.200
 this historical definition of colonialism, and then, you know, this kind of more modern

09:52.200 --> 09:59.680
 technical or data colonialism, but these things are not completely separate, and there's certainly

09:59.680 --> 10:02.680
 overlap and relationship there.

10:02.680 --> 10:13.880
 All right, then another article that I thought interesting was this one from Nature, contracting

10:13.880 --> 10:18.000
 people through phone call data improve their lives.

10:18.000 --> 10:21.880
 Researchers have analyzed anonymous phone records of tens of millions of people in low

10:21.880 --> 10:23.720
 income countries.

10:23.720 --> 10:27.440
 Critics question whether the benefits outweigh the risk.

10:27.440 --> 10:32.600
 So for example, the UN's World Food Program analyzed anonymized call records to find out

10:32.600 --> 10:37.760
 where people needed food or cash assistance after an earthquake in Nepal in 2015.

10:37.760 --> 10:43.640
 In 2012, researchers studied records from nearly 15 million mobile phone subscribers

10:43.640 --> 10:49.080
 in Kenya and quantified the seasonal migrations of people who travel to work on tea plantations

10:49.080 --> 10:54.960
 northeast of Lake Victoria, where malaria is a problem.

10:54.960 --> 11:01.600
 And so I think this, some of these I think are probably kind of harder projects to evaluate.

11:01.600 --> 11:04.120
 You know, are these kind of positive or negative?

11:04.120 --> 11:10.200
 And I thought this was a very thoughtful article, kind of exploring what are some of the downsides

11:10.200 --> 11:16.760
 of, I think particularly, I know in the past I have not always been critical enough around

11:16.760 --> 11:22.440
 data for good projects or the idea of the kind of the positive potential there, but

11:22.440 --> 11:25.800
 recognizing the risk and downsides.

11:25.800 --> 11:32.720
 And so here they give kind of an overview of many of the countries that have had various,

11:32.720 --> 11:37.360
 and this is where kind of cell phone data is collected from the carrier.

11:37.360 --> 11:45.240
 It is technically anonymized, although as we've kind of seen previously, you know, their

11:45.240 --> 11:48.200
 concerns of is data really ever anonymized?

11:48.200 --> 11:52.720
 Can it be de-anonymized?

11:52.720 --> 11:58.880
 So for a few examples, in Turkey, international teams tracked Syrian refugees.

11:58.880 --> 12:06.400
 In Nepal, they tracked people after the 2015 earthquake.

12:06.400 --> 12:13.520
 Sierra Leone, they analyzed call records to examine travel restrictions during the Ebola

12:13.520 --> 12:21.000
 outbreak of 2014 to 2016, and so some of the concerns are there's a lack of consent here.

12:21.000 --> 12:25.560
 This is typically, I think, something that people do not have any choice about whether

12:25.560 --> 12:28.480
 they're opting in or not.

12:28.480 --> 12:34.080
 There's the potential for breaches of privacy, even from anonymized data sets.

12:34.080 --> 12:41.280
 There's the possibility of misuse by a commercial or government entities interested in surveillance,

12:41.280 --> 12:44.280
 and then the kind of this question of are the results even being used?

12:44.280 --> 12:48.740
 In many cases, kind of the, you know, the conclusions may be interesting, but are they

12:48.740 --> 12:57.100
 actually kind of making their way into policy or structural changes, and are there other

12:57.100 --> 12:58.920
 ways to gather this data?

12:58.920 --> 13:04.480
 Because in many cases also the data necessarily, I think they give one example of, you know,

13:04.480 --> 13:09.480
 discovering that refugees were isolated, which, you know, perhaps they didn't need all these

13:09.480 --> 13:11.840
 phone records to discover.

13:11.840 --> 13:16.960
 Would there be ways to kind of survey people in a way where consent is clearer and more

13:16.960 --> 13:17.960
 upfront?

13:17.960 --> 13:21.880
 Are there any kind of thoughts or reactions to this article?

13:21.880 --> 13:22.880
 Ali?

13:22.880 --> 13:23.880
 Yeah.

13:23.880 --> 13:34.280
 When I was sort of thinking about this, it sparked this thought about, so like back in

13:34.280 --> 13:39.880
 my research group, we have this database of all the people that have been affiliated with

13:39.880 --> 13:44.000
 the group, and I realized when I was digging around in the database that for some reason

13:44.000 --> 13:49.320
 there was a column of gender, and it sort of raised all of these kinds of questions,

13:49.320 --> 13:54.840
 which were like gender identity is a really personal thing that's maybe not totally relevant

13:54.840 --> 14:01.040
 to the research group, and so it's questionable why we even have that column.

14:01.040 --> 14:04.920
 It was like generally empty for everybody, but still the implication was that we wanted

14:04.920 --> 14:09.760
 to collect that data for a reason, and like are the results even being used?

14:09.760 --> 14:14.560
 There could be an argument that we could be using that if we collected it to think about

14:14.560 --> 14:19.000
 whether we have gender parity in the group or what have you, but we weren't doing that,

14:19.000 --> 14:23.320
 so it was sort of just like performatively sort of setting ourselves up to collect data

14:23.320 --> 14:28.880
 with no real plan to use it in a useful way, or in a constructive way.

14:28.880 --> 14:34.600
 Yeah, so it sort of raised all these questions of like, okay, well maybe vaguely you can

14:34.600 --> 14:39.840
 just search for something useful or positive, but this isn't actually happening that way,

14:39.840 --> 14:46.840
 and if somebody's like non-binary or something, then that information in particular getting

14:46.840 --> 14:52.000
 sort of like leaked could be, it could endanger them in a really serious way if they're traveling

14:52.000 --> 14:53.960
 to another country or what have you.

14:53.960 --> 15:00.320
 So it just raises all of these issues, and also costs us a lot of energy and effort to

15:00.320 --> 15:05.000
 maintain this information, to say nothing of like, we're not using it for any constructive

15:05.000 --> 15:09.960
 positive use, so what was the point of all this effort and struggle?

15:09.960 --> 15:16.760
 But yeah, it really raised this sort of, it reminded me of that issue where it was like,

15:16.760 --> 15:19.960
 there's all of this stuff going on, why are we doing all of this?

15:19.960 --> 15:20.960
 Cool, thank you.

15:20.960 --> 15:21.960
 That's a great example.

15:21.960 --> 15:29.600
 I should highlight something else they talked about in the article was in the example of

15:29.600 --> 15:34.720
 malaria, where they were tracking the kind of migration of people to an area where malaria

15:34.720 --> 15:37.200
 was a bigger risk.

15:37.200 --> 15:40.080
 Some people that work on malaria pointed out like, we already have all these interventions

15:40.080 --> 15:45.280
 that we know we work, like mosquito nets and pesticide that aren't necessarily getting

15:45.280 --> 15:51.800
 enough support or funding, you know, do we need kind of this fancy data project, like

15:51.800 --> 15:57.560
 is this yielding results compared to things that may seem kind of less exciting but are

15:57.560 --> 16:00.240
 proven and kind of underfunded?

16:00.240 --> 16:07.200
 Let me check the group chat.

16:07.200 --> 16:16.000
 Oh, a lot of comments.

16:16.000 --> 16:30.400
 All right, so I'll read some of the comments from the group chat, and there's a comment

16:30.400 --> 16:33.380
 that the live chat is finicky.

16:33.380 --> 16:39.600
 Without the growth of AI, we wouldn't have Africa's Indaba X, Latin America's Kipu, so

16:39.600 --> 16:46.360
 there are some kind of great local movements.

16:46.360 --> 16:50.760
 Someone says, I'm not surprised it's Huawei given the Chinese government effort in monopolizing

16:50.760 --> 16:51.760
 data.

16:51.760 --> 16:52.760
 Yeah, sure.

16:52.760 --> 16:53.760
 Do you want the...

16:53.760 --> 17:11.840
 I was just thinking about last week on Fresh Air, they did a review of a CIA-backed surveillance

17:11.840 --> 17:20.320
 company that operated from like into World War II through 2018, and it was a Swiss company

17:20.320 --> 17:25.080
 that was also backed by the German intelligence group.

17:25.080 --> 17:31.440
 And so pretty much what they said, and soon it was called cyber, or crypto AG, born directly

17:31.440 --> 17:39.320
 from the tech that we used to, you know, cryptography in World War II, and it was very similar to

17:39.320 --> 17:43.320
 what was described in that Huawei thing, it said we weren't like going back and even like

17:43.320 --> 17:48.960
 Carter during the Egypt and Israeli to go peace negotiations was getting direct information

17:48.960 --> 17:53.520
 about what the Egyptians thought it was saying, and like it continued all over the years.

17:53.520 --> 17:54.520
 Oh, wow.

17:54.520 --> 17:55.840
 Yeah, no, I'll look that up.

17:55.840 --> 17:56.840
 That's interesting.

17:56.840 --> 18:01.760
 Yeah, it was just one of those things like 2018, very recent.

18:01.760 --> 18:02.760
 Thanks.

18:02.760 --> 18:07.200
 Yeah, let me keep going.

18:07.200 --> 18:11.480
 The things Amroot references are in-person talks, community movements, which are great,

18:11.480 --> 18:16.400
 but I'm wondering if there are any tech companies building with these principles in mind.

18:16.400 --> 18:17.400
 That is a good question.

18:17.400 --> 18:23.000
 I don't know of any, like there's no company that I am familiar with and think of as like,

18:23.000 --> 18:29.400
 oh, they're a great example of anti-colonialism, although if anybody has one, feel free to

18:29.400 --> 18:31.920
 share and that goes for folks in the group chat.

18:31.920 --> 18:32.920
 Okay, sure.

18:32.920 --> 18:47.560
 I have heard folks from Salesforce.org talking quite a bit about when you're creating data

18:47.560 --> 18:51.840
 sets from your own data about doing direct-to-date, they put in a bunch of cues into, do you really

18:51.840 --> 18:52.840
 want to include that in your data set?

18:52.840 --> 18:57.800
 Some of these things are being talked about here, and so I'm not going to work for Salesforce

18:57.800 --> 19:03.040
 there, but it sounded like they were trying to make strides to do some of these very direct

19:03.040 --> 19:06.440
 things from the title work side, I don't know if that's what you're talking about.

19:06.440 --> 19:07.440
 Yeah, and that is true.

19:07.440 --> 19:13.240
 I know also Salesforce doesn't let people use their databases for facial recognition

19:13.240 --> 19:19.880
 and I think also for making predictions based on medical data, so they do have some constraints

19:19.880 --> 19:21.720
 there.

19:21.720 --> 19:26.440
 Let me keep reading.

19:26.440 --> 19:31.640
 It is important to note that African startups are also using data in a way similar to Western

19:31.640 --> 19:37.120
 tech companies, meaning in an opaque way.

19:37.120 --> 19:41.760
 Regarding counter examples to tech colonialism, Indonesia has done quite well regarding tech.

19:41.760 --> 19:45.480
 It has been open to foreign tech companies while managing to rein them in much better

19:45.480 --> 19:48.040
 than Myanmar.

19:48.040 --> 19:52.200
 Being the fourth largest population in the world and an important growing market, plus

19:52.200 --> 19:57.320
 being a strongest democracy helps, I guess, Facebook, et cetera, has been used to spread

19:57.320 --> 20:03.120
 extremism however.

20:03.120 --> 20:06.960
 Although tracking people through phone call data via endeavors such as data for good for

20:06.960 --> 20:12.280
 altruistic purposes, it does offer the possibility for governments to then use that technology

20:12.280 --> 20:20.440
 to squash dissent.

20:20.440 --> 20:24.640
 And Erin says, I think it was more socially acceptable for Facebook being so widespread

20:24.640 --> 20:30.720
 as a US company made by the US is seen as superior versus their own companies from the

20:30.720 --> 20:31.720
 Philippines.

20:31.720 --> 20:35.320
 Okay, there are a lot of comments.

20:35.320 --> 20:44.240
 Okay, I may not read them all because I think the most talkative part of the class is all

20:44.240 --> 20:50.380
 live streaming as opposed to here in person.

20:50.380 --> 20:56.680
 I think the seeds of the US-UK alignment goes back to this affinity for colonialist, imperialist

20:56.680 --> 21:03.960
 views of creating more capital driven markets.

21:03.960 --> 21:07.400
 Regarding data providers was also thinking about telecommunications companies that are

21:07.400 --> 21:11.960
 the dominant force that allow Facebook or other companies to be like this.

21:11.960 --> 21:17.700
 That is the case with Facebook's free basics that they typically partner with the kind

21:17.700 --> 21:28.960
 of local phone company in a country to kind of provide the service.

21:28.960 --> 21:32.740
 Alignment of telcos and Facebook in developing markets in Asia largely has to do with how

21:32.740 --> 21:40.960
 the majority of telco subscribers in.ph,.id, et cetera, are prepaid.

21:40.960 --> 21:45.460
 Disregarding the ethics of a company profiting from international customers without sharing

21:45.460 --> 21:47.320
 profits in target countries.

21:47.320 --> 21:49.000
 I think that's a great point.

21:49.000 --> 21:53.720
 All right, okay, I'm gonna keep going.

21:53.720 --> 21:58.040
 But yeah, this is good discussion.

21:58.040 --> 22:04.000
 Another article I wanted to highlight was Sarah Hooker, who is a researcher at AI, but

22:04.000 --> 22:10.240
 she founded something called Delta Analytics, which is a data for good organization, but

22:10.240 --> 22:13.280
 she wrote a post on why data for good lacks precision.

22:13.280 --> 22:17.280
 Data for good says little about the tools being used, the goals of the endeavor, or

22:17.280 --> 22:18.280
 who we are serving.

22:18.280 --> 22:24.400
 So I thought that was a helpful point because data for good does get kind of bandied about

22:24.400 --> 22:26.280
 quite often indiscriminately.

22:26.280 --> 22:31.720
 Okay, let me pick back up now.

22:31.720 --> 22:37.200
 So I want to kind of spend the last part of class talking about where we go from here

22:37.200 --> 22:44.280
 and how we can all kind of continue this work related to data ethics.

22:44.280 --> 22:53.280
 And so first I'm going to address a few risk or common topics that come up.

22:53.280 --> 22:59.420
 And one is the risk of ethics washing, which is, this is something I think we've seen both

22:59.420 --> 23:04.640
 with the green and sustainability movement and the diversity and inclusion movement.

23:04.640 --> 23:09.720
 And it's tough because I think there are a lot of people that genuinely can care about

23:09.720 --> 23:16.320
 about data ethics, just like many people, genuinely care about sustainability and diversity

23:16.320 --> 23:17.320
 inclusion.

23:17.320 --> 23:22.920
 But these movements can kind of get, well, you know, they can both get co-opted in ways

23:22.920 --> 23:25.760
 that are harmful.

23:25.760 --> 23:31.060
 And sometimes I think it's not even an intentional co-opting, but I think there are many people

23:31.060 --> 23:37.840
 that would kind of say that they care about diversity and inclusion or being green, but

23:37.840 --> 23:40.960
 in a company when they have to make a hard decision that would cost them money are not

23:40.960 --> 23:45.400
 able to kind of make that decision or see that objectively.

23:45.400 --> 23:49.620
 And so kind of as a result, I think we've had a lot more talk about all these topics

23:49.620 --> 23:54.160
 than we've had in terms of kind of concrete progress, which is not to minimize the progress

23:54.160 --> 23:55.160
 we've had.

23:55.160 --> 23:59.320
 But I think kind of looking at both of those can be helpful and seeing what some of the

23:59.320 --> 24:07.320
 pitfalls that we are experiencing in data ethics and will probably continue to experience.

24:07.320 --> 24:13.760
 And so in particular, I've looked at, I called it diversity branding, I wrote a post in 2015

24:13.760 --> 24:17.600
 where I looked at a lot of research on this, if I could do it again, I would call this

24:17.600 --> 24:24.240
 diversity washing, but research shows that many diversity programs reduce the number

24:24.240 --> 24:28.640
 of black women and black men in management and they slightly increase the number of white

24:28.640 --> 24:34.880
 women in management, but it's, they're not having the kind of desired impact and in fact

24:34.880 --> 24:38.280
 are making things worse.

24:38.280 --> 24:42.640
 Diversity structures cause people to be less likely to believe women and people of color.

24:42.640 --> 24:49.280
 And so this is a, I got a really interesting study where they, they gave participants two

24:49.280 --> 24:55.120
 different and it was a, it was an actual New York Times article about a class action lawsuit

24:55.120 --> 25:00.640
 against a company by female employees for discrimination.

25:00.640 --> 25:05.400
 And in one, one version, they had an extra sentence saying this company was selected

25:05.400 --> 25:10.240
 by, you know, working mother is a great place to work and the other, they didn't.

25:10.240 --> 25:14.960
 And then they kind of asked people, how valid do you think the woman's complaints are against

25:14.960 --> 25:17.280
 the company for discrimination?

25:17.280 --> 25:21.560
 And if the company had this, you know, accolade from working mother, even though there was

25:21.560 --> 25:26.800
 kind of no, no detail given about that, people were less likely to believe the woman who

25:26.800 --> 25:29.280
 had experienced discrimination.

25:29.280 --> 25:33.960
 And there was a kind of similar study or a similar group where they looked at a company

25:33.960 --> 25:41.440
 and they kind of added a statement about diversity to its company mission or company value statement

25:41.440 --> 25:46.980
 and then asked people, do you believe this account of a black employee who was discriminated

25:46.980 --> 25:52.360
 against and people were less likely to believe it if the, if the company had diversity as

25:52.360 --> 25:53.360
 one of its values.

25:53.360 --> 26:00.760
 And so this is, I think, some, a way that this can be very harmful when companies claim

26:00.760 --> 26:06.400
 to care about something or start kind of branding themselves that way without necessarily kind

26:06.400 --> 26:11.760
 of doing the work to improve their conditions.

26:11.760 --> 26:17.040
 And then some forms of unconscious bias training increase bias as well.

26:17.040 --> 26:23.760
 And so, and this is, none of this is an argument like blanket argument against diversity programs

26:23.760 --> 26:29.560
 or bias training, but it's that you have to be incredibly thoughtful about it.

26:29.560 --> 26:32.600
 And I think there are people that are doing it well, but I think there are also people

26:32.600 --> 26:37.360
 that kind of not doing it well is worse than not doing it at all because it can have the

26:37.360 --> 26:43.860
 opposite effect and can also kind of inspire a kind of backlash or retaliation if there's

26:43.860 --> 26:49.040
 not kind of the right structures in place and kind of like full commitment around it.

26:49.040 --> 26:54.040
 Any questions about this?

26:54.040 --> 26:59.640
 And so I think there's a kind of a very real risk that we'll see this with, with data ethics

26:59.640 --> 27:07.320
 issues.

27:07.320 --> 27:11.000
 But again, I don't want to kind of undermine the people that are doing good work and, and

27:11.000 --> 27:13.560
 genuinely, genuinely care.

27:13.560 --> 27:19.440
 All right, so how about ethics principles?

27:19.440 --> 27:25.400
 So there are a huge number of ethics principles, and I meant to look up the exact number.

27:25.400 --> 27:33.140
 This was a survey, though, I think of over, over 30 different sets of AI ethics principles.

27:33.140 --> 27:38.640
 And so I think at this point, it's helpful, and this was interesting to look at kind of

27:38.640 --> 27:43.880
 the looking at the similarities across different ones, and some of these come from civil societies,

27:43.880 --> 27:49.640
 some are from governments, from companies, various, also kind of different how they describe

27:49.640 --> 27:52.800
 it multi-stakeholder kind of different groups.

27:52.800 --> 27:59.280
 So at this point, though, I do think we probably have too many different sets of ethics principles.

27:59.280 --> 28:04.680
 I don't know that anyone needs to kind of go create their own, their own set.

28:04.680 --> 28:11.160
 And I think there is, you know, there's some potential of how these can be used to potentially

28:11.160 --> 28:16.300
 create a system for kind of accountability or how you even discuss decision making.

28:16.300 --> 28:18.960
 There are downsides as well, though.

28:18.960 --> 28:27.880
 And there was a really interesting paper by this group in AIES a year or two ago, and

28:27.880 --> 28:33.640
 they argue that many of the principles proposed in AI ethics are too broad to be useful, rather

28:33.640 --> 28:37.480
 than representing the outcome of a meaningful debate on how AI should be developed, they

28:37.480 --> 28:38.480
 may even postpone it.

28:38.480 --> 28:44.200
 And I think this is a very, very valid concern, but I think when we have kind of, you know,

28:44.200 --> 28:48.160
 broad principles like, let me look at some of the ones here, you know, promoting human

28:48.160 --> 28:53.680
 values, being responsible, fairness, accountability.

28:53.680 --> 28:56.880
 Most people are not going to say that they're against these things, but then there are all

28:56.880 --> 29:01.960
 sorts of kind of justifications that, you know, this hasn't necessarily determined,

29:01.960 --> 29:06.120
 you know, should, should law enforcement be using facial recognition or kind of a more

29:06.120 --> 29:11.400
 concrete question, and that if we kind of spend too long on kind of like the high level

29:11.400 --> 29:19.240
 or these kind of broader, broader principles, we may be postponing kind of the nitty-gritty

29:19.240 --> 29:24.120
 discussion of what does this mean in practice on kind of particular uses and whether they

29:24.120 --> 29:26.160
 should be allowed or not.

29:26.160 --> 29:29.840
 And so I thought that was kind of a good warning, and this is, this is an interesting paper

29:29.840 --> 29:30.840
 to read.

29:30.840 --> 29:40.960
 Actually, let me just check the group chat.

29:40.960 --> 29:46.000
 So there was a question, did I say that diversity and inclusion training can increase conflict

29:46.000 --> 29:47.000
 or not work?

29:47.000 --> 29:53.160
 And so what I was saying is that in some cases it has the opposite impact from desired, which

29:53.160 --> 29:56.000
 is kind of even worse than not working.

29:56.000 --> 30:03.080
 And there was also, I should have, I should have shared it, Wyvonne Hutchinson, who does,

30:03.080 --> 30:09.480
 is kind of founder of a, she's a former international human rights lawyer and founder of a kind

30:09.480 --> 30:15.200
 of diversity and inclusion consulting group, had a really great thread on situations where

30:15.200 --> 30:19.640
 she refuses to do diversity inclusion training because she thinks it would make the situation

30:19.640 --> 30:20.640
 worse.

30:20.640 --> 30:25.920
 And that, I will find that and post that in the forums because that was really fantastic

30:25.920 --> 30:29.360
 on this topic of when D&I training doesn't work.

30:29.360 --> 30:35.800
 And she gave an example of a police force where there was kind of well-documented issues

30:35.800 --> 30:42.600
 with racism and somebody did kind of racial unconscious bias training and she points out

30:42.600 --> 30:47.880
 that a, their issues weren't even about unconscious bias, they're about kind of very blatant racism.

30:47.880 --> 30:51.400
 Also that there was no change to kind of the power structures, there was no accountability

30:51.400 --> 30:57.640
 for the people involved, and in that sort of scenario there was actually a lot of vitriol

30:57.640 --> 31:02.200
 against the person that did the training, that that kind of, that person had been set

31:02.200 --> 31:06.060
 up to fail and that that was a situation where it really was kind of just going to increase

31:06.060 --> 31:10.020
 resentment and not actually address their underlying problems.

31:10.020 --> 31:14.680
 So I'll find that thread and link to it because that was, that was great.

31:14.680 --> 31:20.500
 Back to principles, let's get to a case study.

31:20.500 --> 31:27.520
 And so this is adapted from Chris Wiggins and Matt Jones and so they have mentioned

31:27.520 --> 31:32.940
 this previously, they have a course at Columbia called Data Past, Present, and Future.

31:32.940 --> 31:37.800
 And Chris Wiggins is a applied math professor and chief data scientist of the New York Times

31:37.800 --> 31:43.240
 and then Chris Jones, or Matt Jones is a history professor.

31:43.240 --> 31:50.000
 And so kind of one of the incidents as they look at is the Tuskegee syphilis trial which

31:50.000 --> 31:51.000
 is just terrible.

31:51.000 --> 31:55.760
 So this was something that went on for 40 years of just observing untreated syphilis

31:55.760 --> 32:03.520
 in African-American men, even though penicillin was discovered as an effective treatment in

32:03.520 --> 32:09.200
 the 40s, participants were not even told that they had syphilis and they were not given

32:09.200 --> 32:13.880
 kind of this well-known effective treatment.

32:13.880 --> 32:21.240
 And then 1972 a whistleblower contacted the press in 1974 in response, so there was kind

32:21.240 --> 32:26.960
 of been a big public outcry about just kind of how horribly unethical this had been.

32:26.960 --> 32:34.780
 Congress passed the National Research Act, which led to the Belmont principles.

32:34.780 --> 32:42.240
 And so, and this is kind of the basis for IRBs, institutional review boards at universities

32:42.240 --> 32:43.240
 now.

32:43.240 --> 32:49.640
 And so this I think is a very, and IRBs are not perfect, but they have, they're definitely

32:49.640 --> 32:52.000
 a big improvement on not having them.

32:52.000 --> 32:57.680
 And this was a really kind of effective I think implementation of moving from principles

32:57.680 --> 33:00.480
 to a format of accountability.

33:00.480 --> 33:07.680
 And something that Chris highlights is the reason that this worked is because universities

33:07.680 --> 33:13.840
 were reliant on federal funding and by having the funding tied to kind of following these

33:13.840 --> 33:19.780
 principles and having an IRB, that that lever of power is what made this possible.

33:19.780 --> 33:25.880
 So it's not something that you could just implement in kind of any situation, you need

33:25.880 --> 33:31.240
 to have the lever of power and accountability to make something like this work.

33:31.240 --> 33:37.640
 This is kind of a positive case study though of people and people spent years kind of coming

33:37.640 --> 33:43.480
 up with, you know, what should the principles of ethical medical research be and then what

33:43.480 --> 33:44.940
 did those look like in practice.

33:44.940 --> 33:48.440
 And then this has been very well studied of kind of how well is this working when it's

33:48.440 --> 33:49.440
 been operationalized.

33:49.440 --> 33:58.560
 And so I offer this is a bit of a successful case study of kind of something terribly unethical,

33:58.560 --> 34:03.000
 but then kind of people responding.

34:03.000 --> 34:05.440
 Any thoughts or comments on this?

34:05.440 --> 34:09.120
 And then I'm linking to their course again, all their materials are online if you wanted

34:09.120 --> 34:16.600
 to to check out more about about this or other other kind of incidences and advances in history

34:16.600 --> 34:21.400
 and how they how they related to data and power.

34:21.400 --> 34:31.280
 Let me check the the talkative group in the chat, so some discussion about D&I training

34:31.280 --> 34:32.280
 not working.

34:32.280 --> 34:39.600
 I agree with this, D&I can be used to paper over real ongoing problems with bad actors

34:39.600 --> 34:42.520
 no one wants to confront.

34:42.520 --> 34:49.840
 This is a great comment, D&I training centers privileged folk folks experience and discussions

34:49.840 --> 34:54.320
 and as a person of color it can be precarious to navigate effectively without casting yourself

34:54.320 --> 34:59.400
 in a negative light or getting saddled with extra unpaid work fixing the problem you bring

34:59.400 --> 35:02.320
 up in discussion and there is an agreement on that.

35:02.320 --> 35:09.840
 Yes, I think those are very very valid critiques of of D&I training and I think there are people

35:09.840 --> 35:15.340
 out there that do it well but you have to also I think really have the people in power

35:15.340 --> 35:19.760
 on board and be interested in making structural changes and kind of very thoughtful about

35:19.760 --> 35:22.200
 it.

35:22.200 --> 35:26.720
 And another comment even saying that diversity and inclusion isn't working as a person of

35:26.720 --> 35:31.200
 color makes one seem like a cantankerous foil sport.

35:31.200 --> 35:36.960
 Yeah, so thank you for that everyone on the group chat.

35:36.960 --> 35:46.560
 Oh and then I was going to note I did a post on the fast.ai blog that I where I interviewed

35:46.560 --> 35:51.000
 Chris Wiggins and he talked talked some more about this.

35:51.000 --> 36:02.320
 Kind of another example of moving towards kind of ethical principles and rights would

36:02.320 --> 36:08.360
 be the Universal Declaration of Human Rights which was adopted in 1948 by the the United

36:08.360 --> 36:14.560
 Nations and I should note this is not universal this was kind of just the countries that were

36:14.560 --> 36:20.640
 in the UN 48 of the 50 countries 58 countries in the UN at the time voted in favor none

36:20.640 --> 36:26.640
 voted against but this has often been elaborated in international treaties and some national

36:26.640 --> 36:33.220
 constitutions so this is kind of an example of one way of kind of trying to put rights

36:33.220 --> 36:40.500
 into a format and it's something I think particularly in Europe you know if there's a country that's

36:40.500 --> 36:45.600
 seen as not or you know seen as violating human rights can be brought up in kind of

36:45.600 --> 36:51.320
 some international courts.

36:51.320 --> 36:54.040
 Any any questions or thoughts?

36:54.040 --> 36:59.200
 Oh Claudia, this one I want to pass the catchbox back.

36:59.200 --> 37:02.200
 Cool, thank you.

37:02.200 --> 37:21.780
 Yeah I mean I think it's it's hard that's a great question yeah so when will this be

37:21.780 --> 37:27.480
 implemented for data can it be I'll say more about this in a moment I do think we're in

37:27.480 --> 37:35.000
 the kind of earlier phases of even deciding like what are how do we properly kind of frame

37:35.000 --> 37:43.240
 the the rights that we're trying to protect and then there is also the issue of getting

37:43.240 --> 37:48.580
 getting the buy-in from countries and kind of the levers of power of how do you go about

37:48.580 --> 37:55.280
 enforcing this and so like the the Declaration of Human Rights really you know grew very

37:55.280 --> 38:01.640
 much out of kind of World War II and post-World War II conditions of even to kind of reach

38:01.640 --> 38:08.760
 this sort of agreement on on what sort of rights do we want to protect and codify so

38:08.760 --> 38:12.360
 I do think we have a ways to go with with data.

38:12.360 --> 38:23.440
 But I guess I'm just thinking we need to get to that level before implementing some regulations.

38:23.440 --> 38:29.120
 Oh no yeah I think we could implement regulations like I think individual country or even individual

38:29.120 --> 38:36.360
 states like we're seeing with and someone brought up CCPA the California in California

38:36.360 --> 38:41.560
 I was can't even remember what oh you had like downloaded a new app and there was a

38:41.560 --> 38:46.280
 screen and it was like you know this app is going to track you unless you live in California

38:46.280 --> 38:51.720
 you can opt out and that was like oh this is such a you know this is like nice to have

38:51.720 --> 38:55.400
 this concrete moment of like I can and it was still kind of annoying to like you go

38:55.400 --> 39:00.640
 get your device ID that you can enter and submit this form so it is something that I

39:00.640 --> 39:11.020
 think like states can implement kind of particular localities in terms of and this is in particular

39:11.020 --> 39:17.600
 talking about the the arm of regulation which is just one way of addressing data ethics

39:17.600 --> 39:25.160
 although I do think it's a particularly important way of addressing data ethics yeah I don't

39:25.160 --> 39:30.320
 have a good answer for that that is a big question that yeah people particularly it's

39:30.320 --> 39:37.040
 kind of a risk needs to seem I think immediate and to be impacting elites often to be taken

39:37.040 --> 39:44.280
 seriously as a risk yeah I'm gonna keep going but yeah those are I mean those are yeah valid

39:44.280 --> 39:49.280
 questions that I think don't have don't have easy answers and let me check the the group

39:49.280 --> 40:05.200
 chat to see if someone else's okay a lot of comments

40:05.200 --> 40:11.560
 okay so going back to the DNI discussion someone's saying that they have gotten heat for being

40:11.560 --> 40:17.100
 a part of affinity groups like Latinx and AI or women WIML which is women in machine

40:17.100 --> 40:45.760
 learning and yeah I'm gonna I appreciate all the discussion about diversity inclusion I'm

40:45.760 --> 40:55.440
 gonna move on with the lecture because I wanted I wanted to spend time for kind of all of

40:55.440 --> 41:03.640
 you to think about what what will you do next and if there are kind of concrete things that

41:03.640 --> 41:07.280
 you've taken from this course or just are not even necessarily from this course but

41:07.280 --> 41:13.240
 kind of where you where you want to move next and continuing with this and there's a few

41:13.240 --> 41:20.040
 suggestions and so there's a post I wrote in 2018 on AI ethics resources and a few kind

41:20.040 --> 41:25.000
 of potential suggestions I have and I'll give you time in a moment to share others if you

41:25.000 --> 41:33.960
 have them one is to join or start a reading group or meetup around this topic also I like

41:33.960 --> 41:39.800
 to highlight that I think we need ethics and kind of all workplaces in all roles you can

41:39.800 --> 41:44.800
 start a lunch group at your workplace but it's not necessarily that you have to be in

41:44.800 --> 41:50.080
 a role that has you know ethics in the title to be doing ethical work and in fact as we

41:50.080 --> 41:55.200
 see with DNI diversity inclusion sometimes that's a limitation of when companies have

41:55.200 --> 41:59.160
 you know some sort of chief diversity officer that ends up just being for show where they

41:59.160 --> 42:06.960
 they don't have any power but we do we really need kind of ethics in all on all roles there

42:06.960 --> 42:13.400
 in my post I list to link to some formal programs this is very much an incomplete list but just

42:13.400 --> 42:19.040
 things to know about the Aspen tech policy hub that's here in San Francisco and it's

42:19.040 --> 42:24.600
 like a kind of like three month boot camp geared towards people that work in tech but

42:24.600 --> 42:30.280
 are interested in getting involved with policy and they kind of develop a white paper or

42:30.280 --> 42:35.600
 draft law on a topic of their interest as part of as part of it and I think it seems

42:35.600 --> 42:39.680
 like a really neat program it just began in the last year but I went to their they're

42:39.680 --> 42:44.940
 kind of equivalent of demo day where you know graduates are all kind of people who are mid

42:44.940 --> 42:51.480
 career in tech spoke about the the problems they've been working on there's tech Congress

42:51.480 --> 42:57.980
 IO which is this has been around for like six or seven years founded by Travis Moore

42:57.980 --> 43:02.440
 with the goal of putting again people that are mid career in tech getting them working

43:02.440 --> 43:09.040
 as congressional staffers and I think he may be planning to try to so that is kind of based

43:09.040 --> 43:14.820
 in DC but I think he's planning to start a local program centered around kind of local

43:14.820 --> 43:20.600
 or state government as well but they're the idea is that they're kind of not not enough

43:20.600 --> 43:27.040
 people in Congress have the technical know-how to understand a lot of the issues we're facing

43:27.040 --> 43:32.720
 and we really also kind of need people that aren't lobbyists kind of representing technical

43:32.720 --> 43:38.920
 technical knowledge the Harvard Berkman Klein Center has a number of programs in particular

43:38.920 --> 43:42.280
 I think assembly sounds like an interesting program and that's something where I think

43:42.280 --> 43:49.040
 you do two weeks are in person but then you're mostly working remotely with a team kind of

43:49.040 --> 43:56.380
 part-time on a project Mozilla media fellowships and Mozilla I think funds a lot of great and

43:56.380 --> 44:03.240
 relevant work to this area the Knight Foundation tends to have more of a journalism focus but

44:03.240 --> 44:07.320
 they have a lot of interesting projects that kind of the intersection of tech and journalism

44:07.320 --> 44:10.880
 and also journalism and disinformation and so definitely kind of look at look at what

44:10.880 --> 44:16.120
 they're funding but I wanted to kind of share those as we've this up if you want to take

44:16.120 --> 44:21.720
 photos as some kind of particular resources but again you don't have to kind of be in

44:21.720 --> 44:28.280
 some sort of formal program or formal role involving ethics to be incorporating ethics

44:28.280 --> 44:43.280
 into your work let me check the group chat and definitely everyone who's mentioned kind

44:43.280 --> 44:47.880
 of interesting studies around diversity and inclusion please post these on the forums

44:47.880 --> 45:07.000
 because I would love love to see them okay some kind of mentions of Zeynep Tufekci,

45:07.000 --> 45:13.440
 Masij Cichlowski and Cathy O'Neill as being favorite authors yeah and I definitely kind

45:13.440 --> 45:17.400
 of everyone that I included in the syllabus of course I like their work and encourage

45:17.400 --> 45:24.540
 you to kind of follow follow them and read more of it so then I went back through and

45:24.540 --> 45:27.840
 actually first I'll give a time also does anyone have any other kind of steps they're

45:27.840 --> 45:33.120
 thinking of taking or or things they would like to do in the workplace based on this

45:33.120 --> 45:46.960
 okay I'll keep going feel free to share some later if you think of them I went back through

45:46.960 --> 45:53.540
 and so it in in all of the lessons I've tried to include some I usually call them steps

45:53.540 --> 45:58.960
 towards solutions because I do realize that I don't think I'm offering something kind

45:58.960 --> 46:03.400
 of concrete enough to be a solution usually but what I think are kind of positive steps

46:03.400 --> 46:09.320
 and so I went back and tried to just pull those from previous lessons so on the topic

46:09.320 --> 46:14.320
 of disinformation I had encouraged people to to practice good social media habits and

46:14.320 --> 46:18.560
 I had a link to Mike Caulfield's work which has been great and he's actually started a

46:18.560 --> 46:24.600
 Twitter account just about COVID-19 and searching for disinformation and how to evaluate the

46:24.600 --> 46:31.560
 information you're seeing keeping perspective strengthening our institutions such as journalism

46:31.560 --> 46:38.640
 education universities and nonpartisan government departments treating disinformation as a cybersecurity

46:38.640 --> 46:46.180
 problem and developing verification tools I offered some steps towards doing better

46:46.180 --> 46:52.640
 on bias such as analyzing a project at work or school and in particular I wanted to highlight

46:52.640 --> 46:58.620
 data sheets for data sets and model cards for model reporting as two potential ways

46:58.620 --> 47:04.680
 to kind of record information about your your data set or your model working with domain

47:04.680 --> 47:13.160
 experts and those impacted increasing diversity in your workplace advocating for good policy

47:13.160 --> 47:22.500
 on privacy I talked about kind of the importance of the meaningful incredible financial penalties

47:22.500 --> 47:28.280
 and actually motivating companies looking at history and some of the kind of big regulatory

47:28.280 --> 47:34.200
 battles that people have fought in the past around issues such as kind of environmental

47:34.200 --> 47:44.780
 pollution or car safety moving towards a frame of treating privacy as a public good and regulating

47:44.780 --> 47:49.720
 data collection and political ads I shared the same up to fact she posed from 2018 where

47:49.720 --> 47:55.420
 she lays out and this was at the time of the congressional hearings where Zuckerberg was

47:55.420 --> 47:59.180
 being questioned and she says you know we don't need to ask him any questions we already

47:59.180 --> 48:03.420
 know enough about Facebook and how they behave and give some recommendations about what we

48:03.420 --> 48:11.960
 should be doing I wrote a post on four principles for responsible government use of technology

48:11.960 --> 48:19.480
 with some of the kind of themes from our tech policy workshop that we here had here at the

48:19.480 --> 48:24.000
 Center for Applied Data Ethics and then this other digital political ethics came from a

48:24.000 --> 48:34.940
 report around I think political advertising online yeah and so I wanted to kind of encourage

48:34.940 --> 48:40.220
 you to and I know I asked this is one of the quiz questions but to think about one thing

48:40.220 --> 48:44.120
 you're taking away from the course or one thing you would like to do or have already

48:44.120 --> 49:04.040
 begun in response to the course I'll give time if anyone wants to share all right and

49:04.040 --> 49:32.040
 that's awesome yeah keep me posted on how that goes it's cool any others

49:32.040 --> 49:47.680
 check the chatty online group let me see okay so I'll read a few of the comments from the

49:47.680 --> 49:55.460
 online group just by taking the course opens the opportunity to talk about the topic with

49:55.460 --> 50:01.160
 friends and co-workers creating space to reflect about the topic that's fantastic yeah I'm

50:01.160 --> 50:09.040
 happy to hear that I'm hiring hiring more my minorities consulting and data ethics I

50:09.040 --> 50:12.680
 will be implementing some of these things in my onboarding program at work that's that's

50:12.680 --> 50:22.120
 great to hear I'll be educating VCs on what to know that's fantastic when to use some

50:22.120 --> 50:26.560
 approach that avoids the stigma of program mandates but gives interviewers tools for

50:26.560 --> 50:30.600
 thinking about hiring in some representative fashion I just want to highlight and I think

50:30.600 --> 50:35.900
 I mentioned this in a previous week but I wrote a post on called how to make tech interviews

50:35.900 --> 50:41.360
 less terrible and that I looked through kind of a lot of the research around interviews

50:41.360 --> 50:48.040
 and hiring and make some recommendations they are in the context of the how much bias there

50:48.040 --> 50:54.320
 is in the process but also that kind of tech interviews are terrible for pretty much everyone

50:54.320 --> 50:58.120
 regardless and they are a hard problem but I give kind of several recommendations there

50:58.120 --> 51:03.980
 so that's a potential resource side note I'm actively working on helping with increased

51:03.980 --> 51:10.160
 and more accurate data collection on coronavirus in the Philippines lots of people who are

51:10.160 --> 51:14.200
 misreporting under reporting on data related to it and that's a great thing to be doing

51:14.200 --> 51:22.960
 oh I like this one find people with roles in the company that are closest to the channels

51:22.960 --> 51:28.760
 that users use to complain to find ways for people listening to issues to take more meaningful

51:28.760 --> 51:35.600
 actions I really like that in a previous week I quoted an Alex fierce post about kind of

51:35.600 --> 51:41.940
 trying to have trust and safety better integrated with product and engineering but along those

51:41.940 --> 51:49.160
 lines of kind of having the the complaints and ways that products misfunction making

51:49.160 --> 51:54.800
 sure that kind of information is getting back more directly to the people that are designing

51:54.800 --> 52:20.760
 and building products whoops the importance of privacy as a human right and talking about

52:20.760 --> 52:30.080
 AI I'm glad to hear that message got through and going to pursue AI policy research roles

52:30.080 --> 52:36.360
 it's great Oh PhD in cybersecurity a lot law and about to launch a consulting firm and

52:36.360 --> 52:46.960
 data protection compliance data ethics and privacy law how data shouldn't be monopolized

52:46.960 --> 52:51.100
 question I've been cold calling the 50 states and talking with public and private sector

52:51.100 --> 52:57.260
 committees they're quite lost and do you have any key go-to resources for state-level policy

52:57.260 --> 53:06.920
 makers that's a good question and I am not sure yes I don't feel that I have kind of

53:06.920 --> 53:12.020
 go-to resources on this although I do feel like groups are writing them does anyone else

53:12.020 --> 53:18.560
 want to chime in or or share resources we can also start a thread around this

53:18.560 --> 53:41.600
 okay yeah thank you yeah and I know I'm I know pa pa I partnership on AI does a lot

53:41.600 --> 53:45.920
 of reports they are such a broad umbrella that I would want to look at the individual

53:45.920 --> 53:52.080
 report to kind of have a sense of it but they do have some interesting people working there

53:52.080 --> 54:00.540
 well thank you yeah thanks for everyone on the live stream those are some great comments

54:00.540 --> 54:13.280
 also thanks to everyone in the classroom here in person okay and so then I wanted to share

54:13.280 --> 54:19.040
 a quote that I've been thinking about and so I I realized I you know I feel somewhat

54:19.040 --> 54:24.300
 dissatisfied with the steps that I offer as towards solutions like I would like to have

54:24.300 --> 54:29.220
 something kind of more concrete and comprehensive of like this is this is what we do and I sometimes

54:29.220 --> 54:32.780
 talk to people that are I think dissatisfied or you know they're like you know give me

54:32.780 --> 54:37.920
 something I can you know a checklist that I can follow and you know know that it's everything's

54:37.920 --> 54:44.820
 ethical then and so something I found really and that does not exist right now I should

54:44.820 --> 54:52.560
 say to be clear you know these are really complicated problems and it's also kind of

54:52.560 --> 54:56.640
 newer that they're being studied and talked about in the way that they are even though

54:56.640 --> 55:01.800
 you know there are fields that have been looking at related work for longer but just kind of

55:01.800 --> 55:07.720
 seeing the impact of some of these new technologies is a newer area and so this is a quote from

55:07.720 --> 55:15.020
 Julia Angwin who was a senior reporter at ProPublica she was one of the investigators

55:15.020 --> 55:22.040
 on ProPublica's investigation of the compass recidivism algorithm in 2016 which really

55:22.040 --> 55:27.880
 kind of helped kick off the fairness accountability and transparency field and she's now the chief

55:27.880 --> 55:35.060
 chief editor at the markup and she she said in an interview last year I strongly believe

55:35.060 --> 55:39.160
 that in order to solve a problem you have to diagnose it and that we're still in the

55:39.160 --> 55:46.480
 diagnosis phase of this if you think about the century the turn of the century and industrialization

55:46.480 --> 55:52.480
 we had I don't know 30 years of child labor unlimited work hours terrible working conditions

55:52.480 --> 55:57.680
 and it took a lot of journalists muckraking and advocacy to diagnose the problem and have

55:57.680 --> 56:06.140
 some understanding of what it was and then the activism to get lost changed I see my

56:06.140 --> 56:11.000
 role as trying to make as clear as possible what the downsides are and diagnosing them

56:11.000 --> 56:15.760
 really accurately so that they can be solvable that's hard work and lots more people need

56:15.760 --> 56:21.800
 to be doing it and so I really I think appreciated this work and this recognition or this this

56:21.800 --> 56:26.160
 quote and kind of this recognition of kind of how complex these problems are and just

56:26.160 --> 56:32.720
 that the work of continuing to diagnose them is is is valuable and kind of one of the steps

56:32.720 --> 56:40.320
 towards towards hopefully solution solving them and supporting kind of the the necessary

56:40.320 --> 56:45.640
 activism to solve them and this comes from an interview called forget about privacy Julia

56:45.640 --> 56:55.120
 Angwin and Trevor Paklin on our data crisis and I think I think that's all that I have

56:55.120 --> 57:00.160
 I also just want to thank all of you I've really enjoyed teaching this course this was

57:00.160 --> 57:04.040
 my first time teaching a course like this I've previously taught kind of much more technical

57:04.040 --> 57:10.400
 courses and so I was really kind of not sure how it would go to to be doing something more

57:10.400 --> 57:15.480
 qualitative and more discussion based but I've really enjoyed it and I've really appreciated

57:15.480 --> 57:21.920
 kind of everyone's comments and how much participation and thought you all have given this so thank

57:21.920 --> 57:26.080
 you.


 Oh, yeah, thank you, Lee. That was fantastic, and that's also a good lead-in. I was going to talk about our ecosystem, and I think I'm going to skip ahead to my section on metrics, just because that seems like such a kind of natural direction to go in from here. And so I'll cover the other material next week, since we definitely won't be finishing, but so this has come up previously in class, this idea of Goodhart's Law, that when a measure becomes a target, it ceases to be a good measure. Another framing, and we're about to see several examples of what this means, another framing of it is Campbell's Law, which is the more any quantitative social indicator is used for social decision-making, the more subject it will be to corruption pressures, and the more apt it will be to distort and corrupt the social processes it is intended to monitor. And so this is a little bit kind of more specific than Goodhart's Law, but covers some of the same cases. And this is a topic that I wrote a blog post about last year, and then ended up kind of extending it into a paper, if you want to hear more about this, but this is something that I think is a really kind of fundamental challenge for AI. But optimizing metrics can lead to manipulation, gaming, myopic focus on short-term goals, unexpected negative consequences, and unfortunately, much of AI machine learning focuses on optimizing a metric, because that's kind of what most techniques are doing, and that's something that we've gotten very, very good at is optimizing a metric. So one example that we saw back in week two of something that can go wrong is measurement bias, and so we talked about a paper then where the researchers wanted to predict who's most likely to have a stroke, and this could be used to prioritize patients at the ER, and they found some really surprising predictive risk factors. Such as accidental injury, benign breast lump, colonoscopy, and sinusitis, and it turns out that what they were picking out was just who utilizes healthcare versus who doesn't. And they're kind of people that have high utilization of healthcare, others that have low utilization, and this is a case where we don't actually have data of stroke, which is a region of the brain being denied oxygen from new blood flow. The data we have is a proxy, it's who had symptoms, went to the doctor, received the right tests, got the right diagnosis, and there are all sorts of factors that can influence that, including who has access to healthcare, who can afford their copay, what sort of racial or gender biases might the doctors have, cultural influences, so a lot of different things. So this is one way that metrics can fail. Another really interesting paper from the 2000s looking at the UK's healthcare system, where they moved to trying to have kind of more metrics, and they found that, it's depressing, they introduced new targets around ER wait times, and this led to people canceling scheduled operations. The hospitals would cancel operations so that they could draft more people into the ER to keep the ER wait time down if it was getting too high. Requiring patients to wait in lines of ambulances outside the hospital, because that didn't count as part of their wait in the ER if they were in an ambulance outside the ER. They turned stretchers into beds by putting them in hallways, because then they could say the person's no longer waiting, they're in a quote bed. They also found a big discrepancy in numbers reported by hospitals versus by patients, so if they surveyed hospitals on how long people were waiting, they were getting different results than if they surveyed patients on how long did you wait. And so this, and this is just kind of one of many examples this report gives of how there are unexpected kind of negative consequences of this push to emphasize numbers more, and this is a mix of, well I guess actually this is a lot of kind of gaming, I mean some of the ones about the discrepancies in numbers, some of that can be I think unintentional, but people kind of knowing what they need leads them to kind of give answers that will line up with that. Another example we've seen in the United States of kind of manipulation in gaming is that as more and more emphasis has been put on standardized testing, there have been kind of widespread scandals of school districts cheating, so this is where teachers or school administrators are altering their students' score, or answers to get higher scores, and this has happened in a number of states now that kind of testing is so high stakes. And I didn't include this, and I don't think I talked about this previously, there was a case that Kathy O'Neill covers in her book Weapons of Math Destruction, and had been covered in the, I believe the Washington Post, of a teacher in either Virginia or D.C. that was fired by an algorithm, and the principal of her school loved her, the parents of her students really loved her, and she couldn't get a conclusive answer of why she was fired, but her suspicion was that her students' scores seemed artificially high from the previous year, and they had kind of been at a different school, and then her students' scores dropped to kind of more normal levels after her teaching, and her teaching was being evaluated, and it's believed one of the factors is how the students' scores changed while they were in her class, and so there, I feel like it's kind of an intersection of two negative trends. You have the teachers who most likely at the previous school may have cheated because they felt pressured to do so, then when she didn't, then you have this other algorithmic system that's firing teachers based on these metrics, and she kind of ends up fired because of it. So that's kind of sobering, sobering example. There was also an article on essay grading software, so there's software that grades essays is being used in, I think, at least 22 states now. It was found that, so the software focuses on what you can measure, right, and we can't measure everything, but you can measure sentence length and vocabulary, spelling, subject verb agreement, however, you can't measure creativity or novelty or some more kind of abstract qualities that can still be very important in writing. And so there was a computer program that someone created that made gibberish essays with sophisticated words and sophisticated phrasing, and that got good scores. It found, the research found that essays by African-American students received lower grades from the computer than from human graders, and essays by students from mainland China received higher scores from the computer than from expert human graders, which caused the researchers to suspect that they may be using chunks of pre-memorized text. So this is kind of, now we're combining what we learned about with bias, with kind of this example of overemphasizing metrics, and for me, this kind of really strikes me in that I think of writing as something that has a very intrinsically human quality, and that it's usually humans communicating with humans, but here, you know, to have an example where a computer program can generate a gibberish essay that then will be evaluated by another computer program and receive, you know, receive a high score. But this also, to me, illustrates some of the qualities that we can't quantify, and that we can't measure, and that those tend to then not kind of be valued if they can't be measured. Now, this is an article just from the last week about TikTok marketing, and so the image is from Chipotle's hashtag burrito campaign, which was where you got, I think, a half-price burrito if you wore a costume on Halloween, and TikTok said, hey, this had 3.9 billion views. That sounds amazing. That's a ton of views. In contrast, TikTok has only been downloaded 145 million times in the USA. This was a US campaign. So what does this mean for these numbers? And TikTok won't say kind of how it classifies views, but you have a lot of companies put a lot of importance on these metrics, and it's often kind of not clear how they're gathered or even kind of what sense they make in terms of impact or even how many people actually saw this campaign. And so then people said that it reminded them of when Facebook pivoted to video several years ago, Facebook exaggerated video watch time by as much as 900% was alleged by one lawsuit, so what happened was Facebook said, you know, we're switching to video and had these really positive numbers of how much video was being watched, and so many news organizations laid off journalists in response and said, oh, you know, we want to focus more on our digital or on our video arms, and then it turns out that the numbers for video were inflated and misleading. There was also kind of an error, and this is something that there was a lawsuit about that Facebook settled for 40 million, although that was just advertisers bringing suit, so this doesn't even impact all the kind of news agencies and journalists that were impacted because many of these organizations laid off written journalists because they wanted to put more resources into video, and then when video turned out to not be as effective, then they laid off the video people they had hired, and so it was kind of two rounds of layoffs. It captures kind of how much metrics can drive, kind of drive behavior of organizations. So there's an interesting paper on the different failure modes of Goodhart's law, kind of breaking them out into four categories. So one is when the difference between the proxy and the goal is important, and I think in almost all cases, we don't actually have the data that we truly want. What we have is a proxy, and sometimes that proxy is closer than others. But in cases where the difference is significant, that has an impact, and so that's what we saw with kind of the health data about who had a stroke. There's extremal Goodhart where the relationship between the proxy and the goal is different when taken to an extreme, and so sometimes a kind of relationship makes sense, but once you emphasize it, that sends you kind of more to an extreme where it's less, where it's just a different relationship. There can be a non-causal relationship between the proxy and the goal, and intervening on one may fail to impact the other. And then there's also adversaries attempting to gain your chosen proxy. And so I thought this was kind of interesting as a breakout of different ways that this can go wrong. And then we've talked previously about YouTube's recommendation system. Guillaume Treslot has a great post about this, former kind of AI engineer at Google slash YouTube, and he quotes a YouTube blog post from several years ago in which Google's YouTube said, if viewers are watching more YouTube, it signals to us that they're happier with the content they found. It means that creators are attracting more engaged audience. It opens up more opportunities to generate revenue for our partners. And so here they're kind of boiling down that watch time is about happiness and revenue. However, I think we can all think of times that we spend online where we're not actually happy and not proud of how we've used our time afterwards. And I think this is kind of a very rough proxy. And Guillaume does a good job of kind of explaining how also this ends up incentivizing content that says that other platforms are lying to you because you should spend more time on the content from that platform in order to maximize the watch time. Are there questions so far on metrics? Erin came past the catch box. I had a question about TikTok and if you had opinions about it in relation to China, the news that's come out about that is my first question. And then I have one small comment. I love, absolutely love Gary Vee. He's an early investor in a lot of social media platforms and he's actually advocating for removing influencer numbers. This is bring value to the user, don't think about the number, but it's so hard because he's basically saying don't be a slave to the algorithm. But in fact, it's sort of like a walking contradiction. You're still making money from the content creators and producers that are sort of the things that algorithms do. It's weird, it's like a two-sided thing that I hear from him, he's promoting TikTok. I don't know if you had. Yeah, I'll put, I'll table TikTok. That's something we can discuss on the forums and I don't feel like I've been kind of informed enough opinion on it. Yeah, and this issue of Instagram removing likes in certain countries. I mean, there's still kind of this asymmetry where Instagram is still kind of like, there's still this asymmetry where Instagram is still using that data on its side to promote things and so then it's impacting people in an opaque way. Although I do also kind of see the impact of, and I can definitely recognize in myself how it influences what I post on various networks, kind of seeing what gets shared and what people respond to. Yeah, and that is, actually that's probably a good example of what Ali was mentioning of how people perform for algorithms, I definitely think there's a sense in kind of in which people can tailor their content based on what they think the algorithm's gonna reward, what they think will get kind of likes or views. And I think that can be very kind of insidious, like it's not necessarily a like, oh, I'm explicitly doing this just for the algorithm, but I think it does kind of influence many of us. Another factor is that our online environments I think are designed to be very addictive. The incentives focus on short-term metrics and the fundamental business models around manipulating people's behavior and monopolizing their time. And I think kind of what we click like on or what we retweet or share is often used as this proxy of what we like, but it's what we like in a very constrained and designed in a specific way environment. I don't know that it's necessarily, or in fact I don't think it's getting it kind of like our purist likes. And I always think of kind of the example of reading a long-form journalism article that potentially is harder to read, but months of research went into that I'm gonna help shape my thinking on a topic and I may be glad in the future that I read it versus kind of the short-term impulse of do I wanna read something more catchy and kind of easier to read in the moment. And I think we've gone through this with the food industry and are a little bit further along of a lot of junk food is really appealing in the short-term if you don't think on a long-term perspective. And then also depending on how your environment's designed, Zeynep Tafekci uses the example of a cafeteria that's kind of actively shoving junk food in our faces. We're not even having to reach for it. It's kind of being brought to us. And so whenever we think about kind of using metrics about what people prefer to remember that we're in this kind of very constrained and short-term focused environment. Hand in the back corner. I was wondering if you had any thoughts on the iPhone stream time metric, like how every week now it tells you how much time you spend on your phone and what the relationship is to your previous week. If you like maybe, or if anyone has any idea of like where that idea came from or how it's being received. That's a good, yes. I've not been following that closely. I know that I will say like the, Tristan Harris's time well spent and totally blanking on the name of Center for Humane Technology has promoted a lot of those kind of design fixes of kind of what can we change about design to help people. And so that includes things like tracking how long you're spending or sending a reminder like, hey, you've been on Twitter for a half hour. Is this how you wanna spend your time right now? Another one I know is using a black and white screen which was already an accessibility feature but using that to kind of make your tech less stimulating or appealing. I think there's value to those efforts. I don't think they get at the fundamental business model. And so if the business model is still around advertising and kind of time spent that there's, I don't think that's fully gonna solve the problem but I do think that there is some benefit to kind of thinking about that design. But it's important to kind of connect that the design didn't come out of nowhere. It's very much tied to the business model and if the underlying business model doesn't change, I don't know how much, I think there are limits to kind of what design changes can do. Oh, and then, okay, we're at eight o'clock. So we'll pick back up here and finish our ecosystem next time. So also we'll kind of talk about what are some of the forces that are kind of acting on entrepreneurs and companies in tech to kind of understand kind of these broader systems that influence us. But I will see you back next week for our last class.

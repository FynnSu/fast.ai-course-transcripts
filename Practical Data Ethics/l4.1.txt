 Yeah kind of a topic for tonight, privacy and surveillance. It's in the news a lot. Thanks, yeah, thanks for kind of discussing this and you've already brought up several points that will come up again kind of throughout this evening. My goal is to talk a little bit just about kind of some facts about the current state of things, how things are surveillance technologies are being used, some of the risks of what can go wrong, my rebuttals to a few common of really these are kind of like rebuttals to rebuttals that I hear, and then some steps towards solutions. So one of the readings was the New York Times article, your app knows where you were last night and they're not keeping it a secret, and this was from a collection of data that I think came from over 200 million phones in the United States. I thought it was kind of helpful how they visually broke it out, kind of showing the stories of individual people. So here's a woman that the New York Times identified and kind of tracked her going to and from her job, visiting her ex-boyfriend, going hiking. At one point she goes to a doctor's office, they keep track of like how long she's there, kind of these very, you know, potentially intimate details. Another, yeah, her going from her home to Weight Watchers and kind of looking at this over time. And it was something where they had, or it says here, her location was recorded over 8,600 times in four months. So a lot of very granular data. And the New York Times has kind of done a series of articles on related, and I think even like a few separate data sets, but all kind of in the same idea of kind of just how much data is being collected from people. Are there any thoughts on this article or reading it? I just heard this talk today actually about political campaigns using this data from all these sort of nefarious companies and merging with the voter records in this way that was like pretty shocking to me. And also I think the thing that was sort of shocking about it is that these campaign cycles are so short there actually can't be a lot of safeguards and how the data persists long-term and sort of there's no central responsibility for this stuff. So each of the campaigns is in some version of it in this way that was pretty frightening to me. Yeah, and that raises kind of another issue also of how data sources can be combined. And so sometimes when you're just looking at a single data source, while this, you know, seems revealing enough, it could be even significantly more revealing when combined with other data sets. Just while it's going back there, Erin raised the point about also how data can be more revealing in aggregate as well. And so there was a case with Strava a few years ago, you'll remember when they released this data set publicly, that ended up revealing not just kind of the location of several foreign military bases, but even kind of a guess at the inside architecture. And that was something that, you know, releasing the data for any one person wouldn't have revealed that, but altogether it did. And another kind of image from the New York Times article of here's someone coming to a Planned Parenthood, they know what entrance they're using, what time, what time they leave, how long they're there. This is kind of very intimate data being sold and distributed. There's another article on Rinder and OkCupid. OkCupid asks, you know, questions about drug use and sexual preferences. Again, kind of very personal and intimate data. And so and it's hard because they're also these like layers of intermediaries that this data often gets passed through. So Grindr's app includes software from MoPub, which is Twitter's ad service. MoPub shares with more than 180 partner companies. One of those partner companies is AT&T, which shares more with more than a thousand third-party providers. And so it can be even hard to kind of, I think, think about the scope of this, of kind of how many intermediaries this data gets passed through and it's not something that's kind of really regulated or tracked in a systematic way to even understand the scope. But again, this is kind of very, very personal data being sold. And something the New York Times article talks about is even, you know, even when data is aggregated it can be, many people talked about, it can, you can de-anonymize data and kind of disaggregate and identify individuals in many cases. So, oh, yes? Can you pass a... Wait, so I don't know if this is the same study that Ben talks about. I think it was Grindr that had to be bought out by the U.S. State Department because it was, I can't remember if it was Grindr or Scruff, it was one of those dating apps. But there were U.S. military personnel who were on the app, and then the app was bought out by some foreign coding command with the China, and then the U.S. State Department had to step in because it was putting their own military personnel in danger. It was one of the wilder stories. Wow, okay, I missed that. That is, yeah, that is significant, yes, yeah. And that's another issue of kind of how... I think Grindr got in a lot of trouble too because a lot of the ways it was sharing data even on itself did not have great security protocols, and they asked a lot of questions about HIV status, and it was problematic in some countries. Yeah. Wow. Thank you for those additional details. This is, yeah. Very, very significant stuff. And another topic, so that's kind of some of the phone, data from apps on our phones, is kind of police use of facial recognition. This is an article about New York Police Department, which has been putting children as young as 11 into a facial recognition database. And it's important to remember that the technology wasn't even developed on children typically, so you've got higher error rates, even if you were okay with this sort of use, but also this is kind of then taking someone at a very young age and kind of adding them to this database. The Georgetown Law Center for Privacy and Technology does a lot of great work. They did a study, Garbage In, Garbage Out, looking at how kind of facial recognition was used by police in practice, and they found a lot of kind of horrifying examples. One is of police had a suspect, it returned no matches, and they said, well, this guy kind of looks like Woody Harrelson. So the actors, they Googled his picture and then entered that into the kind of mugshot database and then use that to surface suspects. So that was covered, but it was like a series of police kind of photoshopping faces, like there was one where like the eyes were closed, so they photoshopped open eyes onto the face, also using kind of, you know, sketches of the, based on a description of the suspect and trying to use facial recognition technology on those. And in many cases, you know, this is kind of surfacing a list of like the closest matches, but that doesn't necessarily mean they're good matches at all. So it's very kind of even more concerning to hear how this is being used in practice. And then kind of another example. So this was early on in the Hong Kong protests last summer. A lot of people were circulating kind of pictures of, so typically people in Hong Kong are using kind of a cashless system with cards that automatically scan. They didn't want to be tracked that they were attending the protest and so kind of having these long lines of people paying for paper tickets. But this is, I think, a common point where some of these technologies can be very convenient when things are going well, but it's also kind of important to think about other scenarios. So next I wanted to talk about the talk by Alvaro Bedoya, and Alvaro Bedoya is director of the Georgetown Law Center for Privacy and Technology. I thought this was a kind of interesting some history on kind of the use of surveillance. So he goes back to Queen Elizabeth I building a network of spies and informants to root out perceived threats and particularly targeting Catholics and Puritans, so kind of religious minorities. In the 1880s in the US, 2,300 Mormons were prosecuted for polygamy and this was facilitated kind of through surveillance and I thought it was interesting to note that kind of the anti-Mormons were saying that Mormons were not white and were actually Muslims as a way of kind of justifying the surveillance and prosecution. The FBI amassed a 1500 page file on Cesar Chavez, who led a lot of the farm worker protests that kind of resulted in kind of more more rights and protections and Professor Bedoya writes that Cesar Chavez was a saint and they actually didn't dig anything up on him, but that very, very few of us would be able to have a FBI file of 1500 pages about our kind of every doing and what all our kind of friends and neighbors and people we interact with do and not have something very embarrassing or even, you know, like illegal show up in it and that that's kind of a very unreasonable bar to gather this much information on somebody. The FBI also tracked Martin Luther King Jr., compiled recordings of his affairs, they threatened to blackmail him and encouraged him to kill himself with this, so there's kind of a, and there are even more examples in the essay, but kind of a long history of surveillance being used in particular to target certain people. And as Professor Bedoya wrote, the pattern is we watch those who are quote, considered less than, will you spy on your superior or will you spy on the poor man, the person of color, the immigrant, the heretic, we watch those who are other. And he identifies this pattern when those others organize, mobilize that watching is redoubled, surveillance becomes a tool to stop marginalized people from achieving power. The key reaction I had to this was that notion of, especially the second pattern of thinking about, or really both patterns, was how immediate the standard is clear when you apply it the other direction. If you even think of like fiction and anybody who is casing a bank or stalking a president, the threat is immediate and obvious, but once you turn it the other way, it's more intrigue. And I think we have that natural narrative reaction that reinforces the observation here, is that we know it when we see it, but we don't recognize it as such, but we don't feel threatened by the action that we observe. Your perspective matters a lot in this, yeah, kind of what you see as a threat. Yeah, I think what I'm thinking about here is that like basically this is happening now, in particular, like I believe I recently read an article where ICE was using location-based data to identify immigrants who they wanted to target for arrest. Yeah, well, yeah, that'll come up later. But yeah, that's a great, great point. And Colin, and then I'll bring a point. There was this book about the Great Far Wall of China, because I don't remember the exact, I think that was the name, I don't remember the subtitle or the author. But they kind of bring up because censorship and surveillance are really kind of two sides of the same coin a lot of the time, because when you're trying to like look into it, you have to see what speech is there so you know what to censor. And they kind of made a point that the number one thing they kind of censor for is actually just like solidarity or people kind of working together. And there was this really interesting example where there was this comedy website, and they were very careful, there were no kind of like offensive or seditious jokes, but people from the website were meeting each other in real life. And then that started getting censored because they just had these signals, they could kind of like put their hands up to each other. And so it's like, oh, you're from this site, and we can talk offline. And so a lot of times I think that it's not just about their surveillance, but it's also there's kind of censorship and pressure to move you from unsurveilled channels of communication to only be able to use surveilled channels of communication. So that's kind of just like the other side of this pattern to watch for pushing the call. Yeah, that's great. Thank you. I'm going to move on, but I'll take more questions later. And then another quote from this. I contend that we are a nation of dissenters, and this was the first Hispanic senator in the US denouncing McCarthyism in 1950. And this was a piece of history I did not know about before reading this, but apparently after Dennis Chavez spoke up about this, that other people then started feeling like they could speak up as well. And he said our nation was created by dissenters, our ancestors were not satisfied, kind of emphasizing kind of the value and importance of dissent. And then an example kind of more recently is in 2015 in Baltimore during the protest over Freddie Gray's death, a black man killed wrongly in police custody. Police used facial recognition to identify protesters. And this was, they said, to identify protesters who had existing warrants. However, you can have warrants for relatively minor things. And this kind of came to light through some of the company that built this technology, Geofeedia, used this as a positive case study. And so, and they quote someone saying, every person we got off the streets before they hurt someone was a win, which I just thought was such a weird quote because it's before they hurt someone, they didn't hurt someone, you know, what did they do? But I think this is kind of very, very concerning to surveil protesters. Then I wanted to include kind of a few, a few things to know about how the government uses technology in practice in many cases. It's often opaque. And then this is a kind of very U.S. specific. In the U.S., they're restrictive NDAs. It's typically not evidence based. And then there's often a lot of dysfunction kind of through the whole pipeline of how contractors bid on proposals, who get selected, the asymmetry in terms of the technical knowledge that the companies have versus the people working in government trying to evaluate these proposals. And I'll say a little bit about that, but it is not a kind of efficient or well-designed process in many ways. So government use of tech is often opaque. An article, this was a year or two ago, came out that Palantir had a project in New Orleans to test predictive policing. It had been going on for seven years and city council members didn't know about it. When they were asked for comment, they were like, we didn't even know that we had this program. And it ended up was canceled kind of a few weeks after The Verge broke the story about it. New York City started an automated decision system task force to evaluate how the city was using automated decisions, automated decision systems, which I was super excited when they announced that they were doing this. They got a lot of fantastic people on the committee and then they did not give them any information about what automated decision systems New York uses because they claimed it was proprietary and that they couldn't tell them. So Meredith Whitaker, one of the founders of the AI Now Institute, was on the committee and at the end she said it was a waste and kind of sets a terrible precedent. A large portion of the committee did release a kind of shadow report dissenting with the official report. But so here, you know, is even a system where you had something that was voted on, we should do this, and then not giving the people involved information they needed to even evaluate. There's also this idea of something called parallel construction, and that's when police will use a technology to kind of gain some knowledge, not want to reveal that they use that technology and have to come up with kind of an alternative case. And so an example is stingrays or cell site simulators, which can kind of identify people's cell phones, and the company that produces these has incredibly restrictive NDAs that actually encouraged police that it was better to drop a case than to reveal that they were using this technology. And Elizabeth Jo, who's fantastic, she's a law scholar at UC Davis, she spoke about this at the tech policy workshop here at USF at Cade. And she highlighted that kind of the law develops through cases, like that's kind of how, you know, the edges of what things mean get refined, and when even the existence of a technology is hidden, there's no way for the law even to kind of like adjudicate or for us to kind of develop what should our approach on this technology be. And so that's kind of a pretty extreme case, but it often operates that way. So another point about government use of technology is that it is often not evidence based, which can be really startling given the amount of money that's often involved. So, face recognition is being used in many schools that many people have been warning for a while that it won't stop mass shootings. One key reason is that many shootings are committed by current students, they are not people that would be on a watch list, many of them don't have any sort of history. So even just like the premise is kind of off of, even if you can recognize people you don't know who who the shooters are. And there was an article more recently that even some of the companies producing this technology are starting to acknowledge that it won't stop shootings that are kind of pivoting to other reasons they still still want to sell. I guess I was reading a couple of research papers that Facebook published maybe about two years ago on their technology, how they were having, they were wondering about like open sourcing it. And particularly this model that helps them to detect depression, anxiety, and you know the behavioral expressions of people that are borderline suicidal and may likely have an attempt. But the problem with this was that like that could be used for nefarious purposes and like it also like stated, like it'll calcify somebody's mental health record through like the years to come. And so I think the follow up question that I had been reading that paper and tying it into this is like how do you identify somebody that has a severe track record of mental health or psychological problems and you can use facial data, like you tie it with a track record and what if they don't I do want to highlight that mass shooters are not correlated with mental health issues, they are correlated with domestic violence though and kind of being domestic abusers. I think it's, it's also kind of on this question of identifying mental health issues. It's very important though to recognize like what are the resources we can offer people I think in many cases, people with mental health issues who are not getting treatment because they can't afford it because there are not enough available affordable therapist and it's kind of often I think more structural and systemic issues as opposed to I don't think that identification is like the kind of the primary pain point there that needs to needs to be solved, but that is it. Yeah, good, good question. Yeah, and that work does raise a lot of ethical issues to discuss. Actually, let's do John over on that side and then Aaron and then I will continue. In the military case, we talked about military industrial complex, is it possible to talk about the tech industrial complex in this case, meaning everything in the tech industry comes up with, while the government use it, even if it's not always efficient to stop machining, no one knows. Yeah. Are you getting at how a lot of this technology is produced by private companies and marketed or sold to government agencies or to the military. Yeah, and I think it results in kind of a very opaque system and often a lack of accountability and I think, I think many people feel more comfortable with, you know, I've seen people that are, you know, very concerned about surveillance in China because they see it as, you know, there it's the government that's kind of developing and deploying the technologies whereas in the US it's private corporations, but which are kind of have all sorts of financial partnerships and deals with the government and so that does give it a kind of different character in some ways, but still raises a lot of issues and even raises, I think, kind of different issues often around the lack of transparency and lack of accountability. And then one more example I wanted to give was San Diego was using, law enforcement in San Diego compiled over 65,000 face scans during a seven year long project that was just finally ended of facial recognition. It's almost completely unclear how effective the initiative was with one spokesperson saying they're unaware of a single arrest or prosecution that stemmed from the program. And so that's just kind of a remarkable amount of data to collect with, without even having kind of any evidence that it was accomplishing its, its purported goals. Another case study comes from Amazon's Ring doorbell, his background Amazon. The Ring has partnerships with over 800 police departments in many cities. This is subsidized with taxpayer money. Amazon can stream 911 calls in real time, and Amazon requires police departments to read pre approved scripts when they talk about the program. So I think this is kind of a remarkable amount of control for a private corporation to be having on something that is, you know, ostensibly kind of like a public service. However, NBC News did an investigation which came out just last week. They talked to officials at over 40 different police departments in the US, which all of which have been using Ring for at least six months, and found kind of no evidence that Ring was effective at fighting crimes. And this was a mix of many, many cities said that they don't actually track anything related to telling if it was effective. There are a lot of examples, though, of people saying, well, like, we've probably caught someone but I can't think of anything. One interesting quote was from, I think, from an official in Houston saying that evidence isn't their limiting factor. He said we already have kind of more evidence than we're able to investigate or act on and so kind of gathering more potential evidence doesn't doesn't necessarily help us in any way. I mean, they also, many police complain they're just getting sent like videos of raccoons in people's yards and kind of irrelevant. But this is, I think, for kind of such a massive program, the fact that there's really kind of no evidence around it. Amazon does cite statistics, that they came up with themselves on how they reduce crime, but they would not share any details or data on how they calculated that. Yes? How do these programs get started? I've dealt with government RFPs before. It's like a whole thing. How are there so many of these pilot programs? And I guess on the flip side, are they not ending because people have just forgotten about them? I don't know. I mean, I think many of the decisions, I mean, I'm sure there's some of that, but I think in many cases, the decisions are not necessarily evidence based or even particularly rational. I think technology can seem promising to people. Ring is something where I, and this issue in general is something where I feel like I know a lot of people that I really disagree with. There are people I know and agree with on other issues and we talk about Ring and they're like, that sounds great. I want one. So I think some of it's also kind of the appeal. I mean, in general, I do think people largely operate on stories. You know, we kind of all find stories very compelling and so you can certainly kind of bring up a few individual stories of, you know, here's someone that was captured. You don't have, you know, the comparison of would this person been arrested otherwise? How did this affect, you know, overall arrest rates? But I do think we kind of operate on stories and kind of what sounds interesting or exciting. Yeah, that's a good question. And then actually, we're past time for a break. So let's take a break now and meet back here in seven minutes. Okay, let's get started again. And then one other point that came up after the break started is that the business model for many of these companies is cloud storage. And so the companies are often thinking about kind of long term profits and are willing to sell at a discount or give a free year initially, which can kind of get departments hooked. So kind of in conclusion, just about understanding kind of how government uses technology. It's opaque. They're often very restrictive NDAs. It's typically not evidence based. And then I mentioned kind of some of these issues of often people in government may not have the technical know how to evaluate the claims that they're hearing from tech companies. Tech companies are often out of touch with kind of what the actual needs and processes of how government operates. There's also an issue I think most kind of governments still require a waterfall approach to software development because they're really kind of thinking about these, you know, projects as a whole, whereas the tech industry is much more in an agile approach. So there's really just a lot of disconnect. I've heard people from the US Digital Service talk about this, that, yeah, there's a lot that could use improvement, significant, significant structural improvement. And then the, you know, the process sometimes of bidding on contracts is kind of so complex and specific that like the major tech companies have giant departments that are just focused on the kind of how you bid, but not necessarily on some of the more substantive parts of the technology. So then I did want to acknowledge, and someone pointed this out about my syllabus, several people pointed this out about my syllabus even kind of prior to the class starting when I was showing it to friends, that I realized my viewpoint is very clear here and that I'm not necessarily representing the other side. I did ask on Twitter looking for kind of what were the most thoughtful, well-reasoned arguments kind of in favor, and I got a lot of discussion on that one. And something that I thought was interesting is someone kind of made the meta point, you know, the responses to this threat are interesting because for many of the examples, I've seen those used as reasons why facial recognition is bad, that you kind of had people using the same examples and arguing that this is a reason for it and this is a reason against it. So that was interesting. And I will try to highlight what I think some of the differences in underlying values are. But I think my hope with talking about kind of the rest of this lesson is that even for kind of proponents of facial recognition, I hope that you are aware of what the risks are and are kind of weighing those. But just the main arguments I've heard for these technologies are public safety, the idea of catching criminals, and convenience and efficiency. So now I want to talk about what are the risks, why does this worry me, and many others. So I'm going to kind of talk about some worst case scenarios. So in Western China, the Uyghurs, which are a Muslim minority, over 2 million or up to 2 million people have been placed in internment camps. And this has been facilitated through a lot of biometric data including scans of people's faces, audio of their voices, blood samples, huge, huge amounts of data have been gathered. This is an article from Wired that I think one of the best articles I've read on the issue because they interviewed a number of Uyghur refugees in Turkey. So there are a number, so disproportionately it's men being put into the internment camps and so there are a number of wives that have escaped. But they kind of talked about, you know, having to be required to go to the police station and like make faces to show different emotions. One woman had a deeper voice and they said they didn't believe her initially when they were taking the audio samples and kind of kept making her repeat them. But they've kind of very invasively gathered quite a lot of data. And this is terrible. There's an article on the engine room called Dangerous Data, the Role of Data Collection in Genocide that covers several, several examples. And so one is the work of IBM with the Nazis during kind of World War II and during the Holocaust. So IBM's computers were used to track people, kind of whether they were Jewish, if they were Gypsies, how they were executed. And this is something to keep in mind is that it was less where, you know, now where someone can just buy a computer and then go use it for several years on their own. These machines required a lot of maintenance and so people were having to go service them regularly and kind of a much closer relationship to maintain the computers than you would need kind of with a modern computer. As Swiss judge ruled, it does not thus seem unreasonable to deduce that IBM's technical assistance facilitated the task of the Nazis and the commission of their crimes against humanity. Acts also involving accountancy and classification by IBM machines and utilized in the concentration camps themselves. And this picture is of Adolf Hitler meeting with Tom Watson Sr., the CEO of IBM. This was in 1937. IBM did later break ties with the Nazis, but it was most people say kind of far too late and many, many companies kind of broke ties far, far sooner than IBM. So this is kind of just thinking about kind of the worst case scenarios of what data collection can facilitate, particularly kind of gathering sensitive, sensitive data. And there is an example from France where someone broke the punch card machine so that the column recording if someone was Jewish no longer functioned and fewer people were killed in that region. So that's kind of like an example of how, or evidence that this was playing a role. IBM also helped. I mean this was in early 30s. Germany did a much, much more detailed census than it had done previously and was able to identify kind of far more Jewish people than they had previously recognized as living in Germany. So those are kind of some worst case scenarios. So concerns about the risk with surveillance are it kind of disproportionately harms already marginalized groups, data collected for one reason will be used for other reasons later. There are errors in data. Internal threats are often the biggest threats and it stops dissent which is crucial for social progress. So kind of already shared this pattern earlier from the Alvaro Bedoya reading about how we watch those who are considered quote less than. A really concerning article was about, so India has implemented a much more comprehensive biometric system and a number of HIV patients have stopped their treatment of antiretrovirals because they are scared that they will be outed as HIV positive. Some of them are gay or sex workers and are worried about being outed for those characteristics. And so yeah, this article kind of had interviews with a number of people that were successfully receiving treatment and then stopped it with the implementation of having to give more biometric data. And this I think kind of came up in the earlier, not this example in particular, but kind of this issue of how already marginalized groups may stay away from a place when they feel like they're being tracked. And then police are illegally giving vehicle location data to ICE, documents suggest, and using kind of massive license plate scanning schemes to share data. So these are kind of already marginalized groups being further harmed. It's an article, trading privacy for survival is another tax on the poor that covers, well as I say, they were talking about in the US, but this is a global pattern I think. Public benefits programs ask applicants extremely detailed and personal questions and sometimes mandate home visits, drug tests, fingerprinting, and collection of biometric info. Prisons across the US are quietly building databases of incarcerated people's voice prints in many states for prisoners to be allowed to make phone calls. They have to consent to giving their voice data. And voice is another form of biometric data, which I use the word consent, that is not meaningful consent when someone is in prison and this is their only option to make a phone call. And then in refugee camps, eye scans are being used, iris scans, it's often being linked to people's kind of food benefits. And so this is something, and I chose this from a paper, this has been covered by other, I found news articles, but they often framed it as, oh, like isn't this so convenient, iris scans are being used to help get these refugees their food. But it's important to notice, again, I don't think there's a meaningful notion of consent for if this is kind of a refugees only option to receive food. This is also another situation where it's a private company that has quote, volunteered, that they are offering the service for free, but their kind of payoff is that they're getting to collect all these iris scans. And so I think these are really disturbing examples. And then this I learned about from a Frank Pascual article. I definitely recommend following Frank Pascual. He does a lot of great work. He's the author of the Black Box Society. But he highlighted that at least one Indian fintech app uses social media to predict people's credit rating, and it reduces the score of people who are engaged in political activity. That's kind of being a bigger credit risk. So that's very concerning. And then here are some stats that were highlighted by Tawana Petty of the Detroit Community Tech Project. And so she's been organizing in Detroit against the use of facial recognition there, which is very pervasive, and I'll say a little bit more about that in a moment. But she pointed out the demographics of facial recognition bans. So the first table is cities that have banned facial recognition, and I have just included white and black for kind of simplicity. And then in Detroit at the bottom, there is no ban despite kind of very extensive organizing. Detroit is 79% black, which is much higher than these other cities. And so this is kind of an example showing even how much harder it can be to to regulate the use of these technologies for black people, specifically more broadly for groups that are already marginalized. So this is a chart about Detroit's, it's called Project Green Light. And so it puts these cameras with green lights outside of businesses. And it is, check, it's already at 256 locations. Oh no, it's expected to expand to over 480 locations by the end of 2019. This graphic is kind of I think from last summer. And I've highlighted an article by Chris Gilliard, who talked about the difference between quote luxury surveillance and forced surveillance. And so, you know, an example of luxury surveillance, we're talking about Strava before kind of an Apple Watch or something that kind of people are using as a luxury item. They, you know, like the data that they're getting versus kind of forced surveillance here. A lot of people complain they have these bright green lights. In some cases, this is, you know, can be shining, you know, across your street into your home. It's really disruptive and kind of ugly. And he kind of talks about some of the contrast between forced surveillance and luxury surveillance. Something else about Project Green Light is when it was first rolled out, they tested it on and they had like the small group of maybe 10 businesses initially. And what they did is they prioritized police calls from those businesses. And so then they said, wow, like this really reduced crime having the surveillance cameras. But actually it was they had prioritized this very small number of businesses. So I mean, of course, that had a kind of positive impact. So it was a very misleading, misleading statistic that's been used to to justify kind of the further rollout. Another reason that massive data collection and surveillance and biometric data collection worry me is that data collected for one purpose will be used for others. So it was only confirmed in 2007 that the U.S. Census Bureau gave up the names of Japanese Americans in World War Two so that they could be placed in internment camps. And this is something that at the time that the census was taken in 1940, it was illegal for the data to be shared this way. And there were guarantees of it will not be shared this way. It's illegal. And then they changed the law and then also hid that they did it. Although many people suspected were like, this must be what happened. And so that's something that's difficult to kind of once data is collected, you don't necessarily know how the laws are going to change in the future, how it will be used differently. Tim Wu wrote a good article about this opinion piece in The New York Times, and he says, one hard truth is that data and surveillance networks created for one purpose can and will be used for others. You must assume that any personal data that Facebook or Android keeps are data that governments around the world will try to get or that thieves will try to steal. So also this idea of even if you did have a lot of trust in the company or group gathering the data that others will try to get that data and that they will use it for for other purposes. I am curious about the census question in particular, because just from the reading that I've done, or like the circles that I wander in, I hear that census data is really important for folks to get the right benefits for federal funding to be allocated in ways that make sense for especially marginalized. Yes. So I'm wondering how you think about that balance. Yeah, this is something I don't feel like I have a satisfying answer to. There is a tension that yes, that data is really important for distributing resources and for people being counted. Another example. So I should also say there were identification cards that were used in the Rwandan genocide. And so it is considered kind of a best practice to not record people's ethnic data or something that could be sensitive. But in the case of the Rohingya refugees in Bangladesh, they are actually demanding cards that list that the Rohingya because Myanmar has really tried to erase them as a people. And so that kind of there's another study on how kind of these a number of countries have biometric kind of identification projects in the works. But that was something that came up through kind of their study and interview with a lot of people that and I don't have a clear answer on this that, you know, in that case, you know, this is people saying like, you know, like our ethnicity has kind of been denied and erased like we want it recorded. And so I don't have a good answer on this. There are other thoughts or does anyone have a way to balance that tension? All right. I do think it's good to at least kind of be aware that it exists and kind of like that these risks exist. Although you're right, there can be kind of like when resources are being allocated based on that data. Yeah, it's important. This is a big thing kind of like public health data because you collect a lot of data to combat specific things and like I think one thing some people suggested and I kind of agree with is that data should also be able to expire. As I mentioned earlier, and so it does make sense for somebody to have the agency to decide what data is theirs and also what data they're called by, right? But then at what point, like with the Japanese internment camps, if that census data has been allowed to expire in some way, possibly, you know, or not being put on to, it might have prevented something. Yeah, no, I like that idea in general of data expiring because I only see this also with kind of more prosaic uses by tech companies of kind of once they have your data, they have it forever. Thank you. Hey, sorry, I just wanted to talk about the census. Yeah, this is something that the census blog writes a lot about the US census. Yeah. And I think it is pretty interesting. Just, I don't know what, if they've written anything recently, but generally, they have thought a lot longer about the problem of collecting individual data and then aggregating it, anonymizing it, de-identifying it than a lot of tech companies have. Yes. So some of the things that they talk about are pretty interesting and you don't really see that in de-anonymization literature that comes out of tech and some of it is non-automatable or like it makes the data pretty useless in some ways. But I do think there are lots of interesting aspects of it, like for census, if you're just using it for allocation, you don't really need where the people came from, so you could mix up the data intentionally. So it's another form of deleting or expiring the data by just mixing up certain aspects so that you can never get to the truth of certain aspects of the collection. Great. Thank you. Yeah. And I would also highlight that I think the census is, you know, it's a very, in some ways, kind of unique example in that, you know, like, I think for like your average tech company, they probably shouldn't, that they should not be collecting as much data as the census and they have very different, very different needs. And so that in many use cases, I think the question of kind of collecting less data is an important one to be asking, although I do think you can make a stronger case for the census on some of the data. But thank you for the recommendation to the census blog. Well, then another example of data collected for one purpose being used for others. So there are over 10 states that allow undocumented immigrants to obtain driver's licenses. And so this is something where states were, you know, encouraging and saying, you know, it's safe to get a driver's license and we would prefer that you have a driver's license than not. And then ICE agents have been accessing those databases. There is also kind of an article about states that are, who are not sharing their records with ICE. DHS is trying to get other states to get those records for them through data sharing agreements. So typically, you know, many states will kind of share data with each other and so as a way to kind of get around states that are trying to protect their their data from ICE. And this also kind of gets into kind of how data can be passed around that because these other states were, you know, we're not going to say like, hey, I'm trying to get this data for ICE. So it's just like, can we share our driver's license data? Alright, so that's kind of how how data can end up being used for different purposes. Another area of concern is that data contains errors. And so there is a database that primarily LAPD, but some other Southern California law enforcement agencies contribute to and an audit found that 42 babies under the age of one were added as gang members and 28 of those were recorded as having admitted to being gang members. And then kind of even worse, this database, there's really no process in place to correct errors or to update people's records. And so I think the, going back to this idea of expiration dates could also be significant here. It said even for people that have had kind of no contact with police for five years, I think maybe they're supposed to be purged from the database, but instead they were set to not expire for 100 years. And so it's kind of once you're in this database, you're in it, which is really concerning. Another article that I just read this weekend said that LAPD are evaluated, and we're going to talk more about metrics in Lesson 5 and how metrics can encourage gaming, but LAPD are evaluated on 16 metrics daily to measure their productivity. And one of those metrics is how many gang members they've interviewed. And so you can probably guess what this is encouraged, but many, many people, over 20 officers are under investigation for kind of forging these documents about people saying that people had evidence they were gang members or admitted to being gang members when they didn't. So this is kind of a risk of this approach. A study, the FTC did a study of credit reports in 2012 and found that 26% had at least one mistake in their files and 5% had errors that could be devastating. And so this article, how the careless errors of credit reporting agencies are ruining people's lives, was written by a reporter who was going to rent an apartment. And the landlord contacted him afterwards and was like, you failed the background check for having felony firearms convictions. But I'm like surprised because you seemed like this mild mannered. He's a reporter for public radio. Most landlords would not have called and followed up. And so he looked into it and he found the error, but the three credit bureaus are TransUnion and Equifax, Experian. I don't remember which one it was, but one of them. So he called and was like, I've identified this mistake. He talked to someone at the Tennessee courthouse about it. And then he said he had to make over a dozen calls and they're just like, oh, we're investigating your case, but they wouldn't update it until he said, I'm a reporter and I'm writing about this. But that's something that most of us would not be able to do. Many of people, he's also white, many people would not have gotten the benefit of the doubt from the landlord and letting them know what had happened with the background check. And so this can, this can really impact people. That kind of related point along those lines is that surveillance often operates without accountability. And so even if there's, you know, this is supposed to be the process in place, that's often there's no, no accountability about it. And this is, I think, very much relates to power and kind of often it's if it's powerful people surveilling less powerful, that helps explain why there's not accountability. So Amazon, so after the research by Joy Balamwini came out of, you know, Amazon having higher error rate, actually, sorry, Amazon contested that research, even though kind of all major researchers were like, no, this is accurate and reproducible. But they also contested the ACLU study, which misidentified 28 members of Congress with criminal mug shots. I actually don't remember which of those this article is about. But they, Amazon, part of their defense was like, oh, you're not using the software properly. But then it turns out that their only known police client wasn't using the software in the way that Amazon said was proper because that's not what the defaults were and they didn't provide any training. And so this is kind of an example of how how things get used in practice and the way that Amazon was kind of trying to evade accountability. And then we kind of talked about some of these, these article, articles or issues before with, you know, Palantir using, or New Orleans using Palantir's technology and people not even knowing about it. So, of course, there is kind of no insider accountability to how it's being used. Then Zeynep Tufekci has said that internal threats are often the biggest threats. And she said this in context of this article on how two operatives for Saudi Arabia were working at Twitter and looked up very sensitive information about Saudi Arabian dissidents. And many people have said this probably led to the imprisonment and torture of people because they, and this kind of goes back to an earlier comment about, you know, who has access to data and tech companies of people were kind of really able to look up this data. And this occurred in 2015, but a lot more details have just come out with the FBI has a case, a case around it, but kind of very, very sobering to read that one of these people looked up information on kind of like a thousand people within kind of activists that were using, using Twitter. Another issue, so this was an Associated Press story about how police officers abuse confidential databases. They found incidences of database misuse by police from all 50 states and 36 large police departments. Over two years, there were over 650 cases where an employee or police officer was fired or suspended for database misuse. And they say this is a definite underestimate of the problem because this is just people that got caught and faced repercussions. And two kind of big categories of who was targeted are kind of ex romantic partners who end up being stalked or abused, as well as people that have protested or spoken out against police. And again, kind of police have access to all this sensitive data that they're looking up kind of for their own purposes. And I think, I think this is one of the points where kind of how people weigh the, the internal risk really can change how people feel about these technologies. I think that people that are kind of more focused on, you know, we need to catch criminals or I'm worried about these external risk. Maybe more positive about surveillance technologies, whereas I think kind of putting weight on these internal threats and misuse of data can lead you to be much more cautious and skeptical of these risks or of surveillance technologies. Any thoughts on these? So these are just kind of several attributes that I think it's important to know about surveillance. So kind of how it's disproportionately impacting people that are already marginalized, the way that data can be used for other purposes later. That it often even has contains errors in it, that there's often not accountability about how how it's being used, and that the threats can be internal. And so that, you know, that's something where even if a company or group has, you know, very strong protections against like exterior hackers, if someone internally is misusing it. Well, that's, that's no good. So then I'm going to respond to a few things I commonly hear and disagree with. And so the first is, don't people know what they're getting into when they use these services? And I hear this a lot, even from people will be like, yeah, this is bad, but you know, people knew what they were doing. And Lindsay, Lindsay Barrett wrote a great article, our collective privacy problem is not your fault. But a few things to keep in mind. So we talked last week how it would take the average American 40 minutes a day to read every privacy policy they saw. We know that people don't have that much time. Also, these privacy policies are typically written at a college reading level, whereas the average American is at an eighth grade reading level. Companies rely on manipulative design tricks to get you to spend more time and more money and share more data. So the design is often really pushing you in a certain, certain direction. Facebook keeps shadow profiles of people who don't use it. So even if you don't use Facebook and never have, they can still have a profile on you. Many employers require the use of certain technologies. And so for many people kind of saying, I'm not going to use Gmail anymore, may genuinely not be an option because their employer may use it or require it. And then kind of more practically, in many cases, people would need to be opting out of modern life. And that could really impact their time, their ability to interact with various social circles and groups. And so these are, and then also there's no granular control. This is something about typically terms of service, you know, it's this all or nothing. Do you accept it? Lauren? And can you pass the? This is kind of in the vein of like things that you like would need to opt out of modern life for but it's like you wouldn't even have a choice. Something that always, I always wonder about is how a lot of people seem completely convinced that their devices are listening to them constantly and their advertising is being targeted based on what they're saying out loud to people. Which based on like my experience in the tech industry. I just think that sounds completely implausible the way that things work right now and it's my guess is that it's all just confirmation bias but I was wondering if you have any thoughts on that. Yeah, I mean my understanding is that that is confirmation bias or that I mean so much other data is being collected that that's how these predictions are made. Yeah, I guess I meant more on the subject of like the fact that people, that people think this but they still use their phone. Yeah, well just the whole psychology around being like angry about this thing that you're convinced is happening, but probably isn't better. Was that a hand? Oh, here past the catch box. Yeah, I think it kind of says a lot that we trust a lot of these companies so little that they're like that story like kind of circulates and everybody's like, yeah, I believe it. I think like I mentioned at one point that like when I was doing fieldwork talking to Uber drivers they mentioned that like Uber had suspended a bunch of people that have like met in a physical place to like talk about unionizing back in 2015. And like, I had never met anybody that had actually happened to like who could say that their account was suspended, but like, it was a thing that you just believe, because it was like of course Uber would track where you are off the clock. Of course, like, I just, I accept that as fact. So it's like, it's sort of a sad state that, like, when somebody is like, Oh yeah, Facebook's like listening to our conversations and so that's how it has all these ads and stuff. Yeah, I believe it. But I think, yeah, like Rachel's, Rachel's relaying that like Facebook and all these other companies have said, we don't, we can't do that. I mean, technically, it wouldn't even work out, but like they just collect so much data from so many other avenues that they can piece together things that are uncannily and scarily representing the stuff that we've experienced and said and done. Sure. I also think too, like, when, when people like believe that they can't do anything about being surveilled, it also creates kind of a brain powerlessness. The things that we can actually fight against, we don't. So I think one really cool thing about the CCPA is that there are now abilities for people that reside in California that actually conduct contesting the data being collected. A really interesting story from a friend of mine, she works on the Cypriot border and Cypriot border on a bicameral project, and their employer wanted to have everybody put their fingerprint in so they could like measure cardience. The people on the Greek Cypriot side were able to contest that face-to-face APR, while her co-workers on the Northern Cypriot or Turkish side are not part of the EU, so are not subject or have the rights of GDPR, and their biometric data was collected. So like I think, I don't know, I think about those two things, like the CCPA works because we use it. And we're able to push for that. And other people, you know, in other states don't have that right now. Great, thank you. Now that was a great example too. Yeah, and I will say, and we'll kind of get to this in the final section which we'll cover next week, but that these are really collective problems that need to be addressed collectively. And so I think it's understandable that it can feel incredibly discouraging as an individual because it is something that you don't solve as an individual by opting out, but that we're going to need kind of collective solutions to. We'll talk more about that that next week. But yeah, thank you. Thank you for that example. And let me briefly start on this kind of other common objection that I hear as well. If you've done nothing wrong, you should have, you should have nothing to hide. And I see eyes rolling and people laughing so I think you realize this, but you know some groups face much higher scrutiny, particularly black people, Muslims, queer people, immigrants, and many others. This is supported by research. Black people in California are stopped far often more by police even though they're not found to be breaking the law at higher rates. The perpetual lineup, another study from Georgetown Law Center for Privacy and Technology found that the photos of black people are disproportionately likely to appear in these kind of databases that are being used by law enforcement with no regulation and no oversight. We also see this with kind of arrest rates for, you know, like marijuana possession or these minor offenses where research shows black and white people use marijuana at the same rates, yet the arrest rates are very different. And a point that kind of came up in a few different articles is that with surveillance you can kind of get somebody on anything. If you collect enough, enough data about people, you in most cases will be able to get someone if you're trying, if your goal is to target that person and certain groups face kind of much, much higher risk of this. And then I guess so. Maybe we'll discuss this next time. So this is one of the readings last week that we didn't get to, but I thought it was actually particularly relevant this week of this idea of do artifacts or particular technologies kind of lend themselves towards certain uses in certain kind of directions of power flow. And just in my final minute, kind of an example that I wanted to share this week. So on Twitter, Joe Redmond, who's a computer vision researcher who wrote the YOLO, You Only Look Once paper, kind of really widely used and popular work said, I stopped doing computer vision research because I saw the impact my work was having. I love the work with the military applications and privacy concerns eventually became impossible to ignore. And so this is kind of really significant and here Joe's identifying that computer vision seems to be kind of overwhelmingly used in this way. And that was cause for him to stop. There's a lot of discussion and debate about this that you can find on Twitter. And then Timnit Gebru, who also has a PhD in computer vision, says that she thinks she is headed towards that direction and has been talking about it constantly. And so that's kind of a very relevant, just happened in the last few days, but here are kind of prominent computer vision researchers kind of weighing does this technology kind of point in that direction. And then we're at eight o'clock. I'm sorry to cut it off. We will pick back up here though and we'll discuss this more next time. I'm also sorry to leave it on a bit of a downer of a class. Next time I will talk more about kind of some what I think the directions towards solutions and trying to address this are.

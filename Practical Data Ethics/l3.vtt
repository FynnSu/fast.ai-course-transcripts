WEBVTT

00:00.000 --> 00:08.000
 Okay, it's six o'clock, thanks for being so punctual everyone, so let's get started.

00:08.000 --> 00:15.600
 I also wanted to highlight that on the forums, I always post after class with links to additional

00:15.600 --> 00:18.800
 papers that came up during class, so be sure to check that out.

00:18.800 --> 00:24.760
 So even though it's on the, so like for fairness, last week after class I kind of added a post

00:24.760 --> 00:29.960
 with even more papers that have been talked about, and I think many of them are interesting.

00:29.960 --> 00:30.960
 Thanks.

00:30.960 --> 00:38.400
 So I wanted to start with talking about this article, so I actually don't watch Game of

00:38.400 --> 00:43.280
 Thrones or The Wire, but I still really enjoyed it as an article, although first I was curious

00:43.280 --> 00:46.280
 about anyone who does watch the shows, whether it resonated.

00:46.280 --> 00:51.280
 I see a hand up here.

00:51.280 --> 01:05.640
 Yes, this is one of my favorite articles so far, and yeah, I think what she, I, I'm not,

01:05.640 --> 01:12.120
 oh, thank you everyone, I'm glad it resonated with many, and I liked her, her kind of tie

01:12.120 --> 01:20.000
 in with the tech industry on how so many of our narratives in the tech industry are around

01:20.000 --> 01:25.280
 these kind of, you know, seemingly larger than life personalities and not as much about

01:25.280 --> 01:34.000
 necessarily kind of the broader sociological forces that, that influence people, and she

01:34.000 --> 01:40.040
 lists some of the sociological influences are business models, the advances in technology,

01:40.040 --> 01:47.600
 the political environment, lack of meaningful regulation, wealth inequality, lack of accountability,

01:47.600 --> 01:53.840
 geopolitical dynamics, although I do, you know, there is something I think human about

01:53.840 --> 01:57.640
 liking stories about people, which can then make communication hard.

01:57.640 --> 02:03.920
 I actually heard people talking this, this weekend about kind of the issue with, even

02:03.920 --> 02:08.480
 in science, like a lot of kind of getting attention to your science is about crafting

02:08.480 --> 02:15.160
 it into a good story, but that isn't necessarily kind of the most accurate way of doing science,

02:15.160 --> 02:20.860
 but that kind of stories resonate with people, although this is, I think, a kind of neat

02:20.860 --> 02:26.560
 perspective on how you can make stories more about kind of broader forces.

02:26.560 --> 02:33.360
 The other final, final thoughts, and this, we'll kind of return to this way of thinking

02:33.360 --> 02:37.320
 a bit in week five when we talk about our kind of broader ecosystem that we're in.

02:37.320 --> 02:48.140
 I also, I liked the line about well-run societies not needing heroes.

02:48.140 --> 02:52.440
 So this evening I'm going to be drawing very heavily on resources from the Markula Center

02:52.440 --> 02:58.760
 for Applied Ethics at Santa Clara University, and this is work done by Shannon Valor, Rina

02:58.760 --> 03:03.800
 Rekou, and Brian Green, and I definitely recommend their website as kind of having lots of articles

03:03.800 --> 03:08.480
 and resources, which included several on the syllabus, so you've probably noticed.

03:08.480 --> 03:14.160
 And so first I want to kind of talk about, so you know in weeks one and two we were looking

03:14.160 --> 03:20.160
 at these very kind of specific areas of disinformation and bias and fairness, and now I want to kind

03:20.160 --> 03:24.600
 of step back a little bit and talk about kind of like the underlying kind of what's the

03:24.600 --> 03:32.180
 foundation for even asking ethical questions, and so there were kind of, this is the article

03:32.180 --> 03:38.720
 by Shannon Valor, there were three different kind of schools of ethics that she shared,

03:38.720 --> 03:41.440
 and these kind of go back quite a ways.

03:41.440 --> 03:47.480
 So one is deontological ethics, which focuses on rights, principles, and duties.

03:47.480 --> 03:53.780
 These principles can include autonomy, dignity, justice, fairness, transparency, consistency,

03:53.780 --> 03:54.780
 and more.

03:54.780 --> 04:02.920
 They may also conflict with one another, so in think of situations where, I don't know,

04:02.920 --> 04:07.880
 like consistency conflicts with justice or with these other principles.

04:07.880 --> 04:15.880
 A few examples are the golden rule, not treating people as ends, or sorry, not treating people

04:15.880 --> 04:20.160
 as means to an end, but considering them ends to themselves.

04:20.160 --> 04:25.600
 The rights approach, which is when considering options kind of what best respects everyone's

04:25.600 --> 04:35.760
 rights, or the justice approach, which option best represents people equally or proportionally.

04:35.760 --> 04:40.300
 And so the reading gave some different questions that you can ask, you know, what rights of

04:40.300 --> 04:43.800
 others and duties to others must we respect?

04:43.800 --> 04:48.160
 How might the dignity and autonomy of each stakeholder be impacted?

04:48.160 --> 04:52.240
 What considerations of trust and of justice are relevant?

04:52.240 --> 04:56.600
 Does this project involve any conflicting moral duties or conflicting stakeholder rights?

04:56.600 --> 04:58.280
 How do we prioritize these?

04:58.280 --> 05:05.120
 And so then I was gonna kind of think about a specific example that maybe we can kind

05:05.120 --> 05:06.120
 of consider.

05:06.120 --> 05:10.480
 So do you remember the Unroll Me backlash in 2017?

05:10.480 --> 05:17.600
 So Unroll Me is a service that would, it said, you know, if you sign up for us, we'll unsubscribe

05:17.600 --> 05:21.640
 you from all the kind of annoying email list you're on, and it would give you these summaries

05:21.640 --> 05:26.200
 of like, hey, it seems like you're on these email lists, do you want to unsubscribe?

05:26.200 --> 05:30.240
 And what people didn't realize is that, so it had access to your email inbox, and it

05:30.240 --> 05:33.080
 was then selling that data.

05:33.080 --> 05:39.720
 And this came out in a kind of roundabout way when Uber mentioned, oh yeah, like we

05:39.720 --> 05:45.440
 know how Lyft is doing because we buy that data from Unroll Me, I mean, I think it actually

05:45.440 --> 05:48.880
 went through kind of like a company with another name.

05:48.880 --> 05:52.840
 And so there was a lot of backlash, particularly because it was, I think, kind of catching

05:52.840 --> 05:59.280
 the Uber backlash as well of, oh my goodness, they're buying email data about how many people

05:59.280 --> 06:02.760
 are using Lyft.

06:02.760 --> 06:11.480
 So there were articles on the Unroll Me CEO being heartbroken about their data being sold.

06:11.480 --> 06:16.160
 And so first I kind of want to pause and ask, how would you kind of look at the story in

06:16.160 --> 06:26.520
 light of the questions we saw on the previous page, and kind of how this was impacting people?

06:26.520 --> 06:37.640
 Any thoughts?

06:37.640 --> 06:45.960
 And in the fourth row, if you can pass the catch box back.

06:45.960 --> 06:51.000
 Well I don't, I haven't thought about this enough to like know that I fully get behind

06:51.000 --> 06:57.560
 it, but one thing that like that prompts is, do people have like a right to not have their

06:57.560 --> 07:00.760
 data sold without their knowledge?

07:00.760 --> 07:01.760
 Right.

07:01.760 --> 07:08.000
 Yeah, so there's, I think, a sense of privacy, or maybe this would go under dignity a bit

07:08.000 --> 07:11.440
 of, you know, is there something kind of, I don't know, that interferes with people's

07:11.440 --> 07:12.440
 dignity?

07:12.440 --> 07:17.160
 Actually, let me look at the list of, list of principles.

07:17.160 --> 07:18.160
 Yeah.

07:18.160 --> 07:21.560
 So they're questions of kind of, is that violating some sort of right?

07:21.560 --> 07:22.560
 Right.

07:22.560 --> 07:26.720
 Now a hand in the first, oh, actually there's a hand, two over, yes.

07:26.720 --> 07:33.520
 Well, in this example, I mean, it's particularly ironic because the purpose of the users was

07:33.520 --> 07:39.040
 to extract themselves from being involved with all of these service providers and to

07:39.040 --> 07:40.040
 turn around.

07:40.040 --> 07:45.640
 So I think it's, it's like insult to injury, so I think it's even worse, just kind of like

07:45.640 --> 07:52.400
 as a, at a high level of morals, it's like just kind of sleazy, because I think that

07:52.400 --> 07:56.800
 they're, they were taking in more trust than maybe another service provider who had that

07:56.800 --> 08:01.680
 same information, whether or not they had consent to sell it, it seems to be another

08:01.680 --> 08:02.680
 level.

08:02.680 --> 08:03.680
 Yeah.

08:03.680 --> 08:07.600
 So it seems to kind of like violate, maybe this is consistency around, people seem to

08:07.600 --> 08:13.340
 value, you know, like simplicity, they're not wanting to be contacted by all these providers,

08:13.340 --> 08:17.920
 and then it's, as you said, kind of even more insulting that their data is being sold.

08:17.920 --> 08:18.920
 Right.

08:18.920 --> 08:25.440
 Lauren, on the opposite end of the row.

08:25.440 --> 08:31.760
 I think from a business perspective, or like when these founders, you know, sit down, everyone

08:31.760 --> 08:38.120
 gets in a room to brainstorm possible revenue generation, like streams of revenue, I think

08:38.120 --> 08:42.680
 it would have been a game changer if they didn't consider selling data.

08:42.680 --> 08:46.600
 And so at what point should these questions be introduced, and like how much weight should

08:46.600 --> 08:51.280
 they be given, because it could drastically change the potential revenue streams of a

08:51.280 --> 08:56.620
 company and their ability to even stay profitable, or even like on that path towards profitability.

08:56.620 --> 08:59.200
 So really good question for business owners.

08:59.200 --> 09:00.200
 That's true, yes.

09:00.200 --> 09:05.320
 And this was, Unroll Me was a free service, free service to the users, so they were not

09:05.320 --> 09:09.060
 paying, but yeah, they were, turns out, giving their data.

09:09.060 --> 09:16.780
 And then can you pass the catch box forward to the front row?

09:16.780 --> 09:21.480
 The specific use of the term heartbroken sort of speaks to this lack of understanding, like,

09:21.480 --> 09:25.840
 oh, this is more than just me, it sort of speaks to that cult of personality type of

09:25.840 --> 09:29.680
 person, versus like, oh, well, I'm very disappointed that, you know, these like ethical principles

09:29.680 --> 09:34.520
 that like perhaps were maybe thought of at some board meeting weren't like rolled out,

09:34.520 --> 09:39.040
 but rather it's like, oh, I'm heartbroken, personal betrayal, versus like the umpteen

09:39.040 --> 09:42.440
 people who have their data like auctioned off.

09:42.440 --> 09:45.720
 I feel like it's a bit strange.

09:45.720 --> 09:50.240
 Yeah, and there was actually, I didn't take the headline from this, but it was like a

09:50.240 --> 09:54.540
 friend of the co-founder, or someone who had formerly been involved in the company wrote

09:54.540 --> 09:59.780
 this medium post, kind of justifying why the company, and they weren't even like involved

09:59.780 --> 10:04.640
 with the company anymore, but kind of very defensive and I think took it very personally.

10:04.640 --> 10:11.200
 I guess this is also about like transparency, because a lot of these are like free and then

10:11.200 --> 10:18.000
 people see free software and they get on it and they use it without knowing that the software

10:18.000 --> 10:23.000
 provider has all the rights to your data as long as you get onto the platform.

10:23.000 --> 10:31.280
 Yeah, no, exactly, I think transparency is definitely a big issue here.

10:31.280 --> 10:36.240
 And so, um, actually something that didn't come up, but the company's defense was, you

10:36.240 --> 10:39.800
 know, if you really read our terms of service, you should have known that you were giving

10:39.800 --> 10:45.320
 away all your data, and so there was a study, and this is from 2008, that it would take

10:45.320 --> 10:51.720
 the average American 40 minutes a day to read every privacy policy that they encountered.

10:51.720 --> 10:55.920
 And so this is something, it's just kind of absurd, we don't all have an extra 40 minutes

10:55.920 --> 11:00.680
 a day, and if we did, we probably wouldn't want to read privacy policies with that.

11:00.680 --> 11:06.640
 And it might be even more now, because that was, that was over 10 years ago.

11:06.640 --> 11:13.460
 And so Casey Fiesler, who I quote a lot, did a study and found that the, and so this was

11:13.460 --> 11:19.920
 for terms of service, the average reading level was 14.8, which is kind of midway through

11:19.920 --> 11:29.040
 college for a terms of service policy, and has 3,800 words, which is significant, yet

11:29.040 --> 11:33.160
 the average person in the US is at an eighth grade reading level.

11:33.160 --> 11:37.600
 So in many cases, these aren't even, you know, pitched at an appropriate reading level or

11:37.600 --> 11:42.920
 intended to be kind of readable by the, by the users.

11:42.920 --> 11:50.680
 And I think this, this also kind of captures the gap between legality and ethics, you know,

11:50.680 --> 11:54.200
 there's something where I think, you know, companies can use it of like, oh no, like

11:54.200 --> 12:01.120
 it's, it's fine because they signed this, but these aren't intended to be read.

12:01.120 --> 12:09.020
 Hi, I'm Rachel Thomas, I'm going to talk about some of the foundations for ethics as well

12:09.020 --> 12:14.800
 as ethical tools you can use in your workplace, particularly in the tech industry.

12:14.800 --> 12:19.680
 And this is a continuation from the lecture I started yesterday, unfortunately the recording

12:19.680 --> 12:24.960
 software malfunctioned partway through, through class, so I'll be picking up here, although

12:24.960 --> 12:30.360
 it's fine, fine to start with this video, I'll kind of briefly, briefly review.

12:30.360 --> 12:35.140
 I'll be drawing very heavily on resources from the Markkula Center for Applied Ethics

12:35.140 --> 12:36.880
 at Santa Clara University.

12:36.880 --> 12:42.120
 They've done some great work on ethics and technology practice, definitely check out

12:42.120 --> 12:47.040
 their website, they have conceptual frameworks, kind of guides for decision-making, an ethics

12:47.040 --> 12:52.600
 toolkit that I'll be covering later, case studies, so, so look at that, it's very, very

12:52.600 --> 12:56.200
 helpful collection of resources.

12:56.200 --> 13:01.480
 And so as we talked about previously, we're kind of looking at three, three different

13:01.480 --> 13:04.040
 ethical theories tonight.

13:04.040 --> 13:09.140
 One is deontological ethics, which focuses on rights, principles, and duties.

13:09.140 --> 13:15.440
 These principles can include autonomy, dignity, justice, fairness, transparency, consistency,

13:15.440 --> 13:16.440
 and more.

13:16.440 --> 13:20.320
 They may sometimes conflict with one another.

13:20.320 --> 13:28.860
 We talked earlier about the example of Unroll Me, which was a company that offered a service

13:28.860 --> 13:33.760
 to people for free, where they would unsubscribe you from email lists, so kind of go through

13:33.760 --> 13:39.400
 your inbox and see like, hey, you're subscribed to all these email lists or advertisements,

13:39.400 --> 13:45.040
 do you want us to automatically unroll you, unsubscribe you, and then it eventually came

13:45.040 --> 13:51.080
 out in 2017 that their business model was selling data from users' email, and this was

13:51.080 --> 13:55.420
 revealed when an Uber executive mentioned that they had been buying data from Unroll

13:55.420 --> 14:00.680
 Me about how many people were using Lyft, and many, many users felt very violated by

14:00.680 --> 14:01.680
 this.

14:01.680 --> 14:07.440
 And while this was something that may have been legal, many people felt that it was unethical,

14:07.440 --> 14:14.720
 and from a deontological kind of viewpoint, it would seem to violate principles perhaps

14:14.720 --> 14:22.640
 of dignity, autonomy, transparency, so that's kind of one, one example of how you can look

14:22.640 --> 14:23.640
 at this.

14:23.640 --> 14:29.600
 Deontological ethics also include the rights approach, which option best respects the rights

14:29.600 --> 14:34.800
 of everyone who has a stake, or the justice approach, which option treats people equally

14:34.800 --> 14:39.480
 or proportionally.

14:39.480 --> 14:43.060
 Another kind of school of ethics is consequentialist ethics.

14:43.060 --> 14:47.040
 If you want to know if an action is ethical, look at its consequences.

14:47.040 --> 14:51.720
 This includes utilitarianism, which asks, you know, which option will produce the most

14:51.720 --> 14:56.980
 good and do the least harm, as well as the common good approach, which asked which option

14:56.980 --> 15:02.420
 best serves the community as a whole, not just some members.

15:02.420 --> 15:09.320
 And I yesterday in class gave the students an exercise to, and this is modified from

15:09.320 --> 15:17.000
 Casey Fiesler's Tech Ethics Scavenger Hunt, but to try ranking five different kind of

15:17.000 --> 15:24.460
 ethics scandals from how they thought a utilitarian would see them as what is worst to least bad,

15:24.460 --> 15:29.140
 and how a deontologist would see them, again, from worst to least bad.

15:29.140 --> 15:33.600
 We had a really interesting discussion on that, I think most of which is captured.

15:33.600 --> 15:36.240
 In the previous video, this is something you can try.

15:36.240 --> 15:41.560
 It's a little bit absurdist in that ethics is not really about ranking kind of the terribleness

15:41.560 --> 15:47.160
 of things, but it was useful for highlighting some of the differences between utilitarianism

15:47.160 --> 15:54.220
 and deontology, and also just kind of how these philosophies would lead people to think

15:54.220 --> 15:58.480
 about problems.

15:58.480 --> 16:04.940
 So while consequentialism and deontological ethics both focus on actions, virtue ethics

16:04.940 --> 16:11.440
 focuses on kind of the person and their character traits, so this is a third school of thought.

16:11.440 --> 16:16.360
 It highlights the need for people with well habituated virtues of moral character and

16:16.360 --> 16:23.440
 well cultivated, practically wise moral judgments, and it's kind of about this lifelong process

16:23.440 --> 16:30.200
 of developing practical wisdom, and kind of the virtue approach asks the question, which

16:30.200 --> 16:36.320
 option leads me to act as the sort of person that I want to be?

16:36.320 --> 16:41.920
 Some questions from the Markkula Center guide on this ask, what design habits are we embodying?

16:41.920 --> 16:46.140
 Are they the habits of excellent designers?

16:46.140 --> 16:50.740
 Will this project weaken any important human habits, skills, or virtues that are central

16:50.740 --> 16:52.840
 to human excellence?

16:52.840 --> 16:54.880
 So will it strengthen any?

16:54.880 --> 17:00.760
 Will this design or project incentivize any vicious habits in users or other stakeholders?

17:00.760 --> 17:06.040
 And how confident are we that we'll feel proud to have our names associated with this project

17:06.040 --> 17:08.320
 in the future?

17:08.320 --> 17:14.360
 And so these are a few questions you can think about kind of asking around work you're doing.

17:14.360 --> 17:21.160
 So there was an optional reading called What Would an Avenger Do by Mark D. White, and

17:21.160 --> 17:26.800
 I have to admit I actually am not that familiar with the Avengers, but I still enjoyed reading

17:26.800 --> 17:28.920
 this and found it helpful.

17:28.920 --> 17:35.440
 And it kind of characterized the Iron Man as a utilitarian who is looking to maximize

17:35.440 --> 17:36.440
 the good.

17:36.440 --> 17:42.120
 He sometimes is willing to kind of let the ends justify the means in that service.

17:42.120 --> 17:47.720
 Captain America was classified as a deontological ethicist in terms of kind of having a notion

17:47.720 --> 17:50.960
 of the right and really adhering to that.

17:50.960 --> 17:56.600
 And then Thor was seen as an example of virtue ethics and living by a code of honor.

17:56.600 --> 18:04.600
 I did want to note that kind of all three of these ethical philosophies we've talked

18:04.600 --> 18:09.660
 about are kind of Western philosophies and that there are many other ethical lenses out

18:09.660 --> 18:12.760
 there as well as from other cultures.

18:12.760 --> 18:18.280
 And I recently was reading about New Zealand's algorithmic impact assessment project that

18:18.280 --> 18:23.520
 they're working on, and one aspect of the project is that they are trying to incorporate

18:23.520 --> 18:26.600
 a Maori worldview as well.

18:26.600 --> 18:30.680
 The Maori are the indigenous people in New Zealand, and so then I was kind of doing some

18:30.680 --> 18:36.560
 reading on the Maori data sovereignty movement and how the Maori view data, which sometimes

18:36.560 --> 18:44.600
 kind of raises specific concerns about kind of how data is used, what's done with it,

18:44.600 --> 18:47.720
 and how it impacts their community.

18:47.720 --> 18:53.440
 And so I don't, I don't feel sufficiently confident that I could explain this accurately

18:53.440 --> 18:57.560
 and I don't want to misrepresent someone else's culture, I mostly just want to highlight that

18:57.560 --> 19:03.960
 there are plenty of ethical philosophies outside the West and there are kind of other ethical

19:03.960 --> 19:08.320
 worldviews to consider.

19:08.320 --> 19:15.240
 So in summary, the kind of five ethical lenses that we've seen are the rights approach, the

19:15.240 --> 19:23.040
 justice approach, these are both deontological ethics approaches, then from consequentialism

19:23.040 --> 19:28.880
 we've seen the utilitarian approach and the common good approach, and then finally from

19:28.880 --> 19:32.980
 virtue ethics the virtue approach, and so this is a list of questions that the Markula

19:32.980 --> 19:39.320
 Center Guide provides that you can consider in kind of looking at a project that might

19:39.320 --> 19:46.160
 be going on in your workplace and trying to answer ethical questions about it and anticipate

19:46.160 --> 19:52.120
 how it'll impact people, which options may be the best options to take.

19:52.120 --> 19:57.000
 Also note that the, I know Irina Raku from Markula Center definitely emphasizes that

19:57.000 --> 20:01.280
 this is something that's best done in a group, it definitely helps to have other people to

20:01.280 --> 20:06.000
 be discussing this in community, have people who can point out kind of different issues

20:06.000 --> 20:09.480
 and maybe see things differently from you.

20:09.480 --> 20:17.800
 All right, so that's kind of in summary just some of kind of what's the underpinnings for

20:17.800 --> 20:23.160
 even talking about ethics or weighing the ethics of different projects.

20:23.160 --> 20:30.480
 Next I want to get into some practical tools that you can use in the workplace that you

20:30.480 --> 20:36.160
 can implement, and as we've talked about earlier having good intentions is not necessarily

20:36.160 --> 20:42.400
 enough to ensure a good outcome, in fact people can have good intentions and still really

20:42.400 --> 20:50.480
 miss major ethical issues.

20:50.480 --> 20:56.040
 So it's helpful, it's helpful to really implement processes and operationalize this in a way

20:56.040 --> 21:01.120
 to kind of make it part of your routine and make it something that the company is doing

21:01.120 --> 21:02.120
 regularly.

21:02.120 --> 21:06.760
 And we're going to go through this ethical toolkit, there are seven tools or practices

21:06.760 --> 21:07.760
 in it.

21:07.760 --> 21:15.320
 The first is ethical risk sweeping, and so this is instituting regularly scheduled risk

21:15.320 --> 21:20.460
 sweeps and so kind of similar to cybersecurity penetration testing, regularly looking for

21:20.460 --> 21:25.440
 risk, no vulnerability found, while that's a good thing that doesn't mean that you stop

21:25.440 --> 21:30.080
 or consider it a waste, you keep doing it.

21:30.080 --> 21:35.280
 And then it's good to assume that you missed some risk initially and so continuing to look.

21:35.280 --> 21:39.440
 It's also important to reward team members for spotting new ethical risk.

21:39.440 --> 21:43.800
 I think sometimes raising an ethical risk can be seen as something that if nothing else

21:43.800 --> 21:48.360
 kind of slows down, slows you down in your speed to get a product to market and that's

21:48.360 --> 21:52.120
 not always rewarded, but it's important to kind of reward this behavior if you want it

21:52.120 --> 21:54.720
 to be incentivized.

21:54.720 --> 22:00.520
 At this point, one student brought up that her friend has been raising ethical issues

22:00.520 --> 22:04.640
 in his workplace and that he's kind of getting a lot of pushback about it.

22:04.640 --> 22:10.760
 People are seeing him as as difficult, it's creating friction, and she asked how to deal

22:10.760 --> 22:12.520
 with this.

22:12.520 --> 22:18.680
 And so I do want to acknowledge that depending on the dynamics of your workplace and in many

22:18.680 --> 22:23.920
 workplaces, I think that it can cost you social capital to speak up.

22:23.920 --> 22:27.360
 People have different amounts of social capital.

22:27.360 --> 22:32.000
 It also can depend on your seniority, how seriously you're taken around this.

22:32.000 --> 22:33.280
 So this can create issues.

22:33.280 --> 22:39.120
 It's not, it's definitely not simple, particularly when your company or your team is not aligned

22:39.120 --> 22:41.960
 on this being an important thing to do.

22:41.960 --> 22:46.160
 So I want to acknowledge that and I think that over time it's possible if you're in

22:46.160 --> 22:51.640
 a workplace where you do experience a lot of friction about bringing up ethical issues,

22:51.640 --> 22:56.400
 that you ultimately may find it like that's not a great fit or is kind of untenable for

22:56.400 --> 22:57.400
 you to continue.

22:57.400 --> 23:01.400
 So I wanted to acknowledge the difficulty of this.

23:01.400 --> 23:05.800
 If you don't kind of have your whole team on board, ideally, you know, you would have

23:05.800 --> 23:10.240
 leadership supporting this and have it where there's a kind of more of a more buy-in around

23:10.240 --> 23:11.240
 the practice.

23:11.240 --> 23:17.760
 At this point, another student brought up that he previously worked at Epic, the makers

23:17.760 --> 23:25.000
 of electronic health records, and he said at Epic they have, I'm forgetting the name,

23:25.000 --> 23:31.520
 but it's a specific job role of people that it's kind of their whole role to investigate

23:31.520 --> 23:35.760
 concerns that potentially relate to patient safety and if something's going to pose a

23:35.760 --> 23:40.880
 patient safety risk, that is something to take very seriously and he said it really

23:40.880 --> 23:41.880
 helped.

23:41.880 --> 23:48.320
 The company can and should raise risk that they think may may impact a patient safety,

23:48.320 --> 23:51.780
 but from there then they kind of have these specialists who take that over, that's their

23:51.780 --> 23:56.840
 job to do so, and that that really helped kind of streamline the process and it was

23:56.840 --> 24:00.960
 also something where it was fine there to raise an issue that ended up not turning out

24:00.960 --> 24:04.440
 to be a risk to patient safety.

24:04.440 --> 24:09.120
 It was better to raise it and investigate and have it turn out to to be alright than

24:09.120 --> 24:13.040
 to not say anything and so that was that was kind of an interesting, interesting personal

24:13.040 --> 24:20.240
 experience to hear from from a member of our class.

24:20.240 --> 24:27.980
 So tool two, tool two is ethical pre-mortems and post-mortems and I had heard of post-mortems

24:27.980 --> 24:33.340
 before and I think they're at least particularly for kind of a technical failures are a well-established

24:33.340 --> 24:36.880
 practice in the tech industry.

24:36.880 --> 24:39.360
 This would be implementing them around ethical failures as well.

24:39.360 --> 24:43.280
 I thought the idea of a pre-mortem was interesting.

24:43.280 --> 24:48.240
 Pre-mortems should ask how could this project fail for ethical reasons, what blind spots

24:48.240 --> 24:55.400
 would lead us into, why would we fail to act, what systems checks or fail safes can we put

24:55.400 --> 25:00.620
 in place to reduce failure or risk, and so I thought these were interesting questions

25:00.620 --> 25:06.780
 and one student in the class raised his hand and shared that he had previously done this

25:06.780 --> 25:12.200
 not even kind of with a an explicit ethical framing but is something that he went through

25:12.200 --> 25:18.840
 with kind of business development in companies and that for many maybe this would kind of

25:18.840 --> 25:23.580
 be an entryway into ethics to get them thinking about it and even if it starts as a business

25:23.580 --> 25:30.160
 prop problem that it's kind of raising, raising ethical risk.

25:30.160 --> 25:36.300
 And so something that reading about this tool reminded me of was the kind of school of thought

25:36.300 --> 25:43.020
 around professors that are using science fiction to teach computer science ethics and so this

25:43.020 --> 25:47.760
 comes from a Wired article that interviewed several different professors and they actually

25:47.760 --> 25:55.920
 had different philosophies on how they incorporate science fiction into their courses so some

25:55.920 --> 26:01.200
 of them are using it more of just a way to kind of think about human characteristics

26:01.200 --> 26:06.400
 and traits to kind of look you know a little ways into the future and see what could go

26:06.400 --> 26:12.020
 wrong some feel like the kind of having it in a different world can make it give students

26:12.020 --> 26:16.480
 a distance that makes it easier to analyze and discuss so it was interesting kind of

26:16.480 --> 26:19.800
 even within the professors that are doing this there are different thoughts on kind

26:19.800 --> 26:23.920
 of what's what's the best approach it was something that I considered doing for this

26:23.920 --> 26:26.840
 course I don't think it's going to happen in this course but it's definitely kind of

26:26.840 --> 26:32.760
 at the back of my mind for for future ones that there was also a nature article where

26:32.760 --> 26:37.120
 they asked I believe six different science fiction writers for their views on kind of

26:37.120 --> 26:42.800
 what science fiction can tell us and can lose said although science fiction isn't much use

26:42.800 --> 26:49.160
 for knowing the future it's underrated as a way of reimagining human humanity in the

26:49.160 --> 26:53.640
 face of ceaseless change so that's that's one aspect of what you can get from science

26:53.640 --> 27:02.080
 fiction Casey Fiesler who I mentioned a lot and deeply admire does a black mirror exercise

27:02.080 --> 27:07.480
 with her students so she actually assigns students to watch an episode of black mirror

27:07.480 --> 27:12.400
 and then does a project called black mirror writer's room I got to participate in this

27:12.400 --> 27:17.760
 at a workshop or she she led a session of it and the idea is to kind of get students

27:17.760 --> 27:24.000
 writing their own episodes of black mirror she says that speculation is a skill that

27:24.000 --> 27:28.880
 we have to practice and develop and that kind of yeah thinking of your own black mirror

27:28.880 --> 27:33.840
 episode is one way to practice this skill of speculation and she thinks it's helpful

27:33.840 --> 27:39.600
 to kind of keep things in the somewhat near future where we can see them as an extension

27:39.600 --> 27:43.880
 of our current technology and she's kind of written about about how she implements this

27:43.880 --> 27:50.240
 in the classroom and so I thought this was this was interesting as a kind of extension

27:50.240 --> 28:07.520
 of the the primordem idea all right tool three tool three is expanding the ethical circle

28:07.520 --> 28:15.760
 so there's several kind of several ways that teams can can fail to to see risk one is group

28:15.760 --> 28:21.360
 think which is when you have a very close-knit group people may start kind of thinking thinking

28:21.360 --> 28:25.860
 similarly due to the dynamics of the group and then as a result may have particular blind

28:25.860 --> 28:31.360
 spots and things that they miss another thing that can occur is the bubble mentality and

28:31.360 --> 28:36.680
 this is when you have a lack of sufficient diversity in your group and again that can

28:36.680 --> 28:42.520
 lead to people that kind of have very similar worldviews and may miss particular risk and

28:42.520 --> 28:49.320
 miss the interest of key stakeholders that aren't represented there's also the Friedman

28:49.320 --> 28:55.540
 fallacy which is a fallacy saying that companies are morally obligated only to maximize shareholder

28:55.540 --> 29:01.700
 profit even if it's very harmful to the public or that stuff that is harmful to the environment

29:01.700 --> 29:07.120
 or elicits public outcry people sometimes try to use this to justify deliberate or reckless

29:07.120 --> 29:13.240
 disregard of legitimate moral interest however it is a fallacy this is not the case and the

29:13.240 --> 29:21.280
 public typically does not respond well to to this kind of reckless behavior so some

29:21.280 --> 29:28.440
 questions you can ask for expanding the ethical circle whose interests desires skills experiences

29:28.440 --> 29:35.080
 and values have we simply assumed rather than actually consulted who are all the stakeholders

29:35.080 --> 29:41.240
 will be who will be directly affected by our product how have their interests been protected

29:41.240 --> 29:47.880
 how do we know what their interests really are have we asked which groups and individuals

29:47.880 --> 29:52.160
 will be indirectly affected and who might use this product that we didn't expect to

29:52.160 --> 29:58.720
 use or for purposes that we didn't initially intend later in the class I asked people to

29:58.720 --> 30:05.520
 kind of reflect on what they thought some of the most useful of the of the tools we

30:05.520 --> 30:10.600
 are and several people highlighted this as something that they thought was crucial crucial

30:10.600 --> 30:17.640
 to do in a particularly important tool for the workplace this made me think of some research

30:17.640 --> 30:24.440
 from the University of Washington tech policy lab by Meg Young et al and so there they did

30:24.440 --> 30:32.120
 a project called diverse voices that is around how to create kind of panels of people from

30:32.120 --> 30:39.080
 different communities I think to kind of gather them in a systematic way pay them for their

30:39.080 --> 30:44.300
 expertise and elicit their feedback around proposed tech policy in this case this could

30:44.300 --> 30:51.280
 also be used though for considering different different products or business practices and

30:51.280 --> 30:56.980
 so they did two case studies they had an augmented reality white paper and they convened an expert

30:56.980 --> 31:02.160
 or several expert panels including with people with disabilities people who are formerly

31:02.160 --> 31:08.600
 are currently incarcerated and women and then they did a separate one on autonomous vehicles

31:08.600 --> 31:15.080
 strategy document holding expert panels with youth with non-car drivers and with people

31:15.080 --> 31:20.440
 with extremely low incomes and so this is great to check out there's an academic paper

31:20.440 --> 31:25.080
 about it and then there's also on the website there's a kind of practical guide kind of

31:25.080 --> 31:29.460
 leading you through how you could do something like this as well so this is a resource I

31:29.460 --> 31:39.520
 wanted wanted you to know about all right tool for case-based analysis so identify similar

31:39.520 --> 31:45.520
 or paradigm cases that mirror the present case identify relevant parallels between or

31:45.520 --> 31:50.160
 differences among all the cases so even if a case is not exactly what you're currently

31:50.160 --> 31:53.760
 working on in the workplace it still may be something you can learn from and it's helpful

31:53.760 --> 31:59.320
 to be explicit about how it does parallel what you're doing how it's different and then

31:59.320 --> 32:05.280
 that can help you identify kind of solutions or risk mitigation strategies and at this

32:05.280 --> 32:11.920
 point one student raised their hand and shared about harvard has started or so you know there's

32:11.920 --> 32:16.680
 the harvard business review that often has business case studies a harvard data science

32:16.680 --> 32:21.080
 review has been started as well and that can be a good source for data science reviews

32:21.080 --> 32:29.320
 although that's just within the last year and is still kind of getting getting up and going

32:29.320 --> 32:34.400
 i kind of been thinking about looking at past cases and this might be going a little bit

32:34.400 --> 32:39.960
 further into the past than the the authors of the toolkit met but i thought about the

32:39.960 --> 32:47.560
 course taught at columbia data past present and future so this is co-taught by matt jones

32:47.560 --> 32:52.360
 who's a history professor and chris wiggins who's an applied math professor as well as

32:52.360 --> 32:56.960
 the chief data scientist for the new york times and i think this is a fantastic idea

32:56.960 --> 33:00.380
 for a course this is i believe they're on their fourth year teaching it so it's really

33:00.380 --> 33:06.360
 been refined we'll be putting a book out about it next year all the materials are available

33:06.360 --> 33:11.400
 online so definitely check them out in this course while it involves coding labs i believe

33:11.400 --> 33:16.640
 it was or i know that it's open to students kind of from across uh humanities as well

33:16.640 --> 33:24.300
 as social sciences and and natural sciences and what they do is kind of go through a history

33:24.300 --> 33:31.360
 of data and particularly how new kind of new discoveries and innovations related to data

33:31.360 --> 33:37.320
 have reconfigured power there's a lot of kind of dark history something i didn't know is

33:37.320 --> 33:43.720
 that regression was first used to do race science so that was kind of part of why regression

33:43.720 --> 33:49.040
 was invented was it was doing race science which is terrible and they kind of go through

33:49.040 --> 33:54.480
 though this history that can really help help us understand kind of our current state and

33:54.480 --> 33:59.160
 also kind of gaining those tools at looking at this reconfiguration of power so definitely

33:59.160 --> 34:09.320
 check out their materials online if this interests you all right tool five remembering the ethical

34:09.320 --> 34:15.160
 benefits of creative work and so i think this comes up because ethics can sometimes seem

34:15.160 --> 34:20.160
 focused on the negatives and you know what terrible things can go can happen what can

34:20.160 --> 34:25.520
 go wrong but to remember that hopefully we're also trying to do to do good with our work

34:25.520 --> 34:31.920
 and to look at the positives as well and to remember you know hopefully we are working

34:31.920 --> 34:38.880
 towards what we see as a greater good are there ways that we can genuinely you know

34:38.880 --> 34:42.640
 trying to help others through our work as opposed to generating kind of inauthentic

34:42.640 --> 34:50.800
 needs or manufactured desires okay tool six is another one that really resonates with

34:50.800 --> 34:59.600
 me and that's think about the terrible people

34:59.600 --> 35:07.560
 so asking who will want to abuse steal misinterpret hack destroy or weaponize what we build so

35:07.560 --> 35:12.120
 this is kind of people are going to use your your products and tools in ways that you really

35:12.120 --> 35:19.920
 didn't anticipate but can you try to start anticipating those who will use use it with

35:19.920 --> 35:27.440
 alarming stupidity or irrationality what rewards incentives openings has our design inadvertently

35:27.440 --> 35:34.080
 created for these people or for those people and how can we remove those rewards and at

35:34.080 --> 35:39.680
 this point one student shared about good hearts law which states that kind of any when the

35:39.680 --> 35:44.040
 measure becomes the target it ceases to be a good measure and that whenever you have

35:44.040 --> 35:49.320
 kind of rewards or incentives people will try to gain that and you'll get unexpected

35:49.320 --> 35:54.840
 consequences we're going to talk more about this in lesson five of this class I wrote

35:54.840 --> 36:01.220
 a blog post this fall called the problem with metrics is a big problem with AI and that

36:01.220 --> 36:09.320
 talks about some of the harms that arise when we overemphasize metrics great and then tool

36:09.320 --> 36:16.840
 seven is closing the loop making making sure you have channels to get feedback and to iterate

36:16.840 --> 36:22.700
 remember that this is never a finished task identify feedback channels that will deliver

36:22.700 --> 36:29.000
 reliable data on ethical impact so if you remember back to lesson one we talked about

36:29.000 --> 36:35.080
 the healthcare algorithm that was healthcare software that was implemented in Arkansas

36:35.080 --> 36:40.880
 to determine people's Medicaid benefits there was a bug in it it cut off care that people

36:40.880 --> 36:46.120
 needed people with cerebral palsy in particular and there was no kind of feedback channel

36:46.120 --> 36:52.400
 in place to even surface this air other than having to go through like a very formal court

36:52.400 --> 36:57.000
 case and so this is something you really want to make sure that you have channels to receive

36:57.000 --> 37:05.660
 face feedback and chains of responsibility as well and so it kind of tools says six and

37:05.660 --> 37:12.000
 seven reminded me of this post by Alex fierce who was previously the chief legal officer

37:12.000 --> 37:18.360
 in media and he interviewed I think I was around 15 people that work in trust and safety

37:18.360 --> 37:22.500
 many of them have been working in trust and safety for years across a range of companies

37:22.500 --> 37:28.640
 including many of the major tech platforms trust and safety includes content moderation

37:28.640 --> 37:36.000
 Alex described trust and safety is both the the judges and the janitors of the internet

37:36.000 --> 37:40.660
 and something that one of the people he interviewed said that struck me was the separation of

37:40.660 --> 37:45.300
 product people and trust people worries me because in a world where product managers

37:45.300 --> 37:49.760
 and engineers and visionaries cared about this stuff it would be baked into how things

37:49.760 --> 37:54.880
 get built if things stay this way the product and engineering are Mozart and everyone else

37:54.880 --> 38:00.280
 is Alfred the butler the big stuff is not going to change and this is true at many companies

38:00.280 --> 38:05.280
 they're often kind of siloed you have product and engineering over here and people dealing

38:05.280 --> 38:11.480
 with trust and safety with abuse that happens on the platform kind of harassment bad actors

38:11.480 --> 38:16.680
 are kind of totally siloed off in another place there's very important feedback that's

38:16.680 --> 38:22.480
 not not getting back to the the people that are building the products and so someone else

38:22.480 --> 38:28.400
 in the article and most of the people in the article were using pseudonyms talked about

38:28.400 --> 38:33.240
 kind of a company where executives were having to spend some time kind of shadowing people

38:33.240 --> 38:37.460
 in trust and safety just to see what sort of abuse arises how are people weaponizing

38:37.460 --> 38:42.080
 the platform which sounded like a promising approach so I thought this was this was an

38:42.080 --> 38:51.900
 interesting one so now there's a topic I want to talk more about that relates to I guess

38:51.900 --> 38:57.800
 particularly to tool three around expanding the ethical circle and that's the lack of

38:57.800 --> 39:06.000
 diversity in tech and particularly in AI less than or only 12% of machine learning researchers

39:06.000 --> 39:11.000
 are women so this is kind of even worse than the tech industry in general and the statistics

39:11.000 --> 39:17.480
 are similarly dire when it comes to race to age to other demographic characteristics so

39:17.480 --> 39:22.460
 we have a very kind of homogeneous group of people building really powerful technology

39:22.460 --> 39:31.240
 that is impacting pretty much everyone kind of an example of the positive of having a

39:31.240 --> 39:38.640
 more diverse team Tracy Chow was the fourth employee at Quora as well as an early engineer

39:38.640 --> 39:43.960
 at Pinterest and she wrote how the first feature she built when she worked at Quora was the

39:43.960 --> 39:48.760
 block button and she wrote I was eager to work on the feature because I personally felt

39:48.760 --> 39:54.040
 antagonized and abused on the site gender isn't an unlikely reason as to why and if

39:54.040 --> 39:58.040
 she had not been there and advocated for this they probably wouldn't have added a block

39:58.040 --> 40:03.440
 block button till later on so this is kind of one example of of the kind of positive

40:03.440 --> 40:13.360
 aspect of having a diverse team so I I went through a period where I became very kind

40:13.360 --> 40:20.160
 of discouraged and disillusioned in the tech industry I I was in my early 30s at the time

40:20.160 --> 40:24.600
 and had been kind of focused on math and computer science since I was a teenager but I was just

40:24.600 --> 40:31.240
 miserable kind of largely due to the toxic culture and so I hired a career counselor

40:31.240 --> 40:36.080
 I retook the GREs because it had been 10 years and I was really thinking though like what

40:36.080 --> 40:40.800
 am I gonna do I just can't can't see myself continuing to do this I wrote a post about

40:40.800 --> 40:45.240
 my experience and about a lot of the research I ended up doing called if you think women

40:45.240 --> 40:51.580
 in tech is just a pipeline problem you haven't been paying attention and so kind of one key

40:51.580 --> 40:57.520
 statistic I want everybody to know is that 41% of women working in tech end up leaving

40:57.520 --> 41:04.440
 the field compared to just 17% of men this is a very very high number and kind of no

41:04.440 --> 41:10.920
 matter how many girls you teach to code it's not going to solve the diversity issues in

41:10.920 --> 41:20.520
 tech if women continue to leave leave at such a high rate meta-analysis of 200 articles

41:20.520 --> 41:26.240
 white papers books found that women leave the tech industry because they're treated

41:26.240 --> 41:31.320
 unfairly underpaid less likely to be fast tracked than their male colleagues and unable

41:31.320 --> 41:37.320
 to advance and so I really encourage everyone if you're interested in diversity to focus

41:37.320 --> 41:41.600
 on the opposite end of the pipeline from what people normally talk about which is the workplace

41:41.600 --> 41:47.000
 and making sure that the women and people of color in your workplace now are treated

41:47.000 --> 41:54.720
 well that you can retain them that they have opportunities to advance oops unfortunately

41:54.720 --> 42:01.360
 often diversity efforts end up focusing primarily on white women which is wrong women of color

42:01.360 --> 42:07.640
 are facing many additional obstacles and barriers and it's kind of even more important to focus

42:07.640 --> 42:18.200
 on them so I that kind of dug into the research on why are women getting fewer fewer chances

42:18.200 --> 42:24.280
 to advance some of some of the research around that is there's a study that found that men's

42:24.280 --> 42:30.720
 voices are perceived as more persuasive fact-based and logical than women's voices even when

42:30.720 --> 42:37.360
 reading identical scripts researchers found that women receive more vague feedback and

42:37.360 --> 42:42.920
 personality criticism and performance evaluations which is not so helpful whereas men receive

42:42.920 --> 42:49.200
 actionable advice tied to business outcomes and then when women receive mentorship it's

42:49.200 --> 42:53.840
 often advice on how they should change and gain more self-knowledge when men receive

42:53.840 --> 42:59.300
 mentorship it's often public endorsement of their authority and so perhaps not surprisingly

42:59.300 --> 43:03.000
 mentorship for women has not been linked to getting a promotion whereas mentorship for

43:03.000 --> 43:09.880
 men has been linked to getting a promotion women also experience being excluded from

43:09.880 --> 43:14.920
 more creative and innovative roles not receiving high visibility stretch assignments which

43:14.920 --> 43:20.760
 are also often useful for advancing and being channeled into less rewarded execution roles

43:20.760 --> 43:26.960
 and so I have links to all this research as well as more in my post the real reason women

43:26.960 --> 43:35.760
 quit tech and how to address it and then just another aspect of this that I've thought a

43:35.760 --> 43:41.400
 lot about is how to make tech interviews a little less awful the interview process in

43:41.400 --> 43:47.760
 tech is terrible for everybody right now there are a lot of problems with it I do think that

43:47.760 --> 43:53.280
 interviewing and hiring are really difficult problems and they are they are tough to get

43:53.280 --> 44:01.120
 right but two two pieces of research I wanted to share with you one is a company called

44:01.120 --> 44:07.040
 triple bite what they what they do is this is specifically as kind of a recruiting company

44:07.040 --> 44:12.680
 for engineers they have engineers take test with them and then they have kind of detailed

44:12.680 --> 44:18.600
 data on where those engineers interviewed where they got offers where they got rejected

44:18.600 --> 44:22.920
 and they can compare that because they have this kind of standardized test that they gave

44:22.920 --> 44:28.600
 gave over 300 engineers the number one finding from triple bites research is that the type

44:28.600 --> 44:33.620
 of programmers that each company looks for have little to do with what the company needs

44:33.620 --> 44:39.040
 or does rather they reflect company culture and the backgrounds of the founders and so

44:39.040 --> 44:45.400
 this is discouraging it's perhaps not surprising people like to hire people like them that

44:45.400 --> 44:50.240
 triple bite post gives the advice to if you're looking for a job to try to find companies

44:50.240 --> 44:54.880
 where the founders have a similar background to you but clearly this is going to be much

44:54.880 --> 45:00.080
 easier for for certain people than for others depending on on your background and on your

45:00.080 --> 45:09.400
 demographic another another study that I love to tell people about is one where they had

45:09.400 --> 45:15.080
 people choose between two resumes one had a male name one had a female name and one

45:15.080 --> 45:20.640
 of the resumes the person had more practical experience and the other they had more impressive

45:20.640 --> 45:27.120
 academic credentials and so people typically picked the the man as the candidate and then

45:27.120 --> 45:31.700
 they would say well I picked him because he had more practical experience or I picked

45:31.700 --> 45:37.840
 him because he had more you know impressive academic credentials and this was they did

45:37.840 --> 45:43.840
 both possible pairings and so this is an example humans are great at post hoc justifications

45:43.840 --> 45:50.040
 it's really important to kind of have a formal credentials ahead of time and to know or not

45:50.040 --> 45:55.640
 credentials but a formal outline of what you're looking for so I linked to those and a bunch

45:55.640 --> 46:00.880
 more research in my on my post on tech interviews but I do acknowledge it's it is a tough a

46:00.880 --> 46:06.680
 tough problem and it's very time intensive to try to create a good good interview process

46:06.680 --> 46:14.680
 all right so in summary we have seen seven practices to implement from the mark Bula

46:14.680 --> 46:21.120
 Center tech ethics toolkit in our previous lesson we saw data sheets for data sets by

46:21.120 --> 46:28.400
 Timnit Gebru we also saw model cards for model reporting by Margaret Mitchell at all we also

46:28.400 --> 46:33.600
 saw the diverse voices that was the framework actually I should say so the idea with data

46:33.600 --> 46:38.660
 sheets for data sets is you're never going to eliminate bias from your data set but let's

46:38.660 --> 46:45.720
 at least be explicit about how this data set was created under what constraints what are

46:45.720 --> 46:50.360
 its limitations and let's kind of kind of to be explicit with that and not just assume

46:50.360 --> 46:55.160
 it's kind of some sort of universal ground truth diverse voices was the work from the

46:55.160 --> 47:01.240
 University of Washington tech policy lab about how to create expert panels with people from

47:01.240 --> 47:06.840
 that kind of various you know various stakeholders such as formerly incarcerated people who don't

47:06.840 --> 47:15.480
 have cars to get their feedback about about a paper or project and the post that interviewed

47:15.480 --> 47:22.520
 15 former or not former and current trust and safety employees and this idea of integrating

47:22.520 --> 47:28.840
 trust and safety more closely with product and eng and then these tasks of retaining

47:28.840 --> 47:34.360
 and promoting people from underrepresented groups and overhauling the interview process

47:34.360 --> 47:38.920
 and so I kind of encouraged everyone in the class and I encourage you to ask which of

47:38.920 --> 47:45.680
 these tools or practices sounds most helpful to you and then also which do you think would

47:45.680 --> 47:52.120
 be the most realistic to implement and so kind of looking back at this many people thought

47:52.120 --> 47:56.800
 tool three seemed particularly crucial although they also thought that was one of I think

47:56.800 --> 48:01.680
 the harder ones to implement but definitely definitely think about this and think about

48:01.680 --> 48:06.000
 it you know is there anything concrete that you can you can take from this kind of back

48:06.000 --> 48:18.080
 to your workplace and then I want to close now by emphasizing that we need both policy

48:18.080 --> 48:25.240
 and ethical industry behavior this this lecture has been more focused on ethical behavior

48:25.240 --> 48:29.680
 in industry and what processes can you implement in a company however that is not going to

48:29.680 --> 48:35.880
 solve everything and we need policy as well policy is the appropriate tool for addressing

48:35.880 --> 48:44.320
 things like negative externalities so a negative externality shows up the classic example is

48:44.320 --> 48:49.960
 a company you know dumping its waste into a river or bay and that influences everyone

48:49.960 --> 48:54.720
 around them and so they're kind of offloading their their cost to society while reaping

48:54.720 --> 48:58.920
 in the profits and I think we're right now seeing the tech industry offload a lot of

48:58.920 --> 49:05.280
 cost to society while they while they make bad profits misaligned economic incentives

49:05.280 --> 49:09.280
 and this is something that I think even when people are well intentioned if it is really

49:09.280 --> 49:16.160
 profitable to do something that is is bad for society there's a kind of a misalignment

49:16.160 --> 49:20.980
 there and policy is the appropriate tool for for addressing that also race to the bottom

49:20.980 --> 49:25.760
 situation sometimes there's something kind of really ethical and even yeah you know even

49:25.760 --> 49:30.720
 if you can convince your company not to do it which is great please do that there's still

49:30.720 --> 49:35.220
 other companies that are going to do it you kind of get sometimes these kind of like worst

49:35.220 --> 49:41.560
 common denominator situations again I think you need policy for that as well as for enforcing

49:41.560 --> 49:48.600
 accountability kind of enforcing meaningful and significant penalties for companies that

49:48.600 --> 49:55.320
 that do wrong and harm people however policy would not be sufficient on its own either

49:55.320 --> 50:01.040
 because the law is not always going to keep up with new technology the law is also not

50:01.040 --> 50:06.840
 always specific enough to kind of capture every every nuance or every edge case and

50:06.840 --> 50:10.880
 so for this reason it's important to have ethical practitioners in industry as well

50:10.880 --> 50:15.880
 so I just wanted to highlight that while while we were focused on kind of what you can do

50:15.880 --> 50:21.800
 kind of assuming you're in a industry workplace I also believe policy is necessary that's

50:21.800 --> 50:26.960
 something we'll talk more about in week five that was also I organized a tech policy workshop

50:26.960 --> 50:32.400
 in November here at the Center for Applied Data Ethics and all kind of in the process

50:32.400 --> 50:59.040
 that we'll be releasing all those videos online for you thank you.


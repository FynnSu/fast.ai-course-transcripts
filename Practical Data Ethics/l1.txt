 All right, so first off, welcome everyone. I'm David Uminski, I'm the Executive Director of the Data Institute. If this is your first time in the Data Institute, welcome. There's a ton of things going on here, including this class, which is what's brought you all in. Every Friday we have a weekly seminar on all things data science, including ethics as well. And I'm delighted that this is the inaugural class. There's been a tremendous amount of interest and volume as well as thought put into this class. And we're excited to launch for the very first time to the public, and then we're gonna have other classes for graduate students and undergraduate students eventually. But you guys are the first to have Rachel teach a formal class in data ethics. I know, right? So congratulations. So if you have any questions around things like wireless and all that stuff, I can't help you. But there are people here to help you. And Michaela in particular, who I think has been your point of contact, is gonna be your most useful. She's still outside, I believe. But without further ado, I know why you guys are here. Rachel Thomas is the Center for Applied Data Ethics Director, the inaugural director, and she was our number one choice, as you can imagine, for anyone to launch anything in ethics here at the institute. She's got an amazing background and experience. And besides being a prolific writer, she got her PhD from Duke in mathematics and is the co-founder of Fast.ai, which is also on the side deck. So without further ado, let's welcome Rachel to the class. Thank you, David. Thank you. Thanks, yeah. So welcome, everyone. It's great to have such a full room for this. So data ethics is kind of in the news, I would say really pretty much every day now, all sorts of kind of really concerning developments. I'm gonna start this evening with just three quick examples that I kind of want everyone to know about and the types of problems we'll be thinking about in this class. And I'll say a little bit about the class and then we'll get into the first topic, which is disinformation. So one example that comes up a lot in data ethics is feedback loops. And so a feedback loop occurs whenever your model is controlling the next round of data that you get. And so your data kind of quickly becomes flawed or at least influenced by the software itself. And this is something I think particularly many of us from science backgrounds tend to overlook because you're thinking, oh, I'm observing the world and kind of seeing what's happening in the data. But if you're building any sort of product that's being used, you're also having kind of this big influence. And so one place we see this is with recommendation systems where, you know, ostensibly they're predicting what content people will like, but they're also controlling what content people are exposed to. Second, and I should note, I'll be kind of stopping periodically for questions and we will have some time for discussion and the later classes will be a bit kind of more discussion oriented. Second instance that I think about is when systems are implemented with no way to identify or address mistakes. And so this is something that kind of happens, you know, outside the data itself, but thinking about the broader system that it's within. And so this is a headline from an article following when a new system was implemented in Arkansas for determining people's Medicaid benefits. So this was for poor people accessing medical care that they needed and there was a software bug that incorrectly cut the care for people with cerebral palsy including Tammy Dobbs pictured here. And so not just for kind of people losing access to care they needed, but there was no way to even point out or discover that there was a mistake because there was no system for recourse and nobody was on the lookout for mistakes. Unfortunately, this was eventually kind of surfaced through a lengthy court case, but this is another kind of important consideration to have of kind of how, you know, what safety mechanisms do you have in place to discover when something's gone wrong. And then a kind of third example when everyone to know about is study by Latanya Sweeney, who's director of the data privacy lab at Harvard, has her PhD in computer science. And she discovered several years ago that when you Google her name, you get ads saying Latanya Sweeney arrested. This is for a background check company. And she's the only person with that name. She's never been arrested. She paid $50 and confirmed her background check says that she's never been arrested. However, when you Google other names like Kristin Lindquist, you get a much more neutral ad saying, we found Kristin Lindquist from the same background check company. And so being a computer scientist, she wanted to approach this systematically. She looked at over 2000 names and confirmed that this pattern held that kind of traditionally African-American names were getting the ads suggesting that they had a criminal record regardless of whether they did, whereas kind of traditionally white or European-American names were getting much more neutral language. And this is something that continues to be an issue. I mean, what was going on in this particular case is the background check company said that they were placing both versions of the ad for each name, but Google ads automatically lets you do AB testing. And so people were clicking on the ad suggesting that someone with an African-American name was more likely to have a criminal record. And that was kind of what was winning in the AB testing. And kind of similar issues continue to surface. Research less than a year ago showed that Facebook's ad system seems to discriminate by race and gender, even when the advertiser is not trying to. For housing ads, having the exact same text, but switching the picture between a white family and a black family served up very different audiences. So these are kind of ongoing and urgent issues, a common issue of bias, which we'll be talking about more next week. So as David mentioned, this is the Center for Applied Data Ethics, kind of housed within the USF Data Institute. If you're interested, we have data ethics seminars a few times a semester that are open to the public. We had a tech policy workshop in November, and we'll be releasing all the videos from that soon. And then just briefly some background about me. I've worked at the Data Institute for three and a half years. I've also worked professionally as a data scientist and software engineer. If you're kind of interested in some of my other writing or Twitter, it tends to mostly be about data ethics. So here's my email and feel free to reach out to me with any questions kind of about the course or in general. So some course logistics. My plan is to give a kind of weekly quiz, mostly just to check if you've done the reading and also to incentivize doing the reading. I also started a spreadsheet of AI ethics issues in the news each week. And this is an idea I borrowed from Casey Fiesler, who's another tech ethics professor. And I thought that could be kind of interesting to see and the hope is that everyone could contribute one article each week and to see what happens over the six weeks of the course. And then there'll be an optional writing assignment. If you're interested, you can research and reflect on the ethics issue of your choice and I'm happy to give you feedback on a draft if you're interested in posting it as a blog post later. I was gonna use forums.fast.ai for discussion. And this is what we've used with the deep learning courses. And it's a pretty nice software. It's open source discourse in terms of it has great search functionality when you start entering something. And so I've started a few threads. In particular, I started an introduce yourself here if you wanna write an introduction, because I definitely hope to get to know more about all of you over the course of the course and I hope that you get to know each other and can kind of build some community around people that are interested in data ethics. And so this will be a great place to kind of keep the discussion going. I did wanna let you know I have this set to private right now just for people in the course, but I was planning to open it to public after the course ends. Let me know if you have concerns about that. It was something more ultimately, and also I'm recording the course right now to share, but I hope to get kind of more people involved in discussing data ethics. And I'll let you know before I do that. So I always interested to read other tech ethics syllabi and have spent a lot of time doing that. And Casey Fiesler, who's a professor at the University of Colorado, created a crowdsourced spreadsheet of over 200 syllabi for tech ethics courses. This was maybe a year or two ago. And then she did a meta analysis on what do we teach when we teach tech ethics, which I thought was really interesting. And she highlighted there kind of a number of open questions or controversies about how to best teach tech ethics. Should it be a standalone course or integrated into every course in a curriculum? Who should teach it? So it resides in a lot of different departments. If you look across schools, should it be a computer scientist, a philosopher, or sociologist? This is a chart kind of just showing the discipline of what department these courses were taught in, of the ones she looked at. What topics to cover is a huge question. And there was kind of a lengthy list of law and policy, privacy and surveillance, philosophy, justice and human rights, civic responsibility, AI and robots, and the list goes on. And so this is far more than you can fit in any course, even if we had twice as long as we do, we would not be able to cover kind of more than a fraction of this. Oh, and then what learning outcomes? And this is something where there is a little bit more of consistency, that kind of one of the top goals of tech ethics courses is to teach the skill of critique and spotting issues, also making arguments. So hopefully we'll get to some of that. Yeah, so I share this just to say that we won't be able to cover everything and that that's kind of typical of tech ethics courses and that there is this huge variety, but hopefully some of the skills around kind of discussion and spotting issues and thinking about them will transfer even to other areas. So the syllabus, and hopefully you received this, Michaela sent it out. I also have it posted in the forums. If you have any trouble accessing the forums or creating account, feel free to email me. And that's something where I have to add you. And so I've tried adding everyone's email for the email you use to sign up for the course. But if you haven't created an account yet, then I have to kind of go back and add it. So just let me know when you do. Oh, and one other note that I forgot earlier, but I want to introduce a special, not really guest, but new resident. This is Ali Al-Khatib. And he's joining us as part of our kind of first class of, we haven't officially announced this yet, but Data Ethics Fellows here at the Center for Applied Data Ethics. So he'll be here. And so briefly, so I'm gonna kind of, I want to start with kind of two, so this week disinformation and next week bias, kind of two really in-depth, more than a case study, but kind of like an issue and really kind of thinking about the issue in depth and then kind of circling back to more, just even talking about what are ethical frames we can use, what are tools we can use to address these. But I think it'll help ground us to kind of first have these issues. So I'm just gonna briefly say this evening that ethics is the discipline dealing with what's good and bad, or a set of moral principles. I linked to or assigned this article from the Markkula Center, and then there's another great one of just kind of overview of ethics and ethics is not the same as religion, law, social norms or feelings, although it does have overlap with all those things to varying degrees. It's not a fixed set of rules. Ethics is well-founded standards of right and wrong. And it's also the study and development of your ethical standards. And that kind of needs to be a continuous and ongoing process kind of as we encounter new situations. Actually, I'll stop and pause. Are there any questions just about the class before we launch into disinformation tonight? Yes, Erin. The Alton Lab with the Jupyter notebook and I saw that you had that as supplemental or at least today, but are you trying to have something more practical at the end of each course? That's a good question. I forgot, I'm gonna use the catch box in general. But so the question was, is there gonna be a supplemental kind of coding lab at the end of each lesson? The answer is probably not. I had wanted to, but I kind of with time and with some of the topics, I don't think that'll happen. But I would love to do that kind of in future iterations. Yeah, and this course has essentially no prerequisites. I want this to be open to everyone, but I also hope you can use kind of whatever particular skills you have from your background. Yes. Just on ethics itself, are we gonna focus on that in any particular area or is there a way to come back to each topic? So it's something we'll come back to. In lesson three, we'll talk about a few different ethical frames and deontological ethics, consequentialist ethics and virtue ethics that can potentially be used. And we'll also talk about kind of toolkit of processes you can implement. Although on the whole, the focus of the course will be on kind of the applied side in particular cases. Thanks. So let's dive into disinformation, which I think is very kind of relevant and urgent issue. So in 2016, a group called the Heart of Texas posted on Facebook about a protest to be held outside an Islamic center in Houston. And then another group called for a counter protest. So people kind of showed up on both sides. You had kind of the counter protest supporting freedom of religion and inclusivity. And a reporter from the Houston Chronicle noticed something unusual, which is that he was not able to get in touch with the organizers for either side. And it came out kind of only months later that actually both sides were organized by Russian trolls. And so this is something that is, it kind of captures how real people can really get caught up in disinformation. And this is also different than the idea of quote, fake news in that the people on both sides kind of were acting on beliefs they had, but they were doing it in a way that was framed kind of very deceptively by foreign operatives who weren't who they claimed. So I think this is kind of an important example to keep in mind and also just how kind of tangible this is in the real world, that these are kind of real people protesting again in this frame that was created in a deceptive way. So disinformation has been in the news a lot, I think particularly with respect to deepfakes videos, but that's just one form of disinformation and we'll talk about many others. And tonight, I want to get a bit into what isn't disinformation, how do the tech platforms make it worse, how will new advances in AI make it worse, and what should we do? So this is a site called Radio Africa, purporting to be a local news source in Khartoum, Sudan. And it came out this fall that this was set up by Russians as part of a influence operation in six different African countries, where they created kind of what seemed to be local news sources and in some cases hired locals as journalists. And they had 73 Facebook pages with over nine million interactions on them. And they used WhatsApp and Telegram as well. So this was kind of multi-platform, they're encouraging people to join groups. And it wasn't just kind of false stories or even stories promoting Russia, it also included the type of thing that you would see kind of promoting local tourism. There was kind of stories about sports and culture and a wide variety of stories. So disinformation is a lot more than just fake news. And in fact, Claire Wardle, who's a kind of excellent expert in this area, discourages the use of the term fake news because it's not just news, it's memes and videos and social media posts. Another article that I included, actually curious who did the reading for this week? Oh, awesome. Okay, great job, everyone. So this was about a kind of campaign in 2014, as you probably read, where some trolls on Fortran said that they wanted to get the hashtag cancel Father's Day trending. They wanted to pretend to be black women to do this as an effort to kind of make feminists look bad. And they were successful in getting this hashtag picked up. And then several kind of far right media outlets, you know, picked up like, oh, look at this hashtag trending. And so this is, so they created accounts like, nae nae can't stop, which again, is a totally kind of fraudulent account. And this is something because it was posted on Fortran was, well, it was discovered because the accounts were not that convincing, particularly to the kind of black women that were in the same community that the trolls were ostensibly trying to imitate, and then was confirmed that it was fake. And so, and this also kind of captures that with disinformation, people can have very different motives. I think in some cases, there are people that maybe think that they're being ironic in kind of promoting something that's offensive or kind of enjoying the kind of hoax aspect of wanting to trick others. There are other people though, that may be outraged by it and take it very seriously. And so you tend to have kind of a range of motivations and also emotions that may be kind of evoked by various materials. And you've got this whole mix of rumors and hoaxes, propaganda, misleading content, misleading context. Most of it is misleading, not fake. So a lot of people refer to fabricated news as something that is totally made up. But a lot of it, there's this kind of this spectrum of, if you think of how to lie with statistics, you can give a statistic that is technically true in a particular context, but which has been presented in a way that's super misleading. And that's often used. And then also the term fake news has been co-opted and is being used to attack the press. And so this is kind of why Claire Wardle recommends staying away from this term, which I mostly try to do. Any questions so far? Okay. Disinformation also includes orchestrated campaigns of manipulation. So it's not necessarily just like a single post, but this kind of entire campaign and network. And so we kind of saw that with this example of the Russian operations in six countries that was uncovered this fall. And that was uncovered by the Stanford Internet Observatory. Disinformation is an ecosystem. And so Kate Starbird of the University of Washington has done a lot of work kind of looking at both Twitter and websites of kind of who links to who. And so this is a kind of diagram of people tweeting about the Syrian white helmets. And blue is kind of supportive or positive towards them. And pink is negative or opposed to. And she, in this example, found that most of the people tweeting about this seem to be kind of sincere, genuine people, not operatives. But if you looked at kind of what sites they were linking to, there were a few sites that showed up over and over. So you see YouTube was a huge one here. Also Russia Today, Sputnik News. And so this is something where you kind of have to think about this, it's not just within Twitter, but kind of who's being linked to. And again, you can have genuine people perhaps sharing information from questionable sources. And Claire Wardle talks about this idea of the trumpet of amplification. And it's how ideas or means can make it from 4chan or 8chan, eventually into our kind of mainstream media conversation. And one common path is kind of to first be picked up by closed messaging groups, such as on WhatsApp, Telegram, or Facebook Messenger, where things are widely shared. Then they may jump into kind of conspiracy communities on Reddit or YouTube, from there to social media, and then are often covered kind of by professional media or politicians. Although we have seen examples where it's a much shorter chain and things kind of jump there sooner. And so this makes it though, it's like a very difficult problem to even study or address because you're looking at kind of so many different companies and sites and organizations. And because this can move very quickly between sites, people can also, or at least the phenomena can kind of leverage inconsistencies in the rules or enforcement of different sites. And one of the reasons I think that this is really important to kind of be thinking about and working on is that disinformation undermines democracy. Ladislav Bittmann, who used to work for a kind of Soviet disinformation office, I believe in the 60s, and then defected to the United States and later became a professor of disinformation, said most campaigns are a carefully designed mixture of facts, half truths, exaggerations, and deliberate lies. I've heard other people say that, you know, good propaganda contains seeds of truth. So again, there's not, you know, necessarily kind of a clear distinction, true or false. Kate Starbird said disinformation is not just about bots and trolls. It targets, cultivates, shapes, and ultimately leverages unwitting crowds to further spread and achieve its objectives. And we saw that kind of with that first example of people actually attending protest. Totally unwittingly, you know, when I saw the protesters that support freedom of religion and diversity, I liked many of their signs, I can relate, but they're, you know, being leveraged kind of as part of a part of this campaign. So we saw that here. And then disinformation pollutes our information environment. And so Zainab Tafekci, who's kind of one of the foremost experts in this area and has also really studied kind of the role of technology in protest around the world, says when I talk to dissidents around the world, they rarely ask me how they can post information anonymously, but they do often ask me how to authenticate the information they post. Dissidents can end up putting their lives on the line to post a picture documenting wrongdoing, only to be faced with an endless stream of deliberately misleading claims. That the picture was taken 10 years ago, that it's from somewhere else, that it's been doctored. And Zainab definitely supports that there are people that need to post anonymously, particularly whistleblowers may need to have, you know, kind of stable pseudonyms that they can use in sharing. So I thought this was really kind of interesting perspective about how often people can be discredited and people want a way to authenticate what they're sharing. Because the problem with disinformation is not just that we may believe things that are not true, but that we may not believe things that are true. And that's kind of a real risk and something that's already happening. Kind of related issue that Zainab has spoken about is how the nature of censorship has really changed. And censorship now works by flooding people with information, by causing distraction, causing confusion, creating doubts and just this question mark and shadow so that you can't really figure out what's going on. And so she was in particular talking about kind of the WikiLeaks dump method and saying that just kind of releasing things isn't necessarily whistleblowing. She refers to it as whistle drowning, although this happens kind of even more broadly than just document dumps, but that kind of we're just flooded and inundated with information and can drown out really important, kind of important news that we need to hear. And so kind of in the context of releasing large troves of documents, this is referred to as hack and leak, includes climate gate and Hillary Clinton's emails. And what happens is something called narrative laundering, where people can build stories on top of real documents. And so kind of taking real documents, but then using them towards kind of certain ends to try to create a narrative. Something that experts such as Renee DiResta have warned about is that we may see more mixing faked documents in with real documents. And so there was a set of cyber attacks on anti-doping agencies sometime last year. And people say that there were kind of some fake documents mixed in, mixed into that. Let me pause. Are there any questions? I have a question, but just kind of through the narrative laundering, one place that I guess I'm- And can you hold the catch box a little bit closer? So one place like I've seen narrative laundering, especially as something like National Geographic and stuff where they're filming animals and then come up with a story. I have a friend who's actually filming for National Geographic and said he took months and months of film and then sent it in. They completely moved everything around and like the clips are not actually in the order that things take place. And the story is totally written afterwards. Oh yeah, that's an interesting example. I think that also gets at that there, I'm sure this happens with, you know, any documentary is going to have to like cut down the material and is imposing some sort of kind of frame on what gets chosen. And I think there are ways to do that ethically and then also ways where you've become deceptive with what you're doing. And I don't know that it's always, I'm sure there are a lot of in-between instances where it may not be clear of like, okay, this is an acceptable amount of editing and shaping. And okay, now you've definitely crossed over and are misleading people. And I think that's also kind of an example where you have just this huge volume of material that you have to kind of cut down because volume is one of the things that I think enables this. Just wondering if you have a concrete example of narrative laundering since there are a few questions around it, maybe it would help. Sure, I mean, so I think like definitely like with Climate Gate and, you know, that's something where kind of all these emails from climate scientists were hacked and released. And really the discussions they were having were completely reasonable. Like there was not, I don't think any scientist thinks there was like a controversy there about what they were doing or how they were doing it, but it was used to kind of things were taken out of context and kind of put together to be like, look, these climate scientists are lying to us. They're kind of doing unethical things to make it seem like climate change is happening when it's not. And so that kind of created this whole narrative that was not accurate. But it's something where kind of if you have enough volume of documents, like I think it gets easier and easier to do that. So do we also see this type of pattern where you have like highly like field specific, so the scientific papers or medical papers that take very specific clips out and then turn it into a form of language as much more easily consumed. So less like area specific language, just common language that the masses can kind of consume and understand in a, does it make sense? So if I'm reading a medical paper, which is maybe like 20 pages long, I'm probably not gonna read the whole thing. You can take a couple of sentences out of there, they're true, and then build a story on top of that that can be put into 140 characters or however many, say I know, this is what I'm saying. Just a typical pattern that we see with narrative laundering. So like what you're describing I think often would not be narrative laundering because someone I think could be right making a story that even though it's oversimplified could be very accurate. And this I should check, but like this definition was in the context of kind of I think like a massive volume of documents, but you're right about the kind of the dynamic of yeah, if you have a 20 page highly specialized paper, for most people to understand it, we do need someone to come along and give us kind of a simpler story about it. Although ideally that someone would be trustworthy and would produce a story that was kind of accurate and in keeping with what was there. All right, another question. Oh wait, can you send it back just for the recording? So I guess kind of my answer that would be yes. I was a psychology major undergrad and read so many psych papers. And one of the pieces of every published psych papers there has to be a conclusion. And so many psychologists take the study that they did and as part of the conclusion they have to draw real world applications, real world, how do you apply it? And even though so many of the studies are just done at colleges, mainly female participants, they generalize it to everyone in the world, even though it's done in the US and they only apply to Americans. And so they do a lot of this kind of creating a story for the whole world where that may not be really where it applies. Yeah, and I think that this gets at broader issues with scientific communication. Although I do want to bring us back to kind of disinformation specifically, but yeah, they're all kind of perhaps similar issues that show up in other forms of scientific communication. Okay, I'll keep going. We'll have more time for questions later as we go on. So now I want to talk a little bit about how the kind of the role of the tech platforms in incentivizing and promoting disinformation. So this is kind of not something that happens in a vacuum. And so this is mostly unintentional, but it shows up in their design and architecture and their recommendation systems, in their business models around kind of what gets incentivized. And so this is, you know, in a lot of these choices when they were originally being made, probably people weren't thinking about disinformation at all, but they do kind of help create the ecosystem that we're in. So Guillaume Chaslot is a former Google engineer who worked on YouTube's recommendation algorithm back in like 2013 and has been very vocal about it since leaving. He's also founder of Algo transparency group. So this is a chart. So he kind of monitors kind of YouTube's recommendation system from the outside now. This is a chart he created that was picked up by the Washington Post. And here the X-axis is the number of channels recommending a video. And the Y-axis is the log of the number of views. And you see there's this extreme outlier that was Russia Today's take on the Mueller report, something that was being recommended a ton, even though it was actually not ending up kind of more popular like you might expect. And I think this is kind of potential evidence that the recommendation system can be gamed or has been gamed, which I think is a risk really kind of anytime that you're really relying on metrics. Wrote a post about this in the fall, the problem with metrics is a big problem for AI that kind of whenever you put a lot of emphasis on a metric, people can and will try to game it. You also see maybe unexpected kind of behavior or side effects to what you're doing. So this is one frame for kind of for thinking about disinformation and this has gotten kind of a lot more media attention, I would say in the last six months to a year about the role of recommendation systems. Another interesting study from this fall looked at basically how people, so they asked people, do you think this story is credible? And they kind of balanced forgetting Republicans and Democrats and found that people could identify kind of whether a story was credible or not, even across their political lines. But then they also kind of a separate large group of people asked, would you share this story or not? And that was basically completely disconnected from whether it was credible and was very tied to political ties and so this kind of suggests that when people are deciding whether to share something, they're not even thinking like, is this credible? There are a lot of other kind of emotions and factors that go into kind of what gets shared. And I liked in the paper, they highlighted that social media platforms may tilt users away from considering accuracy. For instance, they encourage users to rapidly scroll and spontaneously engage, so they're not necessarily kind of encouraging people to spend a long time thinking about a particular post or a particular article before they share it. They also mix very serious news content with emotionally engaging content, so often kind of a really engaging in this emotional way and then also, you know, you've got a cat video or a baby picture and then kind of some very like serious or devastating political news or enraging political news and it's all mixed together. And then also we get this immediate quantified feedback and the number of likes, which really kind of influences people to kind of be getting that response. And so none of this is particularly conducive to getting people to stop and ask, is this credible? So in the study, they did, so there are kind of a few parts to it, but in one part they DM'd people a question and they said, we're doing a survey, do you think the following link is credible? And what they sent them was politically neutral, so it wasn't something that should follow long party lines. And they found that people that received this seemed to tweet more credible links for 24 hours afterwards that just even kind of prompting people to be like, is this credible as a question that you should potentially think about, potentially got them to kind of think about that more. And they only looked at it for 24 hours afterwards. But I thought that was interesting. And so that's, you know, a small study and kind of one piece of data. And then I'll also share this research from Becca Lewis who's a PhD student at Stanford. And she highlights that it's more than just the algorithm. And this is not kind of incompatible with the algorithm playing a role, but she looks at various kind of other dynamics of celebrity culture on YouTube. And so the paper I have listed here, she kind of does, it's a qualitative case study kind of looking at a few YouTube influencers and the way that they have kind of positioned themselves as, you know, the mainstream media is not telling you the truth, but I'm a lot more authentic and credible. And then also kind of the mainstream media is overly pushing kind of liberal ideals, but I'm going to be authentic and credible and kind of tell you these all alt-right ideals and have kind of been effective at aligning these two different axes. And so that's interesting to also kind of keep in mind these other social and cultural dynamics that impact this. So summary, kind of our online environments are designed to be addictive in many cases, so kind of the reading. So I had an article from Guillaume Chaslot in the assigned reading and he talked about how, and YouTube has updated its algorithm, but in the early days it was about maximizing watch time, which I think is true of many platforms. They want to keep people on the platforms longer. The incentives tend to really focus on short-term metrics. And some of this is it's much harder to measure long-term quantities of, you know, what is your long-term kind of trust in the platform or, you know, even just like what's the long-term health of the information being shared. These are tough things to measure and so I think that short-term incentives tend to get overemphasized. And then finally, like the fundamental business model is around manipulating people's behavior and monopolizing their time. And that's something that I think is okay in limited doses, but that ultimately kind of doesn't lead for a great alignment of incentives with the well-being of society. And this is a slide that Renee DiResta, and so Renee is kind of one of the top experts on computational propaganda, and she led one of the teams that analyzed the Russian materials for the Senate House Committee. She was also kind of very involved in studying the anti-vaxxer movement kind of years ago, shared our political conversations are happening on an infrastructure built for viral advertising. And so here there's kind of this real mismatch. So let me pause for a moment. Are there questions on this kind of this component of the role that the tech platforms play? Question in the back, and let me... Oh, perfect. I think it's more of an observation as I'm going through all these points, I'm thinking in particular back to DiResta's article and the shifts in technology, radio to television intelligence, my very broad unqualified observation is that the technology has simply arrived to make this scale out in the way it was intended. I don't know that I see much difference in media, I just see difference in the efficacy of this delivery system. Yeah, I mean, so this definitely is more effective in some of the... Yeah, I mean, there's writing that was done on television in the 80s that you read it, and it's like, oh, it sounds like we're talking about the internet and the problems we're facing now. I mean, the scale is so... Like the scale is significant that it is so big. I mean, I think that around the incentives, like I think that, I don't know, if we didn't have kind of personalized ad targeting, I do think we would be in a different ecosystem, and I think we would still have problems, but they would probably be different problems and there'd be a different nature to the ecosystem, but I think that these are kind of particular choices. You know, like we could... There could be a world where we have the internet and kind of mass communication, but where it's funneling in very different ways. And I do think, like in particular, personalized ad targeting has kind of put us down a very particular path. Kind of a more serious part. So it's also important to note that humans really have kind of evolved as social beings and to have our opinions influenced by others that we kind of consider part of our in-group and often kind of in opposition to people we think are in our out-group, that we're very... We are influenced by others and it can be hard to recognize because I think many of us think of ourselves as independent minded, but society plays a role and people have a lot of different kind of discussions online that can help form opinions and so this is a discussion on Reddit. Someone's saying, I believe the U.S. should cut all defense spending and instead spend money on the military. I know there's a lot of money on the defense budget, but if you take the money we have, somebody else says you're wrong. The defense budget is a good example of how badly the U.S. spends on the military. Someone else says, yeah, but that's already happening. There's a huge increase in the military budget. I didn't mean to sound like stop paying for the military. I'm not saying that we cannot pay the bills, but I think it makes sense to cut defense spending. And so does anyone want to guess what subreddit this is from? That's right. So subreddit simulator, which is the GPT-2 subreddit. So these were all computer generated and they're... I think if you read these closely, you can tell that they're a little bit off, but I think they are close to being compelling of how people might discuss or argue about a topic. And this is something that I think is, and this is clearly marked as having been generated by an algorithm. So it's kind of in good fun, but it's alarming to think about how this could be used. So yeah, GPT-2 and it's all marked. So just raise your hand, who's heard about GPT-2? And GPT-2 is just part of this whole kind of family of research. And so it wasn't... This is a model from OpenAI, and it was kind of along the trajectory of how natural language research in AI has been going. But it can produce stories that are quite compelling. So this is one that they shared when they kind of first published about it. And so here, the human written system prompt that was given said, in a shocking finding, scientists discovered a herd of unicorns living in a remote, previously unexplored valley in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English. So that was written by a human and the response from the algorithm, what the algorithm generated, said, and I'll just read parts of this to you. Dr. Jorge Perez, an evolutionary biologist from the University of La Paz, and several companions were exploring the Andes Mountains when they found a small valley with no other animals or humans. Perez noticed that the valley had what appeared to be a natural fountain surrounded by two peaks of rock and silver snow. Perez and the others then ventured further into the valley. By the time we reached the top of one peak, the water looked blue with some crystals on top, said Perez. Perez and his friends were astonished to see the unicorn herd. While examining these bizarre creatures, the scientists discovered that the creatures also spoke some fairly regular English. Perez stated, we can see, for example, that they have a common language, something like a dialect or dialectic. And so I think that's a really kind of nice story to have been generated by an algorithm. And it was, I mean, it was cherry-picked, not everything is this good, but this is a really good story, and this is computer-generated. And so this is, I think, concerning particularly when you think about combining it with other forms of media. So this is Katie Jones, who is a Russia and Eurasia fellow, connected to people from several kind of mainstream think tanks on LinkedIn. And it was revealed that she's not a real person. So this was discovered by the Associated Press last summer. This is a computer-generated photo kind of created by a GAN. And so again, this is a compelling photo. And so you can start thinking about putting together compelling text with compelling photos, and fake accounts are going to become much, much harder to spot. And I think sometimes, you know, in the past, it felt like, oh, you know, somebody who has an egg for their Twitter profile. Like, you know, this is kind of like a troll or not worth responding to. But those accounts are going to get much, much harder to spot. You can go to thispersondoesnotexist.com to see other examples of GAN-generated photos. So again, this is not a real person, not a real person, not a real person. And I like to highlight this because I think, you know, I think deepfakes and video are getting a lot of attention, but also to think about just the combination of what good profile photos and convincing text will be able to do is alarming. I think online discussions will be swamped with fake manipulative agents, and also at scale. So we've been talking about this idea of volume, but what does it mean to kind of have a high volume? So something that happened back in 2017, which was a long time ago in terms of kind of AI research and developments, is that the FCC was considering repealing net neutrality, and they opened up for comments. So how do people feel about net neutrality, and they got a lot of comments that were opposed to net neutrality in favor of the repeal. Here are a few of them. Americans, as opposed to Washington bureaucrats, deserve to enjoy the services they desire. Civil citizens, as opposed to Washington bureaucrats, should be able to select whichever service they desire. People like me, as opposed to so-called experts, should be free to buy whatever products they choose. So you can kind of see a pattern here, which has also been helpfully kind of color highlighted, but basically this is a kind of mad libs style where you had a few choices for the green spot in the sentence, and then a few choices for the pink spot, and so on. And this was discovered by Jeff Kao, who's now a computational journalist at ProPublica, but he found that there were more than a million pro-repeal net neutrality comments in this cluster, and it's not just that they're kind of using this form, it's that they were designed to look unique and to be different because they're using kind of all these different combinations, and so this is something, this was great work on Jeff's part that he discovered this, but it was still relatively primitive when you think about that this was kind of mail merge style of just plugging in these these different places. And he did, you should check out this blog post, so he found of over 22 million comments submitted, less than 4% were truly unique, and that's not all spam, like there are, you know, campaigns that give you a template to email in, but that is a really small number of unique comments, and more than 99% of the truly unique comments wanted to keep net neutrality, but that was very different than the overall picture you would get if you included kind of these spam campaigns, and this is something that is is concerning, and also thinking back to the kind of more sophisticated language models would be very very hard to identify now, I think, if someone did this. And so yeah, as I've said deepfakes are getting a lot of attention and are something to worry about, but also we need to kind of think about all of these things, as well as also primitive techniques are still really effective, and Photoshop and even just memes on photos are very effective, so it's it's not just about the latest technology, although I do think the latest technology can can certainly exacerbate these things and make them make them even scarier. So my my co-founder Jeremy said last year we have the technology to totally fill Twitter, email, and the web up with reasonable sounding, context-appropriate prose which would drown out all other speech and be impossible to filter, and this also gets back to this idea of kind of volume and how do we filter through the volume to even, I don't know, for real people to have a voice as well as kind of fraudulent campaigns. And the other kind of very concerning aspect of this is that extreme viewpoints become normalized when we're around others who we think hold those views, or if we start thinking that more people hold a certain view, it starts seeming more normal. Actually I should have put people in quotes, but if we think more more entities hold a view. And so kind of on a very serious note, we've seen this rise in white supremacist shootings in the last year or two, kind of mass shootings. So there was kind of the shooting at the Pittsburgh synagogue in 2018 in which 11 people were murdered, and the shooter was very active on social media and even posted directly before committing the shooting. There was the shooting at two New Zealand mosques last year in which 49 people were murdered, and the New York Times characterized this as a mass murder of and for the Internet. The attack was teased on Twitter, announced on 8chan, broadcast live on Facebook. The footage was then replayed endlessly on YouTube, Twitter, and Reddit. And so this is kind of already a very serious problem and we don't need fancy technology to make this work worse, but it is a concern that kind of disinformation and kind of more sophisticated language models and fraudulent accounts could amplify this even further. Alright, with that this is actually kind of a good stopping point for our break. So we'll take a kind of seven minute break and the kind of bathrooms and water fountains are if you kind of turn left towards the end of the hall, and then we'll come back we'll have some more time for kind of discussion and also we'll start talking about solutions more too. Alright, so let's return from return from break. I want to start just by taking questions if there any questions on the last section about this idea of kind of we've seen new technology in text generation, so kind of generating compelling language, generating photos that look like real people but are not, and how this could could be used to really kind of influence or manipulate public discourse. Any questions or thoughts? Alright, oh and can you, and it's fine to throw the catchbox. I shouldn't share this, I hit a girl in the head. She was like she was fine, but I've been more shy about throwing it since then. Yes, well I'll talk about that a little more later and I will just say now Mike Caulfield is a great, he's kind of an expert and on how digital literacy is taught, but he's a great person to look up, but I do have a slide about him later. It's a good question. Okay question up here. This might not be the ethical reaction to this, but you know learning about sort of the arms race around creating this information and how technology to create this stuff. My mind goes to what are the ethical implications of fighting it with the same technology, right? Like drowning out the disinformation. I don't know if there's like a straightforward answer to that, but how much can you use offensive techniques? I think about security, the security role. Yeah, no and there are a lot of analogies to security with disinformation. Yeah, in terms of yeah in terms of drowning out, I don't know, it's an interesting idea, but yeah it is and I'll talk a little bit more about this later, but yeah like for instance Renee DiResta and Mike Godwin, who is the original legal counsel to the EFF, have said you have to think of disinformation as a cybersecurity issue. Question over here, if you can pass the catch box. So the question is, so obviously one alternative you could think is like the companies that the platforms expose this information could have been set is to actually build these techniques to counter and find good fakes or something like that, because obviously a single person as this technology is improving would be harder and harder to what is fake, but I'm sure like big companies, like Google, they can build the tools to do this, right? But then the problem is, I don't know, even if you think in Twitter when they have to report every quarter how many active users they have, probably there's a significant number of users that are fake users and one quarter they have to report, oh yeah, by now by the way we have like 1 million users less than we report, so obviously for Google with the ads there's a more than actually the technical or in other ways the incentive about how the market actually says that, so somehow I don't know, the government needs to increase the downside, I don't know, right? Yes, yeah, and so we'll talk more about this later, but yeah, you're hitting on several things. I mean there are great people at these companies, even though there's a lot I criticize about the the major companies, there are people who are working on these problems, but yeah, I think because of the misaligned incentives and business models that I believe there will be a limit to kind of how much progress we can do, and I do think policy is going to be one component of more effectively addressing them. So what should we do about all of this? So those are some good suggestions. One thing I wanted to note first is to recognize that often the goal of disinformation is to disorient us and to weaken our trust and institutions, and so this is from a post by UHI Bankler earlier this year that did kind of convict me a little bit because I've definitely been trying to talk about the harms of disinformation, but remember that kind of overstating the impact will have the same effect of kind of weakening people's trust in institutions or in shared knowledge, and so to kind of keep our perspective and recognize the things that are still working as well as kind of, I guess, limiting alarmism while taking the threat seriously. So a brief kind of a positive note, and this was something kind of pretty simple, is last year Pinterest, so people were sharing a lot of anti-vaxxer propaganda on Pinterest, and so Pinterest made a change that only kind of well-respected health organizations could even create pins about anything related to vaccines or measles or kind of search terms that people were using, and so this is like the Center for Disease Control, the American Academy of Pediatricians, and the World Health Organization, and so on. They can make pins about vaccine safety, nobody else can, and so this is, you know, a relatively kind of simple solution, but it's something that I think is a really kind of positive step. I mean, I guess the kind of the downside when platforms step in like this, although I totally kind of agree with this application, is it is a lot of a lot of power that the platforms have in terms of how people receive information and kind of which issues they choose as worth acting on. So there was a question earlier about what can, oh, and wait let me, can you pass the catch box? Doesn't the fact that Pinterest has the ability to control what users are exposed to, like the people who are looking for that information, wouldn't they take that as further proof that they're hiding something, that there's sort of these institutions that are working together, and that anecdotal experiences aren't being shared, that there's some value in like just a mom saying, hey this thing happened to me, and that another parent might want to. Yeah, so that argument is made, and that does happen, that like, yes, this would absolutely, I'm sure it was seen as evidence by anti-vaxxers that there's a conspiracy theory trying to suppress the truth. I do think such interventions can, and it's also kind of hard because this is something where it's, this is pretty late in terms of the kind of growth of anti-vaccine propaganda, which is something that, I don't know if you've seen, that has been linked to Russia as well, that there were Russian campaigns kind of both in the US and Europe promoting anti-vaxxer propaganda. It's hard because yeah, reacting to it can be seen as further evidence of the conspiracy theory, although I think there is also an argument that you limit its reach, particularly if you reacted to something earlier, that fewer people seeing it is a good thing and can prevent it from from spreading. There's a, in covering disinformation, there's often kind of this double bind in that even, even picking up a story just to debunk it and saying, you know, this is false is kind of giving it more oxygen, and so this is an area that's still being studied of kind of, there does seem to be some sort of tipping point, which I think is hard to recognize of, you know, if a conspiracy theory is tiny, you don't, you don't want to pick it up because you don't want to draw more attention to it, but then often by the time something is kind of big enough that it's like, oh it's clear, we need to let people know this is false, it's also really big, and so that's something where I don't think there's a clear answer on kind of when, when do you step in. There are, there are best practices for journalists about how do you debunk something in a kind of more responsible way, but that is kind of a very fraught area. Yes? And can you pass the catch box a row back? Are there, so I can tell myself like what is usually a conspiracy theory versus like research reality or whatever, but is there like frameworks or like checklists? Great, great question. So Mike Caulfield is who I would recommend on this topic of digital literacy, and he has a digital literacy course at lessons.checkplease.cc, and one of his big ideas is that in the past there was a lot of media literacy that was kind of giving people like, here's how you can spend a half hour like researching this topic, which nobody's gonna do, you know, and like you can't do that for every tweet you see in your Twitter timeline, and so he really promotes like things that you can do in under a minute because if it's not fast people are just not gonna do it. So I had often felt a little bit skeptical of some media literacy efforts just in that so many of these problems are systemic and I don't want to be tasking individuals with, you know, you have to kind of recognize every false thing. This post from Mike actually found pretty convincing, and he's very aware like teaching individuals to recognize issues is not going to solve the systemic problems and it's not a substitute, but it could help and it can create more resilient networks. And so he gave an example of this tweet that has been retweeted 3,000 times claiming that a husband-and-wife Chinese spy team were recently removed from a level 4 infectious disease facility in Canada for sending pathogens to the Wuhan facility. So it's making this claim about a conspiracy theory about about spies and it's linking to the Canadian Broadcasting Company, which is a very kind of mainstream and respected news outlet, and so his recommendation is number one click on the link and then secondly do ctrl F to search for, you know, you probably don't even have time to read the full link, but just do ctrl F. He also highlights, so ctrl F will search within a web page. He highlights a study from Google that found that 90 percent of web users do not know about ctrl F, and so they're kind of some kind of basic digital literacy tools that are very kind of useful even though they may seem simple. And so he did that, he goes to this this CBC site, he finds that the word spy does not even show up in the article, threat only shows up once in saying that there's no threat to public safety. And so this is something where the tweet has misrepresented the article to kind of try to weave a conspiracy theory. It's perhaps a little bit of an example of what we were talking about earlier, and it's also, you know, it's something that you can check in 30 seconds, you know, you just click the link and he searched for I think a few other words and then saw like, hey, this article does not seem to support what was claimed in this tweet. He also, he makes the point like, I mean, there are plenty of problems with Google search, but doing a Google search of something before you share it is way better than not, and in many cases will surface an issue. And so actually if we have time at the end, I'll go into this. I was going to include it and I wasn't sure about time. He has this game or links to this game called fake out at the beginning where you have to guess if things were fake news or not, and it could be harder than you expect, but those are those are great questions. So I would recommend these materials. And Ali, can you pass the catch box to the front? This just sort of occurs to me having been using mobile devices really extensively for the past few weeks. There is like no control or let's say a smartphone, or at least like not in the iOS as far as I know, and it occurs to me that given how often people use mobile devices to use the Internet in general, that could be a challenge of its own, but that highlights the sort of moral social imperative to think really deeply about like should Apple implement some sort of way to search for text in the page that you're currently doing, and like that seemed like such an esoteric topic until just now. Oh, you should search for the word spy, and if it doesn't show up then question the veracity of using that link in that story. That's just sort of like a thought that came to mind. That's a great example and as I say Stacy Marie Ishmael has a talk that kind of on a similar vein talked about some ways that mobile is less conducive to like you don't see as much of the URL. In some cases you can't even tell that the URL is clickable, like it's not apparent in the design. That there are a lot of things about mobile that are way less user-friendly to letting users know like you know because for disinformation if something's from a sketchy URL we know okay that's a bad sign, but on mobile that is way less clear, and in some apps like in Facebook it's often not clear like hey what is coming from a link that you can click on, can I see that URL, and so these are things that yeah might seem like esoteric design decisions but have a big impact on kind of how users kind of what clues they get that could tip them off about disinformation. Can you get the, sorry, oh okay you can, so the comment was just that it was using Twitter for iPhone. Yeah, I just wanted to ask a question to I guess the group really, but this is so simple. Look for keywords, control app it, or if you're using Chrome on a mobile device they have the version of that, but why can't we just automate that? Why can't Twitter just do this for us and score what it's showing us? I will say that I think this is still kind of a sophisticated question of, so a lot of people ask, so I do a lot of work in deep learning as well with fast AI, and people are like oh you know can't you train a deep learning algorithm to identify disinformation? A lot of it is very context dependent, and I can think of a lot of scenarios where someone could summarize an article and use a new vocabulary word and it would be totally reasonable. You know like part of, part of what makes this suspicious is also just like hey this does seem like a pretty wild thing, potentially a conspiracy theory. You know there are other cues that we're kind of picking up around it, and so this sort of kind of very kind of context dependent is pretty tricky. So I don't think it's something that could be automated, possibly for, I don't even know if, I don't feel, I don't feel comfortable making a prediction that it could be automated, because I think it just involves kind of so much context. But you're right, it's like very simple for a human to do. Yeah, and it's also like with these things. The suggestion was that it scores it, right? So it's not that we're hiding anything, right? You still have the same access to all the same data, you just get like a score that you can choose what to do with, or not do it. I mean just, yeah, I mean two issues with that is that if you have, like, is this legitimate? You know like that research project where they ask people and they make them think about it for hours. One thing I'll say, I mean I like, I like that you're really thinking about this, if you have a score though that's accurate like 90% of the time, people will just start trusting it all of the time and not feel like they need to check the edge cases, and the edge cases are gonna be very significant here. And so I think, I think there are a lot of risk to that approach, but it probably is worth exploring further. And then there's a hand kind of two to your left. I guess the question for you, it seems also pretty easy to game and sort of stay ahead of that metric. Yes, yeah, when you get to kind of algorithm dependent, yeah, there is the, yeah, the risk of gaming or figuring out what kind of what the metrics are. Okay, I'm gonna keep going, but we'll have more time for questions just because there's a lot more kind of potential approaches. So that's a little bit about kind of the role digital literacy could play. Some other approaches detecting fakes and disinformation. It is important to note that this is always going to be an arms race, and so if you're familiar with the idea of a GAN, it's basically you have kind of two algorithms that are learning from each other, and you can use your detection algorithm to make even more compelling fakes. Responsible development tools, addressing the ecosystem, treating as a cybersecurity issue, and verification tools. And I'll get into kind of all of these in the next few slides. So Aviv Ovadia is a researcher on kind of how, if you are making tools for synthetic media, how do you do that responsibly? And so keep in mind like Photoshop is a tool to make synthetic media. Photoshop has a lot of kind of great and legitimate uses. It can be used kind of for for art and kind of all sorts of legitimate things, but Photoshop can also be used to make fake photos. And this is only kind of increasing as different tools are developed that often have kind of, you know, positive uses and have kind of scary potential for misuse. And so if you've had an article in the MIT Tech Review recently with where he kind of goes through what are a few different kind of like categories of how we how we think about this. So one is limiting who can use a tool. So that could be if you are carefully vetting your clients who you give access to, and this would be if you were developing something that let people alter their voices or create fake videos or Photoshop, and I know there are people at Adobe that are kind of working on this issue of disinformation, discouraging malicious use, consent protection. So this would be if you have something that can kind of generate it, like make it sound like someone's voice is saying something they didn't say, have that person have to like say a few kind of generated keys at the beginning to show that they're consenting to using the tool and having having their voice altered, making it easier to detect when when something has been changed or altered. One form of that would kind of be like watermarking it or just making it clear with the, you know, with an image you might not be able to see what the original image is, but just to know like, hey this image has been altered. Usage logs, use restrictions, supporting ethical synthetic media tools, and so he and he goes into more detail kind of in in this article. So this is, and this is not going to solve the problem, this is kind of ways to just try to mitigate it. So this is an idea about our ecosystem. This was an op-ed in the New York Times that I liked, and so here Siva Vaidhyanathan, who he's written a book on, I think, on social networks, proposes that we need to be limiting data collection and the use of personal data to ferry ads and other content to discrete segments of Facebook users. And so kind of one proposal he gives, and so this is something where it really gets into kind of the particular laws of a country, but in the U.S. in a minimum we could restrict targeting of political ads to the level of the district of the race. So one problem with personalized ad targeting combined with combined together with disinformation is that disinformation can be shown to just a very kind of narrow segment of the population. And so, you know, you might not have any journalists see it and realize like, hey this is being shown to this kind of very narrow demographic. And this is, I guess this is true both of targeted ads and also just the way that all our kind of timelines and news feeds are so personalized to us. We don't know what other people are seeing and so we may not hear about a common conspiracy theorem that others are hearing about. So this is kind of one proposal kind of thinking about that targeting there even if people are spreading disinformation if you have to send it to a larger group, at least there are more people to like identify it and hopefully debunk it. This is a report that came out of Johns Hopkins and UNC and one of the authors said kind of in studying how to regulate digital ads that they found a kind of surprising amount of bipartisan support and some of the ideas were having databases of content. And this is something, you know, we're finding now like we don't even necessarily know all the ads that were shown in the 2016 election, but just to even like have that content be discoverable of kind of who who is showing, you know, what ads and describes how many of these are kind of similar to how TV ads are governed. And so kind of remembering that we do have other mediums like TV that have, even if they have shortcomings, at least have kind of more governance of advertising. And then there's a great article by Renee DiResta and Mike Godwin that I mentioned earlier, the seven-step program for fighting disinformation, but really thinking about this as a cybersecurity problem and I think there are a lot of parallels with the cybersecurity here. Stanford did put out a securing American elections report last year that has a number of proposals in it. They are all, I will say they are all things that need to happen kind of on a federal government level though and I think they're all kind of good suggestions but of kind of what we need to be doing to countering disinformation and even just to get a good scope of the problem and kind of what what sort of interference is happening. And I should note it's not just a not just an election problem or a political problem but that is kind of one one key avenue where it shows up. And then another kind of another, I don't know, another category of of tools or things are thinking about giving people ways to verify themselves. And so Zainab, to fact she wrote an article for Wired where she used this analogy with Fidel Castro. It's like in 2006 he had surgery and there were a lot of rumors like is he still alive and so he shared this picture of him holding that day's newspaper to confirm that he was alive. And this you know even now like Photoshop is good enough like this wouldn't be convincing but she talks about kind of we need a digital analog for this for for people to to verify themselves. If you're familiar with PGP keys that kind of idea of giving people a way to verify like this is from me at this time. And she wrote, in the past it often made sense to believe something until it was debunked. In the future for certain information or claims it will start making sense to assume they are fake unless they are verified. And this would require not just kind of whatever verification technology you're using but also a big cultural shift. So this would be kind of quite a shift. Oren Etzioni who's the head of the Allen Institute on AI research made a similar proposal in Harvard Business Review last year. So yeah in summary we have kind of all these different approaches so practicing kind of good social media habits as a as an individual and this kind of digital literacy, keeping our perspective. I also want to highlight just strengthening our institutions such as journalism, education, universities, and nonpartisan government departments. Kind of these these play such an important role in in society and doing what we can to to try to strengthen them. Treating disinformation as a cybersecurity problem and developing verification tools. And I see a hand back there. Can you pass the catch box over? So going forward and then I listed some experts to follow and I also and you can so I started a thread about disinformation on the forums but you can definitely share kind of people that you follow on this topic or if you've read articles or have resources that you like on disinformation please please share them with us. Oh that's it so any any more questions? Okay there's a question behind you. I was supposed to comment on cybersecurity. We have lots of different ways that we think about intrusion of data structure and data integrity. We don't have that same kind of focus typically on content unless it has some kind of commercial value. So I think there's a lot of research that can be immediately applied to just thinking about the corruption of content. Yeah. You've got disinformation as a way to abrade the actual content itself and not just the way it's formed or structured let's say in a relational model or some other kind of graph representation. Yes and actually that reminds me kind of another way that it's disinformation is like a cybersecurity problem is thinking about kind of inorganic kind of inauthentic behavior as opposed to I think people have sometimes thought about disinformation of like oh let me just look at this individual post or something whereas really you need to look like do you have kind of this inorganic activity anomaly detection kind of things that seem inauthentic even if in isolation any one action or one post may not necessarily be like okay this is definitely fabricated news. So you talk a lot about the platforms where going viral is a thing and monetization but what about the more sort of static platforms like Wikipedia which is often like a first go-to in which like especially like the young generations take is like gospel but when you look in the revision history for a lot of the articles you see this separate arms race going in of back and forth people like editing leading and yeah just fighting over who gets the post. Yeah no that's that's a good comparison because yeah Wikipedia in some ways you know not using kind of ad generated model and not gathering all this user data I would say at least has had fewer problems than many of the major platforms but Wikipedia still has significant and serious problems and there's also I know kind of just you know issues about certain groups kind of getting mass deleted or kind of the back and forth although on the whole I would say most people probably consider Wikipedia to be a healthier information ecosystem than than any of the major tech platforms but you're right you can still definitely get the kind of editing back and forth although in general I think there are things that the the tech platforms could potentially learn from Wikipedia as well. Okay and can you pass the catch box forward to the second row? I'll go quick I just wanted to like I was just recently reading a paper about moderation techniques on Wikipedia specifically and I think a lot of the community supported moderation techniques that they have allows for stock gates for is specifically what you're referring to. Yeah and they do like I know like I know Wikipedia like does have ways of freezing posts that are kind of very contentious although I also Casey Fiesler who I mentioned earlier who's fantastic did have a thread recently about kind of her her experience of creating Wikipedia posts and then having people like getting upset and attacking all her posts and going through her revision history so yeah I know it's a kind of a mixed bag. You pass it back to Rose. So back on the social media train how do you think about reconciling trying to detect like the companies themselves trying to detect this information and like downright get or remove it or whatever with the users expectation that their information isn't necessarily all being scrutinized by the company so like I know for example this would be a huge problem for whatsapp because like one of their big assets is that the end yeah but that makes it impossible to really analyze that yeah that's that is a big issue because yeah there's definitely serious disinformation happening on whatsapp so you may have seen there's kind of a study of memes being shared in the run-up to the Brazilian election in which a lot of kind of misinformation and misleading things were shared and played at least somewhat of a role and the far-right leader being elected and there's also been this issue in India of people spreading rumors on whatsapp and several people have been murdered as a result yeah it's hard I don't have an answer in a in our last minute of class but yeah if I think of more I'll say more but there is there is I guess in general I will say so yeah thinking about the importance of privacy I guess whatsapp has made some changes though and just of how how many groups you can share something to or I believe like group size so there are still kind of structural changes you can make while protecting protecting privacy of just like how how you let people share things and then I'm sorry we're at 8 o'clock so I'm gonna stop but feel free to either ask next time or to post on the forums

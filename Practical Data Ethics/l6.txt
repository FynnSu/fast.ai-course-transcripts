 All right, so algorithmic colonialism, and actually week six is gonna have two separate parts. So we'll talk some about colonialism, and then I want to talk about kind of next steps and where do we go from here after that. So I had some articles about Facebook's internet.org, which was later rebranded as Free Basics. And this is something like I definitely remember years ago when I first heard about internet.org, assumed it was a nonprofit, it was not. And so in this, the Atlantic article by Adrienne LaFrance, Facebook and the New Colonialism, she cites this kind of blow up on Twitter in 2016, where Marc Andreessen, kind of famous investor said, anti-colonialism has been economically catastrophic for the Indian people for decades. Why stop now? So kind of very offensive remark, he later deleted this tweet, and this was in response to India at the time had voted to not allow Facebook's Free Basics, since it was a violation of net neutrality. And the idea of Facebook's Free Basics is kind of creating this walled garden of people could access the internet for free. That's all in quotation marks because it was they could access Facebook and a select list of kind of whitelisted sites in which US corporations were overrepresented, but they would have to pay to access other sites. And so for people that couldn't afford data, you know, this is really kind of steering them into what websites they will visit. And many of them were, you know, accessing the web through Facebook as their portal. And actually, I forgot to include this, but there have been studies in kind of in many many countries in the global south of people use Facebook and the internet interchangeably, because when they're accessing the internet, it is through Facebook. But that means there are a lot of kind of design decisions and priorities from Facebook that influence kind of what and how they access sites. And this was actually part of Facebook's business development group. So the idea was kind of like, oh, this potentially will lead us to more customers later. But it was pitched as, you know, we're doing something nice that we can probably profit off of later. I mean, I guess another note that comes up a lot is that in practice, it I think often is not necessarily well, it varies from country to country, but in many cases, the people that are using it are kind of paying for internet soon after. So it isn't necessarily as much of a gateway as it kind of first suggested. So in the article, they quote an English professor at Emory saying, I'm loath to toss around words like colonialism, but it's hard to ignore the family resemblances. And they listed some similarities about using words like equality, democracy, basic rights, masking the long term profit motive, justifying the logic of partial dissemination is better than nothing. So here, you know, even though it's not giving people access to the full internet, and it's just this shorter list of sites, isn't that better than nothing? Partner with local elites, invested interest, and accuse critics of ingratitude. And then kind of one case study is, so Free Basics was, or is present in the Philippines, and Facebook played kind of a very crucial role to the election of Duarte there. And this article, this is a pretty kind of thorough article on it. The platform is a leading provider of news and information, and it was a key engine behind the wave of populist anger that carried Duarte all the way to the presidency, where he's been kind of committing a number of extrajudicial killings in the name of kind of a drug war. Some Filipinos say Facebook treats the Philippines as an absentee landlord might. And so it's been, and again, there was, you know, Facebook, and I think they did this for many political campaigns where, you know, they offered kind of help with the campaign, but on the whole has not been very involved in some of the negative issues or disinformation campaigns in addressing them. Any questions or thoughts? Another, and this is a keynote that Sarita Amrut, who's a director of research at Data and Society, gave on tech colonialism, kind of giving another definition of kind of how colonialism is defined and saying that a colonial relationship ultimately is hierarchical, extractive and exploitative. And I thought those three adjectives were very helpful to think about, and in thinking about tech and kind of what is the relationship between tech companies and the people confusing technology and in which cases is it hierarchical, extractive, and or exploitative. All right, another assigned reading was this one by Abebe Berhane, The Algorithmic Colonization of Africa. And she writes, the discourse around data mining and a data-rich continent, common language within Africa's tech scene, shows the extent to which the individual behind each data point remains inconsequential from their perspective. And so kind of focusing, you know, mining this process that's usually about, about kind of hard materials, not about people. The discourse of mining people for data is reminiscent of the colonizer attitude that declares humans as raw material for the taking. There was an article on Huawei technicians helping African governments spy on political opponents. So in Uganda and Zambia, the government was able to contact Huawei to intercept encrypted communications to track opponents. There was a Wall Street Journal investigation on this, and I think this is incredibly concerning. And this was a map from it of kind of how many, how many countries have surveillance projects. And again, you're kind of, I think, getting here this weird corporate public partnership that often kind of this is not well, not well defined to include accountability. Raman Chowdhury talked about this in her talk on algorithmic colonialism. She said, a lot of this technology lends itself towards a centralization of power. Efficiency seems to dominate everything, but efficiency actually pushes you towards centralization. And this reminded me a bit of the Politics of Artifacts article again, I was at a hand or Politics of Artifacts article of, yeah, this idea of are there technologies that lend themselves more to kind of centralization versus, versus distribution. But right now there is, I think, a lot of focus on efficiency, which has, has some downsides. Another quote from her talk, and this talk is available kind of completely as a transcript and I believe I linked to it in the optional reading. Good governance means systems of accountability, the privatization of civic digital infrastructure is dangerous because we don't have the same level of redress with private companies that we do with government. Okay. So one question was, are there good counter examples for tech colonialism? And there was in the Samrita Amrut keynote at the end, she does cite some examples, I think kind of positive examples of local communities building technology. And I'll even maybe I'll pull that up during the break as kind of yeah, as a positive counter example. That's a good question, Amalia. And then we have a comment, oh, yeah, a great comment from Aaron, that regarding the Deregite presidency, we also have to remember that in the historical context, the Philippines was an American colony. And that's, I think, true of many of these that, you know, we're talking about kind of this historical definition of colonialism, and then, you know, this kind of more modern technical or data colonialism, but these things are not completely separate, and there's certainly overlap and relationship there. All right, then another article that I thought interesting was this one from Nature, contracting people through phone call data improve their lives. Researchers have analyzed anonymous phone records of tens of millions of people in low income countries. Critics question whether the benefits outweigh the risk. So for example, the UN's World Food Program analyzed anonymized call records to find out where people needed food or cash assistance after an earthquake in Nepal in 2015. In 2012, researchers studied records from nearly 15 million mobile phone subscribers in Kenya and quantified the seasonal migrations of people who travel to work on tea plantations northeast of Lake Victoria, where malaria is a problem. And so I think this, some of these I think are probably kind of harder projects to evaluate. You know, are these kind of positive or negative? And I thought this was a very thoughtful article, kind of exploring what are some of the downsides of, I think particularly, I know in the past I have not always been critical enough around data for good projects or the idea of the kind of the positive potential there, but recognizing the risk and downsides. And so here they give kind of an overview of many of the countries that have had various, and this is where kind of cell phone data is collected from the carrier. It is technically anonymized, although as we've kind of seen previously, you know, their concerns of is data really ever anonymized? Can it be de-anonymized? So for a few examples, in Turkey, international teams tracked Syrian refugees. In Nepal, they tracked people after the 2015 earthquake. Sierra Leone, they analyzed call records to examine travel restrictions during the Ebola outbreak of 2014 to 2016, and so some of the concerns are there's a lack of consent here. This is typically, I think, something that people do not have any choice about whether they're opting in or not. There's the potential for breaches of privacy, even from anonymized data sets. There's the possibility of misuse by a commercial or government entities interested in surveillance, and then the kind of this question of are the results even being used? In many cases, kind of the, you know, the conclusions may be interesting, but are they actually kind of making their way into policy or structural changes, and are there other ways to gather this data? Because in many cases also the data necessarily, I think they give one example of, you know, discovering that refugees were isolated, which, you know, perhaps they didn't need all these phone records to discover. Would there be ways to kind of survey people in a way where consent is clearer and more upfront? Are there any kind of thoughts or reactions to this article? Ali? Yeah. When I was sort of thinking about this, it sparked this thought about, so like back in my research group, we have this database of all the people that have been affiliated with the group, and I realized when I was digging around in the database that for some reason there was a column of gender, and it sort of raised all of these kinds of questions, which were like gender identity is a really personal thing that's maybe not totally relevant to the research group, and so it's questionable why we even have that column. It was like generally empty for everybody, but still the implication was that we wanted to collect that data for a reason, and like are the results even being used? There could be an argument that we could be using that if we collected it to think about whether we have gender parity in the group or what have you, but we weren't doing that, so it was sort of just like performatively sort of setting ourselves up to collect data with no real plan to use it in a useful way, or in a constructive way. Yeah, so it sort of raised all these questions of like, okay, well maybe vaguely you can just search for something useful or positive, but this isn't actually happening that way, and if somebody's like non-binary or something, then that information in particular getting sort of like leaked could be, it could endanger them in a really serious way if they're traveling to another country or what have you. So it just raises all of these issues, and also costs us a lot of energy and effort to maintain this information, to say nothing of like, we're not using it for any constructive positive use, so what was the point of all this effort and struggle? But yeah, it really raised this sort of, it reminded me of that issue where it was like, there's all of this stuff going on, why are we doing all of this? Cool, thank you. That's a great example. I should highlight something else they talked about in the article was in the example of malaria, where they were tracking the kind of migration of people to an area where malaria was a bigger risk. Some people that work on malaria pointed out like, we already have all these interventions that we know we work, like mosquito nets and pesticide that aren't necessarily getting enough support or funding, you know, do we need kind of this fancy data project, like is this yielding results compared to things that may seem kind of less exciting but are proven and kind of underfunded? Let me check the group chat. Oh, a lot of comments. All right, so I'll read some of the comments from the group chat, and there's a comment that the live chat is finicky. Without the growth of AI, we wouldn't have Africa's Indaba X, Latin America's Kipu, so there are some kind of great local movements. Someone says, I'm not surprised it's Huawei given the Chinese government effort in monopolizing data. Yeah, sure. Do you want the... I was just thinking about last week on Fresh Air, they did a review of a CIA-backed surveillance company that operated from like into World War II through 2018, and it was a Swiss company that was also backed by the German intelligence group. And so pretty much what they said, and soon it was called cyber, or crypto AG, born directly from the tech that we used to, you know, cryptography in World War II, and it was very similar to what was described in that Huawei thing, it said we weren't like going back and even like Carter during the Egypt and Israeli to go peace negotiations was getting direct information about what the Egyptians thought it was saying, and like it continued all over the years. Oh, wow. Yeah, no, I'll look that up. That's interesting. Yeah, it was just one of those things like 2018, very recent. Thanks. Yeah, let me keep going. The things Amroot references are in-person talks, community movements, which are great, but I'm wondering if there are any tech companies building with these principles in mind. That is a good question. I don't know of any, like there's no company that I am familiar with and think of as like, oh, they're a great example of anti-colonialism, although if anybody has one, feel free to share and that goes for folks in the group chat. Okay, sure. I have heard folks from Salesforce.org talking quite a bit about when you're creating data sets from your own data about doing direct-to-date, they put in a bunch of cues into, do you really want to include that in your data set? Some of these things are being talked about here, and so I'm not going to work for Salesforce there, but it sounded like they were trying to make strides to do some of these very direct things from the title work side, I don't know if that's what you're talking about. Yeah, and that is true. I know also Salesforce doesn't let people use their databases for facial recognition and I think also for making predictions based on medical data, so they do have some constraints there. Let me keep reading. It is important to note that African startups are also using data in a way similar to Western tech companies, meaning in an opaque way. Regarding counter examples to tech colonialism, Indonesia has done quite well regarding tech. It has been open to foreign tech companies while managing to rein them in much better than Myanmar. Being the fourth largest population in the world and an important growing market, plus being a strongest democracy helps, I guess, Facebook, et cetera, has been used to spread extremism however. Although tracking people through phone call data via endeavors such as data for good for altruistic purposes, it does offer the possibility for governments to then use that technology to squash dissent. And Erin says, I think it was more socially acceptable for Facebook being so widespread as a US company made by the US is seen as superior versus their own companies from the Philippines. Okay, there are a lot of comments. Okay, I may not read them all because I think the most talkative part of the class is all live streaming as opposed to here in person. I think the seeds of the US-UK alignment goes back to this affinity for colonialist, imperialist views of creating more capital driven markets. Regarding data providers was also thinking about telecommunications companies that are the dominant force that allow Facebook or other companies to be like this. That is the case with Facebook's free basics that they typically partner with the kind of local phone company in a country to kind of provide the service. Alignment of telcos and Facebook in developing markets in Asia largely has to do with how the majority of telco subscribers in.ph,.id, et cetera, are prepaid. Disregarding the ethics of a company profiting from international customers without sharing profits in target countries. I think that's a great point. All right, okay, I'm gonna keep going. But yeah, this is good discussion. Another article I wanted to highlight was Sarah Hooker, who is a researcher at AI, but she founded something called Delta Analytics, which is a data for good organization, but she wrote a post on why data for good lacks precision. Data for good says little about the tools being used, the goals of the endeavor, or who we are serving. So I thought that was a helpful point because data for good does get kind of bandied about quite often indiscriminately. Okay, let me pick back up now. So I want to kind of spend the last part of class talking about where we go from here and how we can all kind of continue this work related to data ethics. And so first I'm going to address a few risk or common topics that come up. And one is the risk of ethics washing, which is, this is something I think we've seen both with the green and sustainability movement and the diversity and inclusion movement. And it's tough because I think there are a lot of people that genuinely can care about about data ethics, just like many people, genuinely care about sustainability and diversity inclusion. But these movements can kind of get, well, you know, they can both get co-opted in ways that are harmful. And sometimes I think it's not even an intentional co-opting, but I think there are many people that would kind of say that they care about diversity and inclusion or being green, but in a company when they have to make a hard decision that would cost them money are not able to kind of make that decision or see that objectively. And so kind of as a result, I think we've had a lot more talk about all these topics than we've had in terms of kind of concrete progress, which is not to minimize the progress we've had. But I think kind of looking at both of those can be helpful and seeing what some of the pitfalls that we are experiencing in data ethics and will probably continue to experience. And so in particular, I've looked at, I called it diversity branding, I wrote a post in 2015 where I looked at a lot of research on this, if I could do it again, I would call this diversity washing, but research shows that many diversity programs reduce the number of black women and black men in management and they slightly increase the number of white women in management, but it's, they're not having the kind of desired impact and in fact are making things worse. Diversity structures cause people to be less likely to believe women and people of color. And so this is a, I got a really interesting study where they, they gave participants two different and it was a, it was an actual New York Times article about a class action lawsuit against a company by female employees for discrimination. And in one, one version, they had an extra sentence saying this company was selected by, you know, working mother is a great place to work and the other, they didn't. And then they kind of asked people, how valid do you think the woman's complaints are against the company for discrimination? And if the company had this, you know, accolade from working mother, even though there was kind of no, no detail given about that, people were less likely to believe the woman who had experienced discrimination. And there was a kind of similar study or a similar group where they looked at a company and they kind of added a statement about diversity to its company mission or company value statement and then asked people, do you believe this account of a black employee who was discriminated against and people were less likely to believe it if the, if the company had diversity as one of its values. And so this is, I think, some, a way that this can be very harmful when companies claim to care about something or start kind of branding themselves that way without necessarily kind of doing the work to improve their conditions. And then some forms of unconscious bias training increase bias as well. And so, and this is, none of this is an argument like blanket argument against diversity programs or bias training, but it's that you have to be incredibly thoughtful about it. And I think there are people that are doing it well, but I think there are also people that kind of not doing it well is worse than not doing it at all because it can have the opposite effect and can also kind of inspire a kind of backlash or retaliation if there's not kind of the right structures in place and kind of like full commitment around it. Any questions about this? And so I think there's a kind of a very real risk that we'll see this with, with data ethics issues. But again, I don't want to kind of undermine the people that are doing good work and, and genuinely, genuinely care. All right, so how about ethics principles? So there are a huge number of ethics principles, and I meant to look up the exact number. This was a survey, though, I think of over, over 30 different sets of AI ethics principles. And so I think at this point, it's helpful, and this was interesting to look at kind of the looking at the similarities across different ones, and some of these come from civil societies, some are from governments, from companies, various, also kind of different how they describe it multi-stakeholder kind of different groups. So at this point, though, I do think we probably have too many different sets of ethics principles. I don't know that anyone needs to kind of go create their own, their own set. And I think there is, you know, there's some potential of how these can be used to potentially create a system for kind of accountability or how you even discuss decision making. There are downsides as well, though. And there was a really interesting paper by this group in AIES a year or two ago, and they argue that many of the principles proposed in AI ethics are too broad to be useful, rather than representing the outcome of a meaningful debate on how AI should be developed, they may even postpone it. And I think this is a very, very valid concern, but I think when we have kind of, you know, broad principles like, let me look at some of the ones here, you know, promoting human values, being responsible, fairness, accountability. Most people are not going to say that they're against these things, but then there are all sorts of kind of justifications that, you know, this hasn't necessarily determined, you know, should, should law enforcement be using facial recognition or kind of a more concrete question, and that if we kind of spend too long on kind of like the high level or these kind of broader, broader principles, we may be postponing kind of the nitty-gritty discussion of what does this mean in practice on kind of particular uses and whether they should be allowed or not. And so I thought that was kind of a good warning, and this is, this is an interesting paper to read. Actually, let me just check the group chat. So there was a question, did I say that diversity and inclusion training can increase conflict or not work? And so what I was saying is that in some cases it has the opposite impact from desired, which is kind of even worse than not working. And there was also, I should have, I should have shared it, Wyvonne Hutchinson, who does, is kind of founder of a, she's a former international human rights lawyer and founder of a kind of diversity and inclusion consulting group, had a really great thread on situations where she refuses to do diversity inclusion training because she thinks it would make the situation worse. And that, I will find that and post that in the forums because that was really fantastic on this topic of when D&I training doesn't work. And she gave an example of a police force where there was kind of well-documented issues with racism and somebody did kind of racial unconscious bias training and she points out that a, their issues weren't even about unconscious bias, they're about kind of very blatant racism. Also that there was no change to kind of the power structures, there was no accountability for the people involved, and in that sort of scenario there was actually a lot of vitriol against the person that did the training, that that kind of, that person had been set up to fail and that that was a situation where it really was kind of just going to increase resentment and not actually address their underlying problems. So I'll find that thread and link to it because that was, that was great. Back to principles, let's get to a case study. And so this is adapted from Chris Wiggins and Matt Jones and so they have mentioned this previously, they have a course at Columbia called Data Past, Present, and Future. And Chris Wiggins is a applied math professor and chief data scientist of the New York Times and then Chris Jones, or Matt Jones is a history professor. And so kind of one of the incidents as they look at is the Tuskegee syphilis trial which is just terrible. So this was something that went on for 40 years of just observing untreated syphilis in African-American men, even though penicillin was discovered as an effective treatment in the 40s, participants were not even told that they had syphilis and they were not given kind of this well-known effective treatment. And then 1972 a whistleblower contacted the press in 1974 in response, so there was kind of been a big public outcry about just kind of how horribly unethical this had been. Congress passed the National Research Act, which led to the Belmont principles. And so, and this is kind of the basis for IRBs, institutional review boards at universities now. And so this I think is a very, and IRBs are not perfect, but they have, they're definitely a big improvement on not having them. And this was a really kind of effective I think implementation of moving from principles to a format of accountability. And something that Chris highlights is the reason that this worked is because universities were reliant on federal funding and by having the funding tied to kind of following these principles and having an IRB, that that lever of power is what made this possible. So it's not something that you could just implement in kind of any situation, you need to have the lever of power and accountability to make something like this work. This is kind of a positive case study though of people and people spent years kind of coming up with, you know, what should the principles of ethical medical research be and then what did those look like in practice. And then this has been very well studied of kind of how well is this working when it's been operationalized. And so I offer this is a bit of a successful case study of kind of something terribly unethical, but then kind of people responding. Any thoughts or comments on this? And then I'm linking to their course again, all their materials are online if you wanted to to check out more about about this or other other kind of incidences and advances in history and how they how they related to data and power. Let me check the the talkative group in the chat, so some discussion about D&I training not working. I agree with this, D&I can be used to paper over real ongoing problems with bad actors no one wants to confront. This is a great comment, D&I training centers privileged folk folks experience and discussions and as a person of color it can be precarious to navigate effectively without casting yourself in a negative light or getting saddled with extra unpaid work fixing the problem you bring up in discussion and there is an agreement on that. Yes, I think those are very very valid critiques of of D&I training and I think there are people out there that do it well but you have to also I think really have the people in power on board and be interested in making structural changes and kind of very thoughtful about it. And another comment even saying that diversity and inclusion isn't working as a person of color makes one seem like a cantankerous foil sport. Yeah, so thank you for that everyone on the group chat. Oh and then I was going to note I did a post on the fast.ai blog that I where I interviewed Chris Wiggins and he talked talked some more about this. Kind of another example of moving towards kind of ethical principles and rights would be the Universal Declaration of Human Rights which was adopted in 1948 by the the United Nations and I should note this is not universal this was kind of just the countries that were in the UN 48 of the 50 countries 58 countries in the UN at the time voted in favor none voted against but this has often been elaborated in international treaties and some national constitutions so this is kind of an example of one way of kind of trying to put rights into a format and it's something I think particularly in Europe you know if there's a country that's seen as not or you know seen as violating human rights can be brought up in kind of some international courts. Any any questions or thoughts? Oh Claudia, this one I want to pass the catchbox back. Cool, thank you. Yeah I mean I think it's it's hard that's a great question yeah so when will this be implemented for data can it be I'll say more about this in a moment I do think we're in the kind of earlier phases of even deciding like what are how do we properly kind of frame the the rights that we're trying to protect and then there is also the issue of getting getting the buy-in from countries and kind of the levers of power of how do you go about enforcing this and so like the the Declaration of Human Rights really you know grew very much out of kind of World War II and post-World War II conditions of even to kind of reach this sort of agreement on on what sort of rights do we want to protect and codify so I do think we have a ways to go with with data. But I guess I'm just thinking we need to get to that level before implementing some regulations. Oh no yeah I think we could implement regulations like I think individual country or even individual states like we're seeing with and someone brought up CCPA the California in California I was can't even remember what oh you had like downloaded a new app and there was a screen and it was like you know this app is going to track you unless you live in California you can opt out and that was like oh this is such a you know this is like nice to have this concrete moment of like I can and it was still kind of annoying to like you go get your device ID that you can enter and submit this form so it is something that I think like states can implement kind of particular localities in terms of and this is in particular talking about the the arm of regulation which is just one way of addressing data ethics although I do think it's a particularly important way of addressing data ethics yeah I don't have a good answer for that that is a big question that yeah people particularly it's kind of a risk needs to seem I think immediate and to be impacting elites often to be taken seriously as a risk yeah I'm gonna keep going but yeah those are I mean those are yeah valid questions that I think don't have don't have easy answers and let me check the the group chat to see if someone else's okay a lot of comments okay so going back to the DNI discussion someone's saying that they have gotten heat for being a part of affinity groups like Latinx and AI or women WIML which is women in machine learning and yeah I'm gonna I appreciate all the discussion about diversity inclusion I'm gonna move on with the lecture because I wanted I wanted to spend time for kind of all of you to think about what what will you do next and if there are kind of concrete things that you've taken from this course or just are not even necessarily from this course but kind of where you where you want to move next and continuing with this and there's a few suggestions and so there's a post I wrote in 2018 on AI ethics resources and a few kind of potential suggestions I have and I'll give you time in a moment to share others if you have them one is to join or start a reading group or meetup around this topic also I like to highlight that I think we need ethics and kind of all workplaces in all roles you can start a lunch group at your workplace but it's not necessarily that you have to be in a role that has you know ethics in the title to be doing ethical work and in fact as we see with DNI diversity inclusion sometimes that's a limitation of when companies have you know some sort of chief diversity officer that ends up just being for show where they they don't have any power but we do we really need kind of ethics in all on all roles there in my post I list to link to some formal programs this is very much an incomplete list but just things to know about the Aspen tech policy hub that's here in San Francisco and it's like a kind of like three month boot camp geared towards people that work in tech but are interested in getting involved with policy and they kind of develop a white paper or draft law on a topic of their interest as part of as part of it and I think it seems like a really neat program it just began in the last year but I went to their they're kind of equivalent of demo day where you know graduates are all kind of people who are mid career in tech spoke about the the problems they've been working on there's tech Congress IO which is this has been around for like six or seven years founded by Travis Moore with the goal of putting again people that are mid career in tech getting them working as congressional staffers and I think he may be planning to try to so that is kind of based in DC but I think he's planning to start a local program centered around kind of local or state government as well but they're the idea is that they're kind of not not enough people in Congress have the technical know-how to understand a lot of the issues we're facing and we really also kind of need people that aren't lobbyists kind of representing technical technical knowledge the Harvard Berkman Klein Center has a number of programs in particular I think assembly sounds like an interesting program and that's something where I think you do two weeks are in person but then you're mostly working remotely with a team kind of part-time on a project Mozilla media fellowships and Mozilla I think funds a lot of great and relevant work to this area the Knight Foundation tends to have more of a journalism focus but they have a lot of interesting projects that kind of the intersection of tech and journalism and also journalism and disinformation and so definitely kind of look at look at what they're funding but I wanted to kind of share those as we've this up if you want to take photos as some kind of particular resources but again you don't have to kind of be in some sort of formal program or formal role involving ethics to be incorporating ethics into your work let me check the group chat and definitely everyone who's mentioned kind of interesting studies around diversity and inclusion please post these on the forums because I would love love to see them okay some kind of mentions of Zeynep Tufekci, Masij Cichlowski and Cathy O'Neill as being favorite authors yeah and I definitely kind of everyone that I included in the syllabus of course I like their work and encourage you to kind of follow follow them and read more of it so then I went back through and actually first I'll give a time also does anyone have any other kind of steps they're thinking of taking or or things they would like to do in the workplace based on this okay I'll keep going feel free to share some later if you think of them I went back through and so it in in all of the lessons I've tried to include some I usually call them steps towards solutions because I do realize that I don't think I'm offering something kind of concrete enough to be a solution usually but what I think are kind of positive steps and so I went back and tried to just pull those from previous lessons so on the topic of disinformation I had encouraged people to to practice good social media habits and I had a link to Mike Caulfield's work which has been great and he's actually started a Twitter account just about COVID-19 and searching for disinformation and how to evaluate the information you're seeing keeping perspective strengthening our institutions such as journalism education universities and nonpartisan government departments treating disinformation as a cybersecurity problem and developing verification tools I offered some steps towards doing better on bias such as analyzing a project at work or school and in particular I wanted to highlight data sheets for data sets and model cards for model reporting as two potential ways to kind of record information about your your data set or your model working with domain experts and those impacted increasing diversity in your workplace advocating for good policy on privacy I talked about kind of the importance of the meaningful incredible financial penalties and actually motivating companies looking at history and some of the kind of big regulatory battles that people have fought in the past around issues such as kind of environmental pollution or car safety moving towards a frame of treating privacy as a public good and regulating data collection and political ads I shared the same up to fact she posed from 2018 where she lays out and this was at the time of the congressional hearings where Zuckerberg was being questioned and she says you know we don't need to ask him any questions we already know enough about Facebook and how they behave and give some recommendations about what we should be doing I wrote a post on four principles for responsible government use of technology with some of the kind of themes from our tech policy workshop that we here had here at the Center for Applied Data Ethics and then this other digital political ethics came from a report around I think political advertising online yeah and so I wanted to kind of encourage you to and I know I asked this is one of the quiz questions but to think about one thing you're taking away from the course or one thing you would like to do or have already begun in response to the course I'll give time if anyone wants to share all right and that's awesome yeah keep me posted on how that goes it's cool any others check the chatty online group let me see okay so I'll read a few of the comments from the online group just by taking the course opens the opportunity to talk about the topic with friends and co-workers creating space to reflect about the topic that's fantastic yeah I'm happy to hear that I'm hiring hiring more my minorities consulting and data ethics I will be implementing some of these things in my onboarding program at work that's that's great to hear I'll be educating VCs on what to know that's fantastic when to use some approach that avoids the stigma of program mandates but gives interviewers tools for thinking about hiring in some representative fashion I just want to highlight and I think I mentioned this in a previous week but I wrote a post on called how to make tech interviews less terrible and that I looked through kind of a lot of the research around interviews and hiring and make some recommendations they are in the context of the how much bias there is in the process but also that kind of tech interviews are terrible for pretty much everyone regardless and they are a hard problem but I give kind of several recommendations there so that's a potential resource side note I'm actively working on helping with increased and more accurate data collection on coronavirus in the Philippines lots of people who are misreporting under reporting on data related to it and that's a great thing to be doing oh I like this one find people with roles in the company that are closest to the channels that users use to complain to find ways for people listening to issues to take more meaningful actions I really like that in a previous week I quoted an Alex fierce post about kind of trying to have trust and safety better integrated with product and engineering but along those lines of kind of having the the complaints and ways that products misfunction making sure that kind of information is getting back more directly to the people that are designing and building products whoops the importance of privacy as a human right and talking about AI I'm glad to hear that message got through and going to pursue AI policy research roles it's great Oh PhD in cybersecurity a lot law and about to launch a consulting firm and data protection compliance data ethics and privacy law how data shouldn't be monopolized question I've been cold calling the 50 states and talking with public and private sector committees they're quite lost and do you have any key go-to resources for state-level policy makers that's a good question and I am not sure yes I don't feel that I have kind of go-to resources on this although I do feel like groups are writing them does anyone else want to chime in or or share resources we can also start a thread around this okay yeah thank you yeah and I know I'm I know pa pa I partnership on AI does a lot of reports they are such a broad umbrella that I would want to look at the individual report to kind of have a sense of it but they do have some interesting people working there well thank you yeah thanks for everyone on the live stream those are some great comments also thanks to everyone in the classroom here in person okay and so then I wanted to share a quote that I've been thinking about and so I I realized I you know I feel somewhat dissatisfied with the steps that I offer as towards solutions like I would like to have something kind of more concrete and comprehensive of like this is this is what we do and I sometimes talk to people that are I think dissatisfied or you know they're like you know give me something I can you know a checklist that I can follow and you know know that it's everything's ethical then and so something I found really and that does not exist right now I should say to be clear you know these are really complicated problems and it's also kind of newer that they're being studied and talked about in the way that they are even though you know there are fields that have been looking at related work for longer but just kind of seeing the impact of some of these new technologies is a newer area and so this is a quote from Julia Angwin who was a senior reporter at ProPublica she was one of the investigators on ProPublica's investigation of the compass recidivism algorithm in 2016 which really kind of helped kick off the fairness accountability and transparency field and she's now the chief chief editor at the markup and she she said in an interview last year I strongly believe that in order to solve a problem you have to diagnose it and that we're still in the diagnosis phase of this if you think about the century the turn of the century and industrialization we had I don't know 30 years of child labor unlimited work hours terrible working conditions and it took a lot of journalists muckraking and advocacy to diagnose the problem and have some understanding of what it was and then the activism to get lost changed I see my role as trying to make as clear as possible what the downsides are and diagnosing them really accurately so that they can be solvable that's hard work and lots more people need to be doing it and so I really I think appreciated this work and this recognition or this this quote and kind of this recognition of kind of how complex these problems are and just that the work of continuing to diagnose them is is is valuable and kind of one of the steps towards towards hopefully solution solving them and supporting kind of the the necessary activism to solve them and this comes from an interview called forget about privacy Julia Angwin and Trevor Paklin on our data crisis and I think I think that's all that I have I also just want to thank all of you I've really enjoyed teaching this course this was my first time teaching a course like this I've previously taught kind of much more technical courses and so I was really kind of not sure how it would go to to be doing something more qualitative and more discussion based but I've really enjoyed it and I've really appreciated kind of everyone's comments and how much participation and thought you all have given this so thank you.

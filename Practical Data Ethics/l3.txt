 Okay, it's six o'clock, thanks for being so punctual everyone, so let's get started. I also wanted to highlight that on the forums, I always post after class with links to additional papers that came up during class, so be sure to check that out. So even though it's on the, so like for fairness, last week after class I kind of added a post with even more papers that have been talked about, and I think many of them are interesting. Thanks. So I wanted to start with talking about this article, so I actually don't watch Game of Thrones or The Wire, but I still really enjoyed it as an article, although first I was curious about anyone who does watch the shows, whether it resonated. I see a hand up here. Yes, this is one of my favorite articles so far, and yeah, I think what she, I, I'm not, oh, thank you everyone, I'm glad it resonated with many, and I liked her, her kind of tie in with the tech industry on how so many of our narratives in the tech industry are around these kind of, you know, seemingly larger than life personalities and not as much about necessarily kind of the broader sociological forces that, that influence people, and she lists some of the sociological influences are business models, the advances in technology, the political environment, lack of meaningful regulation, wealth inequality, lack of accountability, geopolitical dynamics, although I do, you know, there is something I think human about liking stories about people, which can then make communication hard. I actually heard people talking this, this weekend about kind of the issue with, even in science, like a lot of kind of getting attention to your science is about crafting it into a good story, but that isn't necessarily kind of the most accurate way of doing science, but that kind of stories resonate with people, although this is, I think, a kind of neat perspective on how you can make stories more about kind of broader forces. The other final, final thoughts, and this, we'll kind of return to this way of thinking a bit in week five when we talk about our kind of broader ecosystem that we're in. I also, I liked the line about well-run societies not needing heroes. So this evening I'm going to be drawing very heavily on resources from the Markula Center for Applied Ethics at Santa Clara University, and this is work done by Shannon Valor, Rina Rekou, and Brian Green, and I definitely recommend their website as kind of having lots of articles and resources, which included several on the syllabus, so you've probably noticed. And so first I want to kind of talk about, so you know in weeks one and two we were looking at these very kind of specific areas of disinformation and bias and fairness, and now I want to kind of step back a little bit and talk about kind of like the underlying kind of what's the foundation for even asking ethical questions, and so there were kind of, this is the article by Shannon Valor, there were three different kind of schools of ethics that she shared, and these kind of go back quite a ways. So one is deontological ethics, which focuses on rights, principles, and duties. These principles can include autonomy, dignity, justice, fairness, transparency, consistency, and more. They may also conflict with one another, so in think of situations where, I don't know, like consistency conflicts with justice or with these other principles. A few examples are the golden rule, not treating people as ends, or sorry, not treating people as means to an end, but considering them ends to themselves. The rights approach, which is when considering options kind of what best respects everyone's rights, or the justice approach, which option best represents people equally or proportionally. And so the reading gave some different questions that you can ask, you know, what rights of others and duties to others must we respect? How might the dignity and autonomy of each stakeholder be impacted? What considerations of trust and of justice are relevant? Does this project involve any conflicting moral duties or conflicting stakeholder rights? How do we prioritize these? And so then I was gonna kind of think about a specific example that maybe we can kind of consider. So do you remember the Unroll Me backlash in 2017? So Unroll Me is a service that would, it said, you know, if you sign up for us, we'll unsubscribe you from all the kind of annoying email list you're on, and it would give you these summaries of like, hey, it seems like you're on these email lists, do you want to unsubscribe? And what people didn't realize is that, so it had access to your email inbox, and it was then selling that data. And this came out in a kind of roundabout way when Uber mentioned, oh yeah, like we know how Lyft is doing because we buy that data from Unroll Me, I mean, I think it actually went through kind of like a company with another name. And so there was a lot of backlash, particularly because it was, I think, kind of catching the Uber backlash as well of, oh my goodness, they're buying email data about how many people are using Lyft. So there were articles on the Unroll Me CEO being heartbroken about their data being sold. And so first I kind of want to pause and ask, how would you kind of look at the story in light of the questions we saw on the previous page, and kind of how this was impacting people? Any thoughts? And in the fourth row, if you can pass the catch box back. Well I don't, I haven't thought about this enough to like know that I fully get behind it, but one thing that like that prompts is, do people have like a right to not have their data sold without their knowledge? Right. Yeah, so there's, I think, a sense of privacy, or maybe this would go under dignity a bit of, you know, is there something kind of, I don't know, that interferes with people's dignity? Actually, let me look at the list of, list of principles. Yeah. So they're questions of kind of, is that violating some sort of right? Right. Now a hand in the first, oh, actually there's a hand, two over, yes. Well, in this example, I mean, it's particularly ironic because the purpose of the users was to extract themselves from being involved with all of these service providers and to turn around. So I think it's, it's like insult to injury, so I think it's even worse, just kind of like as a, at a high level of morals, it's like just kind of sleazy, because I think that they're, they were taking in more trust than maybe another service provider who had that same information, whether or not they had consent to sell it, it seems to be another level. Yeah. So it seems to kind of like violate, maybe this is consistency around, people seem to value, you know, like simplicity, they're not wanting to be contacted by all these providers, and then it's, as you said, kind of even more insulting that their data is being sold. Right. Lauren, on the opposite end of the row. I think from a business perspective, or like when these founders, you know, sit down, everyone gets in a room to brainstorm possible revenue generation, like streams of revenue, I think it would have been a game changer if they didn't consider selling data. And so at what point should these questions be introduced, and like how much weight should they be given, because it could drastically change the potential revenue streams of a company and their ability to even stay profitable, or even like on that path towards profitability. So really good question for business owners. That's true, yes. And this was, Unroll Me was a free service, free service to the users, so they were not paying, but yeah, they were, turns out, giving their data. And then can you pass the catch box forward to the front row? The specific use of the term heartbroken sort of speaks to this lack of understanding, like, oh, this is more than just me, it sort of speaks to that cult of personality type of person, versus like, oh, well, I'm very disappointed that, you know, these like ethical principles that like perhaps were maybe thought of at some board meeting weren't like rolled out, but rather it's like, oh, I'm heartbroken, personal betrayal, versus like the umpteen people who have their data like auctioned off. I feel like it's a bit strange. Yeah, and there was actually, I didn't take the headline from this, but it was like a friend of the co-founder, or someone who had formerly been involved in the company wrote this medium post, kind of justifying why the company, and they weren't even like involved with the company anymore, but kind of very defensive and I think took it very personally. I guess this is also about like transparency, because a lot of these are like free and then people see free software and they get on it and they use it without knowing that the software provider has all the rights to your data as long as you get onto the platform. Yeah, no, exactly, I think transparency is definitely a big issue here. And so, um, actually something that didn't come up, but the company's defense was, you know, if you really read our terms of service, you should have known that you were giving away all your data, and so there was a study, and this is from 2008, that it would take the average American 40 minutes a day to read every privacy policy that they encountered. And so this is something, it's just kind of absurd, we don't all have an extra 40 minutes a day, and if we did, we probably wouldn't want to read privacy policies with that. And it might be even more now, because that was, that was over 10 years ago. And so Casey Fiesler, who I quote a lot, did a study and found that the, and so this was for terms of service, the average reading level was 14.8, which is kind of midway through college for a terms of service policy, and has 3,800 words, which is significant, yet the average person in the US is at an eighth grade reading level. So in many cases, these aren't even, you know, pitched at an appropriate reading level or intended to be kind of readable by the, by the users. And I think this, this also kind of captures the gap between legality and ethics, you know, there's something where I think, you know, companies can use it of like, oh no, like it's, it's fine because they signed this, but these aren't intended to be read. Hi, I'm Rachel Thomas, I'm going to talk about some of the foundations for ethics as well as ethical tools you can use in your workplace, particularly in the tech industry. And this is a continuation from the lecture I started yesterday, unfortunately the recording software malfunctioned partway through, through class, so I'll be picking up here, although it's fine, fine to start with this video, I'll kind of briefly, briefly review. I'll be drawing very heavily on resources from the Markkula Center for Applied Ethics at Santa Clara University. They've done some great work on ethics and technology practice, definitely check out their website, they have conceptual frameworks, kind of guides for decision-making, an ethics toolkit that I'll be covering later, case studies, so, so look at that, it's very, very helpful collection of resources. And so as we talked about previously, we're kind of looking at three, three different ethical theories tonight. One is deontological ethics, which focuses on rights, principles, and duties. These principles can include autonomy, dignity, justice, fairness, transparency, consistency, and more. They may sometimes conflict with one another. We talked earlier about the example of Unroll Me, which was a company that offered a service to people for free, where they would unsubscribe you from email lists, so kind of go through your inbox and see like, hey, you're subscribed to all these email lists or advertisements, do you want us to automatically unroll you, unsubscribe you, and then it eventually came out in 2017 that their business model was selling data from users' email, and this was revealed when an Uber executive mentioned that they had been buying data from Unroll Me about how many people were using Lyft, and many, many users felt very violated by this. And while this was something that may have been legal, many people felt that it was unethical, and from a deontological kind of viewpoint, it would seem to violate principles perhaps of dignity, autonomy, transparency, so that's kind of one, one example of how you can look at this. Deontological ethics also include the rights approach, which option best respects the rights of everyone who has a stake, or the justice approach, which option treats people equally or proportionally. Another kind of school of ethics is consequentialist ethics. If you want to know if an action is ethical, look at its consequences. This includes utilitarianism, which asks, you know, which option will produce the most good and do the least harm, as well as the common good approach, which asked which option best serves the community as a whole, not just some members. And I yesterday in class gave the students an exercise to, and this is modified from Casey Fiesler's Tech Ethics Scavenger Hunt, but to try ranking five different kind of ethics scandals from how they thought a utilitarian would see them as what is worst to least bad, and how a deontologist would see them, again, from worst to least bad. We had a really interesting discussion on that, I think most of which is captured. In the previous video, this is something you can try. It's a little bit absurdist in that ethics is not really about ranking kind of the terribleness of things, but it was useful for highlighting some of the differences between utilitarianism and deontology, and also just kind of how these philosophies would lead people to think about problems. So while consequentialism and deontological ethics both focus on actions, virtue ethics focuses on kind of the person and their character traits, so this is a third school of thought. It highlights the need for people with well habituated virtues of moral character and well cultivated, practically wise moral judgments, and it's kind of about this lifelong process of developing practical wisdom, and kind of the virtue approach asks the question, which option leads me to act as the sort of person that I want to be? Some questions from the Markkula Center guide on this ask, what design habits are we embodying? Are they the habits of excellent designers? Will this project weaken any important human habits, skills, or virtues that are central to human excellence? So will it strengthen any? Will this design or project incentivize any vicious habits in users or other stakeholders? And how confident are we that we'll feel proud to have our names associated with this project in the future? And so these are a few questions you can think about kind of asking around work you're doing. So there was an optional reading called What Would an Avenger Do by Mark D. White, and I have to admit I actually am not that familiar with the Avengers, but I still enjoyed reading this and found it helpful. And it kind of characterized the Iron Man as a utilitarian who is looking to maximize the good. He sometimes is willing to kind of let the ends justify the means in that service. Captain America was classified as a deontological ethicist in terms of kind of having a notion of the right and really adhering to that. And then Thor was seen as an example of virtue ethics and living by a code of honor. I did want to note that kind of all three of these ethical philosophies we've talked about are kind of Western philosophies and that there are many other ethical lenses out there as well as from other cultures. And I recently was reading about New Zealand's algorithmic impact assessment project that they're working on, and one aspect of the project is that they are trying to incorporate a Maori worldview as well. The Maori are the indigenous people in New Zealand, and so then I was kind of doing some reading on the Maori data sovereignty movement and how the Maori view data, which sometimes kind of raises specific concerns about kind of how data is used, what's done with it, and how it impacts their community. And so I don't, I don't feel sufficiently confident that I could explain this accurately and I don't want to misrepresent someone else's culture, I mostly just want to highlight that there are plenty of ethical philosophies outside the West and there are kind of other ethical worldviews to consider. So in summary, the kind of five ethical lenses that we've seen are the rights approach, the justice approach, these are both deontological ethics approaches, then from consequentialism we've seen the utilitarian approach and the common good approach, and then finally from virtue ethics the virtue approach, and so this is a list of questions that the Markula Center Guide provides that you can consider in kind of looking at a project that might be going on in your workplace and trying to answer ethical questions about it and anticipate how it'll impact people, which options may be the best options to take. Also note that the, I know Irina Raku from Markula Center definitely emphasizes that this is something that's best done in a group, it definitely helps to have other people to be discussing this in community, have people who can point out kind of different issues and maybe see things differently from you. All right, so that's kind of in summary just some of kind of what's the underpinnings for even talking about ethics or weighing the ethics of different projects. Next I want to get into some practical tools that you can use in the workplace that you can implement, and as we've talked about earlier having good intentions is not necessarily enough to ensure a good outcome, in fact people can have good intentions and still really miss major ethical issues. So it's helpful, it's helpful to really implement processes and operationalize this in a way to kind of make it part of your routine and make it something that the company is doing regularly. And we're going to go through this ethical toolkit, there are seven tools or practices in it. The first is ethical risk sweeping, and so this is instituting regularly scheduled risk sweeps and so kind of similar to cybersecurity penetration testing, regularly looking for risk, no vulnerability found, while that's a good thing that doesn't mean that you stop or consider it a waste, you keep doing it. And then it's good to assume that you missed some risk initially and so continuing to look. It's also important to reward team members for spotting new ethical risk. I think sometimes raising an ethical risk can be seen as something that if nothing else kind of slows down, slows you down in your speed to get a product to market and that's not always rewarded, but it's important to kind of reward this behavior if you want it to be incentivized. At this point, one student brought up that her friend has been raising ethical issues in his workplace and that he's kind of getting a lot of pushback about it. People are seeing him as as difficult, it's creating friction, and she asked how to deal with this. And so I do want to acknowledge that depending on the dynamics of your workplace and in many workplaces, I think that it can cost you social capital to speak up. People have different amounts of social capital. It also can depend on your seniority, how seriously you're taken around this. So this can create issues. It's not, it's definitely not simple, particularly when your company or your team is not aligned on this being an important thing to do. So I want to acknowledge that and I think that over time it's possible if you're in a workplace where you do experience a lot of friction about bringing up ethical issues, that you ultimately may find it like that's not a great fit or is kind of untenable for you to continue. So I wanted to acknowledge the difficulty of this. If you don't kind of have your whole team on board, ideally, you know, you would have leadership supporting this and have it where there's a kind of more of a more buy-in around the practice. At this point, another student brought up that he previously worked at Epic, the makers of electronic health records, and he said at Epic they have, I'm forgetting the name, but it's a specific job role of people that it's kind of their whole role to investigate concerns that potentially relate to patient safety and if something's going to pose a patient safety risk, that is something to take very seriously and he said it really helped. The company can and should raise risk that they think may may impact a patient safety, but from there then they kind of have these specialists who take that over, that's their job to do so, and that that really helped kind of streamline the process and it was also something where it was fine there to raise an issue that ended up not turning out to be a risk to patient safety. It was better to raise it and investigate and have it turn out to to be alright than to not say anything and so that was that was kind of an interesting, interesting personal experience to hear from from a member of our class. So tool two, tool two is ethical pre-mortems and post-mortems and I had heard of post-mortems before and I think they're at least particularly for kind of a technical failures are a well-established practice in the tech industry. This would be implementing them around ethical failures as well. I thought the idea of a pre-mortem was interesting. Pre-mortems should ask how could this project fail for ethical reasons, what blind spots would lead us into, why would we fail to act, what systems checks or fail safes can we put in place to reduce failure or risk, and so I thought these were interesting questions and one student in the class raised his hand and shared that he had previously done this not even kind of with a an explicit ethical framing but is something that he went through with kind of business development in companies and that for many maybe this would kind of be an entryway into ethics to get them thinking about it and even if it starts as a business prop problem that it's kind of raising, raising ethical risk. And so something that reading about this tool reminded me of was the kind of school of thought around professors that are using science fiction to teach computer science ethics and so this comes from a Wired article that interviewed several different professors and they actually had different philosophies on how they incorporate science fiction into their courses so some of them are using it more of just a way to kind of think about human characteristics and traits to kind of look you know a little ways into the future and see what could go wrong some feel like the kind of having it in a different world can make it give students a distance that makes it easier to analyze and discuss so it was interesting kind of even within the professors that are doing this there are different thoughts on kind of what's what's the best approach it was something that I considered doing for this course I don't think it's going to happen in this course but it's definitely kind of at the back of my mind for for future ones that there was also a nature article where they asked I believe six different science fiction writers for their views on kind of what science fiction can tell us and can lose said although science fiction isn't much use for knowing the future it's underrated as a way of reimagining human humanity in the face of ceaseless change so that's that's one aspect of what you can get from science fiction Casey Fiesler who I mentioned a lot and deeply admire does a black mirror exercise with her students so she actually assigns students to watch an episode of black mirror and then does a project called black mirror writer's room I got to participate in this at a workshop or she she led a session of it and the idea is to kind of get students writing their own episodes of black mirror she says that speculation is a skill that we have to practice and develop and that kind of yeah thinking of your own black mirror episode is one way to practice this skill of speculation and she thinks it's helpful to kind of keep things in the somewhat near future where we can see them as an extension of our current technology and she's kind of written about about how she implements this in the classroom and so I thought this was this was interesting as a kind of extension of the the primordem idea all right tool three tool three is expanding the ethical circle so there's several kind of several ways that teams can can fail to to see risk one is group think which is when you have a very close-knit group people may start kind of thinking thinking similarly due to the dynamics of the group and then as a result may have particular blind spots and things that they miss another thing that can occur is the bubble mentality and this is when you have a lack of sufficient diversity in your group and again that can lead to people that kind of have very similar worldviews and may miss particular risk and miss the interest of key stakeholders that aren't represented there's also the Friedman fallacy which is a fallacy saying that companies are morally obligated only to maximize shareholder profit even if it's very harmful to the public or that stuff that is harmful to the environment or elicits public outcry people sometimes try to use this to justify deliberate or reckless disregard of legitimate moral interest however it is a fallacy this is not the case and the public typically does not respond well to to this kind of reckless behavior so some questions you can ask for expanding the ethical circle whose interests desires skills experiences and values have we simply assumed rather than actually consulted who are all the stakeholders will be who will be directly affected by our product how have their interests been protected how do we know what their interests really are have we asked which groups and individuals will be indirectly affected and who might use this product that we didn't expect to use or for purposes that we didn't initially intend later in the class I asked people to kind of reflect on what they thought some of the most useful of the of the tools we are and several people highlighted this as something that they thought was crucial crucial to do in a particularly important tool for the workplace this made me think of some research from the University of Washington tech policy lab by Meg Young et al and so there they did a project called diverse voices that is around how to create kind of panels of people from different communities I think to kind of gather them in a systematic way pay them for their expertise and elicit their feedback around proposed tech policy in this case this could also be used though for considering different different products or business practices and so they did two case studies they had an augmented reality white paper and they convened an expert or several expert panels including with people with disabilities people who are formerly are currently incarcerated and women and then they did a separate one on autonomous vehicles strategy document holding expert panels with youth with non-car drivers and with people with extremely low incomes and so this is great to check out there's an academic paper about it and then there's also on the website there's a kind of practical guide kind of leading you through how you could do something like this as well so this is a resource I wanted wanted you to know about all right tool for case-based analysis so identify similar or paradigm cases that mirror the present case identify relevant parallels between or differences among all the cases so even if a case is not exactly what you're currently working on in the workplace it still may be something you can learn from and it's helpful to be explicit about how it does parallel what you're doing how it's different and then that can help you identify kind of solutions or risk mitigation strategies and at this point one student raised their hand and shared about harvard has started or so you know there's the harvard business review that often has business case studies a harvard data science review has been started as well and that can be a good source for data science reviews although that's just within the last year and is still kind of getting getting up and going i kind of been thinking about looking at past cases and this might be going a little bit further into the past than the the authors of the toolkit met but i thought about the course taught at columbia data past present and future so this is co-taught by matt jones who's a history professor and chris wiggins who's an applied math professor as well as the chief data scientist for the new york times and i think this is a fantastic idea for a course this is i believe they're on their fourth year teaching it so it's really been refined we'll be putting a book out about it next year all the materials are available online so definitely check them out in this course while it involves coding labs i believe it was or i know that it's open to students kind of from across uh humanities as well as social sciences and and natural sciences and what they do is kind of go through a history of data and particularly how new kind of new discoveries and innovations related to data have reconfigured power there's a lot of kind of dark history something i didn't know is that regression was first used to do race science so that was kind of part of why regression was invented was it was doing race science which is terrible and they kind of go through though this history that can really help help us understand kind of our current state and also kind of gaining those tools at looking at this reconfiguration of power so definitely check out their materials online if this interests you all right tool five remembering the ethical benefits of creative work and so i think this comes up because ethics can sometimes seem focused on the negatives and you know what terrible things can go can happen what can go wrong but to remember that hopefully we're also trying to do to do good with our work and to look at the positives as well and to remember you know hopefully we are working towards what we see as a greater good are there ways that we can genuinely you know trying to help others through our work as opposed to generating kind of inauthentic needs or manufactured desires okay tool six is another one that really resonates with me and that's think about the terrible people so asking who will want to abuse steal misinterpret hack destroy or weaponize what we build so this is kind of people are going to use your your products and tools in ways that you really didn't anticipate but can you try to start anticipating those who will use use it with alarming stupidity or irrationality what rewards incentives openings has our design inadvertently created for these people or for those people and how can we remove those rewards and at this point one student shared about good hearts law which states that kind of any when the measure becomes the target it ceases to be a good measure and that whenever you have kind of rewards or incentives people will try to gain that and you'll get unexpected consequences we're going to talk more about this in lesson five of this class I wrote a blog post this fall called the problem with metrics is a big problem with AI and that talks about some of the harms that arise when we overemphasize metrics great and then tool seven is closing the loop making making sure you have channels to get feedback and to iterate remember that this is never a finished task identify feedback channels that will deliver reliable data on ethical impact so if you remember back to lesson one we talked about the healthcare algorithm that was healthcare software that was implemented in Arkansas to determine people's Medicaid benefits there was a bug in it it cut off care that people needed people with cerebral palsy in particular and there was no kind of feedback channel in place to even surface this air other than having to go through like a very formal court case and so this is something you really want to make sure that you have channels to receive face feedback and chains of responsibility as well and so it kind of tools says six and seven reminded me of this post by Alex fierce who was previously the chief legal officer in media and he interviewed I think I was around 15 people that work in trust and safety many of them have been working in trust and safety for years across a range of companies including many of the major tech platforms trust and safety includes content moderation Alex described trust and safety is both the the judges and the janitors of the internet and something that one of the people he interviewed said that struck me was the separation of product people and trust people worries me because in a world where product managers and engineers and visionaries cared about this stuff it would be baked into how things get built if things stay this way the product and engineering are Mozart and everyone else is Alfred the butler the big stuff is not going to change and this is true at many companies they're often kind of siloed you have product and engineering over here and people dealing with trust and safety with abuse that happens on the platform kind of harassment bad actors are kind of totally siloed off in another place there's very important feedback that's not not getting back to the the people that are building the products and so someone else in the article and most of the people in the article were using pseudonyms talked about kind of a company where executives were having to spend some time kind of shadowing people in trust and safety just to see what sort of abuse arises how are people weaponizing the platform which sounded like a promising approach so I thought this was this was an interesting one so now there's a topic I want to talk more about that relates to I guess particularly to tool three around expanding the ethical circle and that's the lack of diversity in tech and particularly in AI less than or only 12% of machine learning researchers are women so this is kind of even worse than the tech industry in general and the statistics are similarly dire when it comes to race to age to other demographic characteristics so we have a very kind of homogeneous group of people building really powerful technology that is impacting pretty much everyone kind of an example of the positive of having a more diverse team Tracy Chow was the fourth employee at Quora as well as an early engineer at Pinterest and she wrote how the first feature she built when she worked at Quora was the block button and she wrote I was eager to work on the feature because I personally felt antagonized and abused on the site gender isn't an unlikely reason as to why and if she had not been there and advocated for this they probably wouldn't have added a block block button till later on so this is kind of one example of of the kind of positive aspect of having a diverse team so I I went through a period where I became very kind of discouraged and disillusioned in the tech industry I I was in my early 30s at the time and had been kind of focused on math and computer science since I was a teenager but I was just miserable kind of largely due to the toxic culture and so I hired a career counselor I retook the GREs because it had been 10 years and I was really thinking though like what am I gonna do I just can't can't see myself continuing to do this I wrote a post about my experience and about a lot of the research I ended up doing called if you think women in tech is just a pipeline problem you haven't been paying attention and so kind of one key statistic I want everybody to know is that 41% of women working in tech end up leaving the field compared to just 17% of men this is a very very high number and kind of no matter how many girls you teach to code it's not going to solve the diversity issues in tech if women continue to leave leave at such a high rate meta-analysis of 200 articles white papers books found that women leave the tech industry because they're treated unfairly underpaid less likely to be fast tracked than their male colleagues and unable to advance and so I really encourage everyone if you're interested in diversity to focus on the opposite end of the pipeline from what people normally talk about which is the workplace and making sure that the women and people of color in your workplace now are treated well that you can retain them that they have opportunities to advance oops unfortunately often diversity efforts end up focusing primarily on white women which is wrong women of color are facing many additional obstacles and barriers and it's kind of even more important to focus on them so I that kind of dug into the research on why are women getting fewer fewer chances to advance some of some of the research around that is there's a study that found that men's voices are perceived as more persuasive fact-based and logical than women's voices even when reading identical scripts researchers found that women receive more vague feedback and personality criticism and performance evaluations which is not so helpful whereas men receive actionable advice tied to business outcomes and then when women receive mentorship it's often advice on how they should change and gain more self-knowledge when men receive mentorship it's often public endorsement of their authority and so perhaps not surprisingly mentorship for women has not been linked to getting a promotion whereas mentorship for men has been linked to getting a promotion women also experience being excluded from more creative and innovative roles not receiving high visibility stretch assignments which are also often useful for advancing and being channeled into less rewarded execution roles and so I have links to all this research as well as more in my post the real reason women quit tech and how to address it and then just another aspect of this that I've thought a lot about is how to make tech interviews a little less awful the interview process in tech is terrible for everybody right now there are a lot of problems with it I do think that interviewing and hiring are really difficult problems and they are they are tough to get right but two two pieces of research I wanted to share with you one is a company called triple bite what they what they do is this is specifically as kind of a recruiting company for engineers they have engineers take test with them and then they have kind of detailed data on where those engineers interviewed where they got offers where they got rejected and they can compare that because they have this kind of standardized test that they gave gave over 300 engineers the number one finding from triple bites research is that the type of programmers that each company looks for have little to do with what the company needs or does rather they reflect company culture and the backgrounds of the founders and so this is discouraging it's perhaps not surprising people like to hire people like them that triple bite post gives the advice to if you're looking for a job to try to find companies where the founders have a similar background to you but clearly this is going to be much easier for for certain people than for others depending on on your background and on your demographic another another study that I love to tell people about is one where they had people choose between two resumes one had a male name one had a female name and one of the resumes the person had more practical experience and the other they had more impressive academic credentials and so people typically picked the the man as the candidate and then they would say well I picked him because he had more practical experience or I picked him because he had more you know impressive academic credentials and this was they did both possible pairings and so this is an example humans are great at post hoc justifications it's really important to kind of have a formal credentials ahead of time and to know or not credentials but a formal outline of what you're looking for so I linked to those and a bunch more research in my on my post on tech interviews but I do acknowledge it's it is a tough a tough problem and it's very time intensive to try to create a good good interview process all right so in summary we have seen seven practices to implement from the mark Bula Center tech ethics toolkit in our previous lesson we saw data sheets for data sets by Timnit Gebru we also saw model cards for model reporting by Margaret Mitchell at all we also saw the diverse voices that was the framework actually I should say so the idea with data sheets for data sets is you're never going to eliminate bias from your data set but let's at least be explicit about how this data set was created under what constraints what are its limitations and let's kind of kind of to be explicit with that and not just assume it's kind of some sort of universal ground truth diverse voices was the work from the University of Washington tech policy lab about how to create expert panels with people from that kind of various you know various stakeholders such as formerly incarcerated people who don't have cars to get their feedback about about a paper or project and the post that interviewed 15 former or not former and current trust and safety employees and this idea of integrating trust and safety more closely with product and eng and then these tasks of retaining and promoting people from underrepresented groups and overhauling the interview process and so I kind of encouraged everyone in the class and I encourage you to ask which of these tools or practices sounds most helpful to you and then also which do you think would be the most realistic to implement and so kind of looking back at this many people thought tool three seemed particularly crucial although they also thought that was one of I think the harder ones to implement but definitely definitely think about this and think about it you know is there anything concrete that you can you can take from this kind of back to your workplace and then I want to close now by emphasizing that we need both policy and ethical industry behavior this this lecture has been more focused on ethical behavior in industry and what processes can you implement in a company however that is not going to solve everything and we need policy as well policy is the appropriate tool for addressing things like negative externalities so a negative externality shows up the classic example is a company you know dumping its waste into a river or bay and that influences everyone around them and so they're kind of offloading their their cost to society while reaping in the profits and I think we're right now seeing the tech industry offload a lot of cost to society while they while they make bad profits misaligned economic incentives and this is something that I think even when people are well intentioned if it is really profitable to do something that is is bad for society there's a kind of a misalignment there and policy is the appropriate tool for for addressing that also race to the bottom situation sometimes there's something kind of really ethical and even yeah you know even if you can convince your company not to do it which is great please do that there's still other companies that are going to do it you kind of get sometimes these kind of like worst common denominator situations again I think you need policy for that as well as for enforcing accountability kind of enforcing meaningful and significant penalties for companies that that do wrong and harm people however policy would not be sufficient on its own either because the law is not always going to keep up with new technology the law is also not always specific enough to kind of capture every every nuance or every edge case and so for this reason it's important to have ethical practitioners in industry as well so I just wanted to highlight that while while we were focused on kind of what you can do kind of assuming you're in a industry workplace I also believe policy is necessary that's something we'll talk more about in week five that was also I organized a tech policy workshop in November here at the Center for Applied Data Ethics and all kind of in the process that we'll be releasing all those videos online for you thank you.

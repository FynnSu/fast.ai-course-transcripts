 So yeah, so what we're picking up here is privacy and surveillance, which we did not finish last week. So there's a lot of material and even more stuff happened in the past week related to it. And so I just kind of briefly wanted to kind of review where we were at. And the first is some issues around kind of what can go wrong. Data will be used for other purposes. And so this link, Emulia posted in the forums about Grindr being owned by a Chinese firm, which has created concerns around U.S. national security of could this be used for China to get data about military or foreign officials on their sexual preferences or even just location tracking. And this is something that couldn't have been foreseen or, you know, wasn't obvious that it would happen, say, five or ten years ago that it would end up being bought by a Chinese company. So companies can change hands. Another story from the last week is that Clearview AI, which is the incredibly controversial facial recognition software that kind of violated the terms of service for most major companies to scrape faces from everywhere. Its entire client list was stolen in a data breach and has since been published. It includes a variety of governments, also lots of companies. I think it's, I can't remember, definitely over a thousand customers. And then also raised security concerns about just the fact that they had this data breach. And then there was a separate article, some journalists were able to access some of their kind of unsecured app on Amazon S3, I believe. And so they didn't have a client login, but they could still tell certain things about the app. So that's a news story. Last week I shared about the story about how Saudi Arabia infiltrated Twitter. And this was with two Twitter employees who kind of ended up being bought by the Saudi Arabian government to pass information. The article, though, mentioned how someone on Twitter's team, or formerly on Twitter's team, mentioned how it was very common to be approached by governments including the US, the CIA, the UK government. So many governments had approached them. So this is another thing that can happen with your data. Either employees can be secretly sharing it or governments can request access and in some cases get it. And then we also saw the example of police officers widely abusing confidential databases, in many cases, to stalk former romantic partners or to harass people who were protesting police brutality. And a common kind of pattern with surveillance is that it disproportionately harms marginalized groups. We saw this with India's intrusive biometric ID, forcing HIV patients to forgo treatment. And so this was an article that interviewed a number of people that were HIV positive and had successfully been getting antiretroviral treatment and then stopped because they were worried about being outed as being HIV positive or sex workers or gay. Trading privacy for survival is another tax on the poor. This is one of the, this is one of the articles I linked to last week. And then the fact that several states are requiring prisoners to give up their voice biometric ID in order to even be allowed to make phone calls. We also saw that there's little evidence that surveillance makes us safer. So San Diego's massive seven-year experiment with facial recognition technology appears to be a flop. This is something in which I think 65,000 images were collected and there was no proof that it led to, led to any arrest. The investigation with interviews with 40 different police departments using Ring, which again found little evidence that it's effective, and then even kind of some of the companies selling facial recognition to schools are admitting that it's not going to stop school shootings. And then also surveillance is used to suppress dissent and this has been shown kind of throughout history and we have examples happening in Hong Kong and then also in the US. And so this is very, very concerning. So kind of in summary, harms already marginalized, data can be used for other purposes, whether that's governments requesting it, data breaches, sale of the company, errors in data, and it stops dissent, which is crucial for social progress. And that's something that Ali pointed out last week is that there are concerns either way. So when the system is not working properly, so the evidence that these products don't even work or the data being full of errors. However, like if you came to me and had a new and updated study that now Ring is really effective at fighting crime, I would still think that the risk outweigh the positives. And so my, I should say my hesitation is not just that this doesn't work, but around these other issues. And so I think it's something that it's also bad when the systems do work as intended because many of the bad consequences are about the system kind of working as intended. And I thought that the line between these is a bit blurry. Some of the examples, I wasn't sure exactly is this the system working as intended or not. You know, for instance, like government collecting data from the, from a company, is that kind of the intention or not since it is such a kind of common pattern and seems built in. So I just wanted to note this, though, that it's, particularly for me, it's not just that it doesn't work currently, but that there are a lot of harms, even when the system does work as intended. Any more thoughts on this? Okay. So, oh, and then I also shared the examples, so this is from two weeks ago of computer vision researchers saying that they were giving up or considering giving up doing computer vision research just because the negative applications, the military applications and privacy concerns were too hard to ignore. And I thought that would be a good segue into discussing the Langdon Winner article, do artifacts have politics? And so this was in the reading, I believe, two weeks ago. And we didn't have a chance to discuss it then, but this idea of kind of rather than thinking of technology as a neutral tool and it's just about how the person using it, towards what purpose they use it, but is there something about particular technologies that lend themselves to certain uses? And actually this issue, I thought, kind of came up, and if you saw, I think this was also in the last week, the quote from the WhatsApp founder explicitly saying like this is just technology doesn't have morals, it's just how people use it. There was a lot of discussion about that quote on Twitter. So I wanted to ask about kind of your thoughts both on this article and the kind of the more general concept of whether technologies are neutral or inclined towards certain uses and certain redistributions of power. Yeah. And the factory example really struck me because it was kind of presented under this veneer of like, oh, this is about efficiency, we're getting these machines, when really it was the machines were less efficient than the workers, but it was about unionization or blocking it. And I think the Zeynep Tafekci article from this week kind of talks about that of how, you know, initially a lot of these technologies were seen as really kind of democratizing or liberating forces, and then as people in power learned how to utilize them towards their ends, we're seeing kind of the opposite where they're becoming kind of more oppressive forces. Kind of around that technologies evolve. I particularly, when reading this article, like thinking about like the early days of any artifact or technology, especially with all the conversation about like iterative design and iterative development is that even if you imagine that a technology starts in a certain direction, the areas, the populations, and the people who build it are building the early kind of like infrastructure or baseline design decisions. So in some ways it seems like the core scaffolding of technical systems of today suit themselves better to be in the environment that they were built in. A simple example could be GPS or like travel app and GPS navigation apps were built in cities to support cities, and you can imagine that like they didn't work very well. The early versions in places like India, which were not structured, did not have addresses, or in Japan where it didn't start up and required a lot of modification. In India, I think in both directions, where cities, where streets started having defined names because people were starting to use navigation tools. Yeah, that's an interesting point, yeah. More than kind of what happens later. Thanks. I love what you just shared. Actually, so in, I used to study archaeology and when I read this article, I loved it. It sort of reminded me a little bit of the ways that we actually infer politics from artifacts in the axis of writing. So a lot of early kind of excavations you have, like how the rooms are set, and from that you see like here was the king class, the survey class, et cetera, et cetera. I'm sure there's much better studies than anything I did before, but I think like inferring the thing about the time, or it being used in different places, it probably is to me that artifacts do change how things get used and have power, just because that's literally the only things, those physical objects are the only things you have in archaeology, even to the extent that you're often careful instead of needing something at all, maybe to structure until you have the rest of the context, be able to, you know, like might. Yeah, thank you, that's really helpful to hear about from archaeology. I have one comment, and then one other small comment. The first one is that in philosophy there's an area called social ontology, and so ontology is the being or existence of something, and so social ontology is the study of how we as social creatures create meaning out of the objects in the world. I think most social ontologists would say that all objects have some sort of strong meaning that plays with it, so that's just sort of a comment. And the second one is that whenever you're dealing with an object or a system that involves corporate profit, I think that we have to look closely at what that object or art really, you know, what its intentions are. Cool, thank you. I think that several of the articles keep bringing up this aspect of corporate profit, you know, the photography one that we had for last week did this as well, and where are you getting your money for when you create this tech, you know, where you're coming from. Yes, yeah. Yeah, and we'll talk even more about that in the ecosystem stuff in the second half of tonight. Cool, thank you. And this is, yeah, I should say this is something that people kind of disagree about. Then I, in the interest of time, I think we'll not spend too long on the Philip Rogaway paper, but I really, I really liked it, and just a few points that he makes are that surveillance is an instrument of power, mass surveillance tends to produce uniform compliant and shallow people, privacy is a social good, which I'll talk more about in a moment, but creeping surveillance is hard to stop due to interlocking corporate and government interest, and something I learned from the paper that I didn't know before is that Eisenhower originally talked about the military-industrial-academic complex in a draft of his speech, but then the final speech he just said the military-industrial complex, but I thought that was interesting that academia was originally in there as well. Oh, do you want a photo? Oh, no, cool. Yeah, so now I want to talk about some kind of steps toward solutions, and I'm gonna first go through some proposals that I do not think will solve the problem, but that come up frequently. An example of what motivates companies to change, some hope from history, the idea of privacy as a public good, and then some kind of the specific use cases around regulating data collection and political ads. So an idea that comes up a lot is whether we should pay people for their data, and most recently Andrew Yang announced on Twitter last week that he is going to be discussing this with Kara Swisher at South by Southwest, and this was kind of one of his policy proposals. So I think I see it's coming from a good place, this idea of wanting people to be compensated because kind of their data is allowing companies to make all this money, but I disagree with this. Kind of one reason is this fails to treat privacy as a public good, and so, and we saw this in kind of the Masij Cichlowski article last week on kind of making these analogies with the environment of when we had kind of rivers catching on fire because they're so heavily polluted or terrible smog, you know, those problems couldn't be solved by kind of companies paying individuals for how they were being impacted or letting individuals decide like, am I okay personally with it, with the company dumping this waste in the river, but that you needed a kind of more collective response and to start kind of reframing privacy as a public good. And so Masij referred to it as ambient privacy, which actually I think I have a slide on later. The other is that it fails to treat privacy as a human right, and so I think that there's a concerning precedent around the idea of kind of paying money, and this is hard because I think how we classify and think about privacy is still being framed, and so one article referred to privacy as something that emanates from human rights, which I like, so even if it's not officially a human right yet, but realizing that it's at least kind of related to them, that when we start putting a monetary value on that, that that is kind of not a great direction to go in because it legitimizes the idea of kind of it not being essential. And also it kind of increases the class issues in which poor people may have no choice but really feel compelled to give up their data for money. Also this could flip and become a scenario more where people are paying to try to get some sort of privacy as opposed to being paid and kind of will exacerbate the class difference that we already see. It's very virtually impossible for individuals to calculate the value of their data. This is something that's spread over time and really changes in aggregation, and it's hard as an individual to know, you know, what will happen with your data when it's aggregated with the data of other people and with data from other sources. Crypticali on Twitter, who's at I will leave now, had a great thread on this, highlighting how this really puts the burden of time and education on the consumer, not on the firms that have all the power. And then Arvind Narayanan also had a great thread on this, saying that this would entrench the asymmetric and exploitative relationship between firms and individuals. So this is kind of my take on this, and not just my take, kind of many, many other privacy experts have spoken out about this. Are there thoughts on kind of this proposal, or we'll say that there's, even if an individual's data is not out there, there are things that I think we lose as a society when we lose kind of a broader sense of privacy. Yeah, and sort of similar to, I guess, the experience that you were talking about, I forgot where I heard about it, but there's like, in certain states, if you get arrested, your mugshot and other information gets put online, and sometimes third-party companies and organizations will often basically re-host your photo and information about your arrest and all this other stuff, before you've been tried, before you've been charged, before you've been convicted, and it's created this entire industry around, if you want your name and photo off of this website, whenever someone searches your name, sort of tying you to some alleged crime that maybe you've been found innocent of, then you have to pay $30 or whatever it is. And $30 may or may not be a ton of money in practical terms, but a lot of people, when they find out about this, find it really repulsive, on a personal, principled level, and I think that that sort of goes back to this idea that it's not a property right issue, it's a human dignity issue, it's not reasonable or fair that somebody can take information about us and coerce us into paying for that information to be dealt with in a way that conforms to our consent, and like, yeah, the information is true that somebody was arrested or whatever, but again, to echo what Rachel said, privacy is a human, or we tend to think of privacy as a human right and not as a property right that can be like using it out in that way. Oh, briefly wanted to say, so differential privacy, which came up last week, although not by name, under, when we were talking about the census and kind of what are ways to kind of help, and I think that is a use case where differential privacy can be useful, I do think differential privacy is often overhyped, and so I share some of the critiques that Rogaway included in his paper, that it often implicitly assumes that the database owner is the kind of the good guy and that you're protecting the data from others, and he does share a kind of rebuttal to this of kind of more decentralized designs, although I think that kind of default is the more centralized version, and so, you know, as we saw kind of with some of the previous examples that is often not necessarily the case, it's still kind of framing harm as something that's individual and not necessarily community-wide, and it rarely considers the alternative of collecting less data, and so I think that's kind of a key thing that is important, I think, sometimes technical fixes can be very appealing, but I think it's really important to consider the kind of less technical, could we just collect less data, and it also gives corporations potentially a means for whitewashing the risk, and so this is not to say that differential privacy is never the answer, but that I think it can be overhyped as an answer, and I do share these kind of concerns about when it can be misused. Okay, so now on towards solutions, and so first I'm going to just share an example of what motivated a company to do something differently. This starts very darkly with Facebook, so you know the UN has found that Facebook played a determining role in the Myanmar genocide of the Rohingya. One of the best articles I've read on it is from Timothy McLaughlin in Wired, and he interviewed people that warned Facebook execs in 2013 and 2014 and 2015 about how the platform is being used to incite violence, and the person that warned them in 2015 even said that there was the potential for Facebook to play the role in Myanmar that the radio broadcast played during the Rwandan genocide, and yet as of 2015 Facebook only had four contractors that spoke Burmese on staff, which is just wild, it's terrible that they really did not take significant action, and someone said in the article this is not 20-20 hindsight, the scale of this problem was significant and it was already apparent, and so this is yeah kind of very tragic and it's just really difficult to read kind of how little action Facebook took. So then this might have been 2018 when when Zuckerberg was testifying before Congress and he said okay now we're gonna hire dozens of Burmese language content reviewers to try to address this, so in contrast Germany passed a stricter law about hate speech called Net DG, and Facebook hired 1,200 people in under a year, and so the the difference here is that yes that Germany was if Facebook violated this law they could have been fined, I think it was around 50 million euros, so a very significant number, and so yes as someone said money is the difference between these examples, and so I'm sharing this not to say that kind of the particular German law is a model, but just this contrast between kind of being told that you are contributing to an actual genocide versus facing a very hefty penalty, and so that shows kind of this is something that got Facebook to take action when they thought there would be a substantial penalty, and it's important that the penalty is not just a cost of doing business fine, which many end up being, but it has to be significant and a credible threat that it's likely to happen, and so I always kind of think about this as an example of legislation, and the threat of credible and significant financial penalties making an impact, yes yeah because GDPR is something that, and some of this is I think there is definitely more up to see kind of how strictly GDPR is enforced and in what cases, but yeah that kind of credible threat of a significant penalty can motivate companies in a way that that nothing else does, yeah so I wanted to kind of share that some hope from history, so I find I mean I think the problems we're facing in many areas are pretty overwhelming and complex, and so I know that can feel kind of discouraging of just how can we even tackle this when it seems so complex, and so I think it's helpful to remember kind of previous successes, many of which I kind of now take for granted. Something I really liked about data sheets for data sets, which was assigned reading in week two, is that they covered three case studies of how standardization and regulation came to different industries, and so one in particular I'll talk about is car safety. I also listened to a 99% invisible episode on this that was really fascinating, this is a design podcast, but early cars had sharp metal knobs on the dashboard that would lodge in people's skulls during crashes, non-collapsible steering columns would frequently impale drivers, and the collapsible steering column was invented but was not implemented for many many years because there was no financial reason to implement it, but it's said that the collapsible steering column has saved more lives than related to car safety than anything other than seat belts. There was also this widespread belief that cars were dangerous because of the people driving them, and so for really for decades the kind of prevailing sentiment was like, you know, cars are just the way they are, this is how cars are, the problem is when we have bad people driving them, and so there's nothing we can do. There was also the glass, they used kind of regular glass that would shatter in very dangerous ways, and the car companies were very resistant to people even discussing car safety because they didn't want people, you know, if people start thinking about car safety they're gonna think about death and accidents, and so they really tried to stifle that discussion, and kind of advocates and activists had to work for decades to even change the conversation around this, GM hired private detectives to shadow Ralph Nader and try to dig up dirt on him to discredit him, so it was really, and I did not know most of this history, is something that people worked very hard and now, I mean while there are many problems with car culture, it is something at least where car companies have acknowledged they can change their designs, they even brag about safety as a feature, and that is kind of drastically different than the situation a few decades ago. Claudia's got her hand up, oh, you do, or don't, okay, can you pass the catchbox back, and I'll say one more thing about cars while you're passing it back, it was only in 2011 that they mandated that crash test dummies needed to also represent the average woman's body and not just be representing men, and again that's 2011, so relatively recently, I think that's also just kind of a very concrete example of kind of the difference that regulation can make, and something, and I don't know what the more recent statistics are, but up until that point, women were 40% more likely to be injured in a car crash of the same impact compared to a man, and kind of very likely due to these differences in testing, alright, Claudia? I guess the feeling I have about this is that it just seems so much slower than the rate at which technology has accelerated, I mean in the last five years, you know, things have changed dramatically, so, you know, I don't know how long it took for them to implement seat belts or car seats for kids, but I don't know if we really have that much time. Yeah, no, that is a valid point, this was something that took a while, technology is rapidly evolving, and it is, I mean there are other ways that kind of the parallels break, oh, the other interesting point they make is just beginning to even collect the kind of the data on car crashes was kind of a key victory to even kind of have that data. Well I don't even think we've had, well, yeah, we've had the car crashes data-wise, but I feel like policymakers don't even get the Internet yet, so how are they going to get a grip on this in a timely manner? Yeah, no, that is a concern about a timely manner. Another analogy, Julia Angwin, who was a senior reporter at ProPublica and now is editor-in-chief of the Markup, and I'm going to link to an interview she gave in a few slides, but one thing she compared it to the Industrial Revolution and talked about, you know, with the Industrial Revolution we had a few decades of, you know, children working in factories, 12-hour days in incredibly unsafe conditions, and she talks about how it just took a while to even kind of gather the language and be able to describe the problem and that journalists kind of had to do a lot of this just even covering what is the problem and how do we talk about this and that, you know, and then the kind of from there that helps for kind of advocates for organizing around this and activists in organizing, but that she says we're kind of still in this phase of just how do we even kind of talk about and describe the problems we're facing, which I found both kind of reassuring that it's like, okay, it's okay that, you know, we don't have like the, this is the exact solution to implement, but that we do need to kind of just even talk about the problems and kind of build up our language and understanding of them. And then I'll say data sheets for data sets also talked about the pharmaceutical industry and the industry for kind of electronic components like circuits and resistors and transistors. So then there is the example from Masij Ceglowski's post that I mentioned earlier of, you know, just kind of pollution and again this is an area where it can be discouraging because we are still facing very significant environmental issues, but to look back at some of the environmental winds that we've had can be helpful to just kind of remember that we have made some progress on this issue and kind of what those winds can look like. He wrote the infrastructure of mass surveillance is too complex and the tech oligopoly too powerful to make it meaningful to talk about individual consent. To what extent is living in a surveillance saturated world compatible with pluralism and democracy, which is also kind of a, I think, a big question to consider and gets at kind of what we were talking about earlier with public goods. But this notion that kind of ambient privacy and having a society where not all our interactions are on the record can have kind of positive, positive impacts. And I think there are a lot of kind of helpful analogies to be made there with kind of the environment. I'm sorry, I'll get to a slightly more positive framing in a moment. This is the towards solutions section, but I do really think it's helpful to try to kind of learn from history and look at victories in history. So this, I really liked this. This was an interview with Julia Angwin and Trevor Paglen, and they kind of argue that even privacy they think is not the right framing for what we're talking about. And so I think this quote was from Trevor, who's an artist, but he refers to anonymity as a public resource. And so kind of thinking about, he says there, you know, there are a lot of kind of de facto rights and liberties that arise from not having every single action in your everyday life, having economic and political consequences, and that's something that kind of we've been drawing from and not necessarily realizing the benefit of. And Julia said, I really, I like this framing, privacy is not about wanting to be alone. It's about wanting to be able to participate in the wonderful connectedness that the internet has brought to the world, but without giving everything up, and kind of recognizing that there are, you know, is something wonderful about being connected and that privacy is not an antisocial thing, but it's about kind of wanting to enjoy some of these benefits without having to sacrifice so much. Okay, so now kind of a hopeful story. So this is Tawana Petty, who gave one of the keynotes at our tech policy workshop, and she's the director of digital justice for the Detroit tech community, or Detroit Community Tech Project. And so in Detroit, and I think we talked about this last time, has this project Greenlight that's putting kind of surveillance cameras all over the place. And she talked about this program they're doing called Green Chairs Instead of Green Lights, and the idea is to give people free chairs if they'll agree to sit on their porches more and talk to their neighbors. And this actually started, I believe in the 80s in Michigan, and they're kind of, people were worried about like the safety of children walking home from school, and so they did this community project of let's have more people sitting outside during the hours that children are walking home from school and get people talking to their neighbors more again. And that was kind of successful, and so I really liked this, and so this is a very like low-tech solution. And she contrasted, oh, I was gonna, thought I quoted it, so I wrote about this and I linked to a source about it in a blog post I wrote last month. But she said that surveillance is not safety. And so kind of making this distinction between kind of surveillance and more cameras and kind of what actually makes people feel safe and potentially having more community and better relationships with their neighbors can contribute to safety. Raman Chowdhury, something she said at the conference that stood out to me was that how surveillance is often kind of part of increasing militarization, and even though it's kind of in the name of increasing safety, we think of heavily militarized societies as pretty low trust societies. And so that the kind of the relationship doesn't seem to hold in terms of militarization, doesn't actually make people kind of safer, increase trust in society. So okay, now I'm going to talk about some kind of, so this is kind of, I like this example of just thinking about, you know, what are some kind of not necessarily even involving technology solutions that could address kind of the same sort of problems that surveillance is offering to address. Onto policy proposals, and so these are going to be kind of a little bit more narrow in their focus. This was op-ed in the New York Times by the author of, I think it's the anti-social network is the name of his book, but he says that, this is Siva Vaidhyanathan, the key is to limit data collection and the use of personal data to ferry ads and other content to discrete segments of Facebook users, unfortunately that's the core of Facebook's business model. So one concrete proposal that could potentially work in the US would be to restrict the targeting of political ads in any medium to the level of the electoral district of the race. So kind of not allowing, you know, this extreme micro-targeting, but really trying to keep it broader. Other kind of proposals around advertisements are, you know, to have them based on the content of the page you're looking at and not so much on this compilation of your personal data. This was a report that came from John Hopkins in UNC and one of the authors said there was even bipartisan agreement on how to regulate digital ads in a basic way, similar to how TV ads are governed, more transparency, databases of content, actual government oversight, and so this is something that we kind of had for particularly for political ads on TV that we don't yet have for for digital ads. And as Zeynep Tufekci wrote an article in 2018, and this is during the Facebook hearings and she actually said, you know, we don't need to interview Mark Zuckerberg, we already know more than enough about Facebook, we can just look at their actions for the last 10 years, and she proposed that data collection should only be through clear, concise, and transparent opt-in. People should have access to all data collected on them, data collection should be limited to specifically enumerated purposes, and I think this also includes the time limit that came up in previous, a previous class, but the idea of, you know, when data is collected it's we're gonna use it for this purpose, for this length of time, and that's it, as opposed to right now where it's kind of this, you know, indefinite, they have it forever. Actually, I'll pause, oh, actually, we're, okay, I will finish this, I think I just have like two slides left, and then we'll have our break, you know, we're running a little bit late. So, a quote I like to remember from Zeynep, who I really admire, is, what we need to fear most is not what AI will do to us on its own, but how people in power will use AI to control and manipulate us, and so I think that's a kind of important principle to keep in mind when considering how, how can and should we regulate, and then these are kind of some of the top experts I recommend following on, on issues of privacy and surveillance. And then with that, we'll take, we'll take our seven minute break, so let's meet back at 7.15.

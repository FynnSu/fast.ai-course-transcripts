 And I'm going to start by going through a few different case studies of bias to kind of ground what we're talking in some practical real-world examples, and these also will illustrate some of the different types of bias from the Harini Suresh and John Guttag paper. And the first is gender shades, which I would imagine many people are familiar with. This has received a lot of media attention, which is good. And this was a study from Joy Balamwini and Tibnit Gebru evaluating commercial computer vision products from Microsoft, IBM, and Face++, and then they did a follow-up study that looked at Amazon, Kairos, and a few other companies. And they kind of consistently found that these classifiers had significantly worse performance on dark-skinned women. And here, it's really significant that they didn't just look at light-skinned versus dark-skinned or women versus men, but kind of broke it out into subcategories to kind of capture that, for instance, IBM's product was 99.7% accurate on light-skinned men and then only 65.3% accurate on dark-skinned women. And again, these were commercial products that had been released. So, yeah, the follow-up found something similar with Amazon. A separate study from the ACLU found that Amazon's facial recognition incorrectly matched 28 members of Congress to criminal mugshots, and this disproportionately included people of color who were wrongly matched. This technology is already in use, even though it's virtually unregulated. This is, I think, pretty concerning and alarming. And so, I really like this paper that I assigned on a framework for understanding unintended consequences of machine learning because it kind of breaks down bias into different sources have different causes, which is helpful for addressing it. And that kind of pretty quickly, you know, it's helpful to know that bias is an issue and exists, but you kind of need to move beyond that surface level understanding to think about how to address it and where it's coming from. So, in this case, this is representation bias where the kind of data that you've trained on is not representative of the data that this is being deployed on. However, because this was a problem kind of not just for one company, it's also an example of evaluation bias. And so, in machine learning, benchmark data sets spur on a lot of research, which in some ways is a positive, but it also means that biases in those benchmark data sets end up kind of being replicated at scale and that you have like a whole body of research kind of on top of those biases. And so, a lot of the popular data sets of faces primarily included light-skinned men up until kind of just in the last year or two where people are starting to address this. And so, for instance, IGBA was a data set of faces and only 4% of the images were of dark-skinned women. We also see this with ImageNet, which is I think probably the best studied computer vision set out there. Two-thirds of ImageNet images are from the West, and so ImageNet is for kind of classifying all sorts of different, you know, types of animals and household appliances and vehicles. And so, it's not people for the most part, but kind of all these different things, but which look different in different cultures. And so, as a result, there are higher error rates. Kind of if you, one example that is of a person is bridegroom, so a man getting married and this study found that there was a much higher error rate identifying bridegrooms from really like Egypt and India and countries outside the West. And so, a way to address kind of representation data is creating more representative data sets, and Joy and Timnit did that as part of gender shades. It's really important to keep in mind consent when building data sets and not taking people's images without their consent. And so, this was a well-built data set, but there are examples that were not. And I will come back because this is not the full answer and there's more to this, so we'll kind of return to this example later. Any questions on that? Oh, Lauren. And who has the catch box? How are there ways to reduce bias in open source computer vision projects or competition like Kaggle or Omdina, where especially with like Omdina, a nonprofit, has a challenge and then crowd sources, you know, ML folks or data scientists from around the globe. Is there a way, like is Kaggle or Omdina or organizations that are doing open source projects like that, how are they reducing bias, if any? That's a good question. I'll get to some. I have a section on kind of toward solutions later on, but the kind of brief answer, and I actually don't know about Kaggle or Omdina in particular, but kind of being very thoughtful about how you construct the data set, which is in general machine learning has often kind of just focused on gathering huge amounts of data efficiently and not as much about kind of how that data is structured or what sort of risk might be in there. But yeah, we'll come back to that in the later section. Any other questions? Okay, further down the fourth row. It would seem to me that as the population that it's being used on changes, we should be adjusting the historical data to match the population as it's growing and changing. Do we see that or how could that be seen? I mean, so it depends how your population's changing. It may be, it depends on also the type of data, it may be hard to change the data. Like, I think it's easier to realize that your data is no longer representative of what you need in many cases than to actually generate the data that would be more representative. But we will talk about it, because I think that relates some to historical bias, which I'll talk about in a moment. All right. Next up is the compass recidivism algorithm used in prison sentencing. This is another kind of very famous and well-studied example. But in 2016, ProPublica did an investigation inspecting the software that is sold by a for-profit company. It's in use in many states in the U.S. and it can be used for pretrial decisions. So this is deciding who has to pay bail prior to even having a trial. And many people in the U.S. are in jail because they're too poor to afford bail. It's also used in sentencing decisions and in parole decisions. So having a very significant impact on people's lives. And what they found is, looking at kind of a county in Florida, that the false positive rate for black defendants was almost twice as high as for white defendants. And so this is people being labeled as high risk of committing another crime or of being rearrested who were not rearrested. And the false positive rate was 45 percent, which is also just a really terribly high false positive rate when you're labeling people as just a binary high risk or low risk. There was a study from Dartmouth that came out, I can't remember, a year or two ago that found that the software is no more accurate than Amazon Mechanical Turk workers. So random people on the internet who were kind of just as accurate in their judgments of whether someone was high risk or low risk. Another thing the Dartmouth study found was that, so the Compass Recidivism software is a black box that takes, it's either 130 or 140 inputs, and it was not any more accurate than a linear classifier on three variables. So it's also just not, not particularly accurate. Yet the Wisconsin Supreme Court upheld its use. And it's used in states other than Wisconsin, but this is one place where it was challenged. And this is kind of a, in Arvind Narayanan's 21 definitions of fairness talk, a lot of that is kind of based off of kind of going deeper around, around Compass. And it is true that the company was using this different definition of fairness that is not about false positives, that it does satisfy. And so one, one key thing to really highlight is that race is not an input to the software. And so even though there was this huge discrepancy in the false positive rates, race was not an explicit input. And it's important to remember that machine learning is kind of largely about identifying latent variables, which is, you know, a variable you haven't explicitly named or recorded in your data set. And so you can still have these kind of hugely different results on different groups even when you're not using that input. And this is a really common misconception. A lot of companies when they're accused of bias are like, no, you know, we swear we're not using gender as an input or we're not using race as an input, but you can still be very biased and have very different results even if you're not. Questions on that? All right, this is also an example of a feedback loop, which we talked about a little bit last week in the context of recommendation systems, and a feedback loop is kind of whenever your model is controlling the next round of data you get, the data that is returned quickly becomes flawed by the software itself. So in the case of predictive policing, this is a slightly different case, but where you're trying to predict which neighborhoods are going to have the most crime so you can send more police to those neighborhoods, having more police in a neighborhood may also result in more arrests just because there are more police around to see something or to arrest someone, which could then, you know, feed back into your algorithm and say, let's send even more police to that neighborhood. One researcher who works on this, Suresh Venkatasubramanian says, predictive policing is aptly named. It is predicting future policing, not future crime, and so I liked that quote as an example of kind of how a feedback loop can happen. And this is an example of historical bias, and historical bias is a fundamental structural issue with the first step of the data generation process and can exist even given perfect sampling and feature selection. So this is a case where going out and saying like, oh, let's get crime data for more U.S. jurisdictions is not necessarily going to remove the, or it's not going to remove the racial bias because this is really kind of present in the data, and it's really true because of our history. And unfortunately, historical bias is, I think, very hard to address, and it's also a very common type of bias. Yes, and who has the catch box? Okay, pass it to the front. So I might not be phrasing this question accurately, but I'm curious about how it seems like the biases that come up in these examples are most frequently race and gender, which are characteristics, they're physical attributes. Are there biases that have shown up that are less physical but still, I guess I'm trying to ask around the latent variables, are there things that have shown up that are not necessarily physical in a data set? I am sure that there are, because like in machine learning, like whatever you're trying to predict, you can describe as a latent variable because that's something that's not in the data set. I think why so many of the examples on bias are around race and gender, just because they're easier to spot when they happen, whereas some things, I mean you can, I guess like education or socioeconomic status or country of origin or language, there has been some research around language. So yeah, bias definitely exists on other variables, and then things that are also more seemingly neutral. I don't know if you were identifying species of a tree or something, like you could have bias, but in this context we're kind of talking about, actually I should have made that clear, bias has multiple definitions which can be confusing, and so here I'm not talking about the statistical term, but I'm talking about kind of unjust bias. Ali? This could go down a rabbit hole, but arguably like race and gender are actually socially constructed and they're not really representative of an exact biological thing, in particular when you think about what is white. What has qualified as white has changed over the years, and 100 years ago, people that we would now consider to be white, based on their country of origin or whatever, would not have been considered that. So there is actually this strongly socially constructed component, and I realize that's kind of a cop-out to your question, but it is sort of worth, I guess like, noticing like oh actually a lot of the stuff that it's picking up on are things that we've socially constructed already. Yeah, that's a good point. I think something I would like to add to that is like, I think basically these bias are things that people is intentionally growing for, right? So it's not like somehow you have the model and the model will tell you kind of what is bias for, and people kind of breaking up this lab data in these categories, and they're finding this imbalance, right? So for sure, probably there is bias in many things, but it's what people kind of will be willing to check for, okay, there's bias or not, right? And yeah, and I don't know really about probably there's some ways to try to do some unsupervised manner where you see some activations and try to find clusters, and then you kind of dig in which data points are associated to those clusters in a way, and then you do the other way around. Yeah, and actually there is a paper on that topic. I can find the link of, because like Elise said, because these are kind of, these are socially constructed categories, there is something that can be, you know, iffy about like assigning, I think this person is this race, or something around this, you know, socially constructed category, and so there was a paper last year I know on trying to unstructured approach in an unstructured way and looking for categories. There's also a talk from our tech policy workshop, and we'll be hopefully releasing those in the next month, that Christiane Lum, who works for the human rights data analysis group, and she does a lot of work with kind of how data is used in our criminal justice system, and she kind of talked about inspecting this a lot of data from police. It's just the police are assigning what race that they think the person is without asking them, and so, you know, they're like, there's, I mean this is like a kind of problem with the data source, but then it's also like that's the data we have to look at. So it's definitely imperfect, and there are kind of issues around it, although I do think it is like, you know, you want to be looking for racial bias, and so sometimes you are having to kind of work with the data you have, even when it's going to contain inaccuracies, and is around this, yeah, kind of socially constructed categorization. I have a question about the Cornell reading, problem formulation and fairness. I was looking at the, I guess it was like a subscript for KDD, knowledge discovery databases. I didn't know it was around since 1989, and I was thinking about like why only now is ethics coming up as an issue, even though we've had this, you know, sort of like practitioners in the field who've been considering this. Is it only because of, I don't know, an overreaction from like the 2016 election? Is it because we're understanding more deeply now machine learning and practice is biased? So I was just curious. I was going to say briefly, so I think there were people, like there are people that have been considering ethics for longer, and often in these kind of adjacent fields of like STS or library science, I think have more of a history of thinking about ethics, but I think right now what we've seen is kind of the explosion of these, of machine learning and different algorithm or automated decision systems being used much more widely in practice, and so it's, I think, a lot of why we're seeing kind of more ethical alarm is the widespread use. And there is, I will note that like many, many fields have had to deal with kind of the ethical quandaries of their field, and actually Ali has a great essay that we'll read later on kind of anthropology dealing with ethical issues, but people have talked about, you know, physics and the atomic bomb and how that raised ethical issues. And so it's also kind of not, you know, we are kind of experiencing like a major kind of reckoning for the field, but we're not the first field to go through that as well. Well, I'll keep moving and we'll come back to, I think, some kind of related things on all of this as we go. So what, and I'm here to give kind of one recommendation of something you can do towards trying to address this, and that is speaking with domain experts and people impacted. And so at the Fat Star Conference, I think in 2019, Christian Lum, Elizabeth Bender, and Terrence Wilkerson hosted a really great workshop, and it's available online. And so Christian Lum is the statistician I mentioned who works with Human Rights Data Analysis Group. And then Elizabeth Bender is a former public defender. And Terrence Wilkerson is an innocent man who is wrongly accused of committing a crime and couldn't afford bail. And Elizabeth and Terrence were able to give a lot of insight into how the criminal justice system works in practice, and it's things that I wouldn't necessarily know as a computer scientist and coming from a more technical background, and it's really important to get those kind of implementation and human details of how things actually work. So Elizabeth talked about when she would go to visit one of her defendants on Rikers Island, she was having to take a bus like two hours each way and only getting like 30 minutes with the defendant if the guards brought them in on time and kind of all these limitations. And Terrence talked about a lot of the just how kind of terrible it is to be in prison and the pressure to take a guilty plea bargain to get out quicker and also how it can impact how you appear before the judge when you've been in prison. And so it was a really helpful session and those sorts of collaborations of talking closely with the people that have the domain knowledge and are impacted is really crucial and I think one step towards trying to address this type of bias or at least towards trying to address the harms from it. All right. The next, so the third case study I'll talk about is online ad delivery. And I shared this last week, so we'll just briefly go over it again, but we talked about Latanya Sweeney, computer science professor at Harvard who was getting these ads saying Latanya Sweeney arrested. She had never been arrested. She found names like Kirsten Lindquist, who has been arrested three times, got more neutral language just saying we found Kirsten Lindquist. She studied over 2,000 names and confirmed this pattern that disproportionately African American names were getting the ad suggesting they'd been arrested. And then this kind of discrimination in advertising has continued to show up in a lot of different forms. There was a paper last year of even when advertisers aren't trying to discriminate, their ads get shown to very different audiences and so the researchers found that a job ad, depending on what the job was, could be shown to an audience of 90% men or 90% women. This is on Facebook, although I think this is true on many platforms. Housing ads changing the picture between a white family and a black family. They got very different audiences even when the text was identical. And that this is an issue because things like housing and jobs really relate to kind of our civil rights. And this is just a series of ProPublica's, ProPublica's fantastic, it does fantastic work, but they found that Facebook, dozens of companies were using Facebook to only show job ads to young people. Facebook was also letting housing advertisers explicitly exclude users by race. And actually I should update this. So there was an initial discovery in 2016 that they were doing this and Facebook apologized and then over a year later they were still doing this. And so as you could place a housing ad and say, you know, I don't want to show this to Latinx people or I don't want to show it to, I think there was like, I don't want to show this to wheelchair users, but kind of very, very discriminatory. So then they were still doing it over a year later and then they did reach a settlement last year and implemented this new system that is supposed to limit this and then ProPublica did another investigation how their, their new platform is not, not actually effective. And then we saw Amazon matching members of Congress to criminal mugshots. There's also the Amazon resume screening tool that penalized resumes with the word women's in them. So this, this shows up lots of places, kind of this bias. And so I do want to highlight that many AI ethics concerns are about human rights and civil rights. I like, there's an article from Anil Dash where he says there is no tech industry anymore. Tech is used in every industry and so this term has kind of become meaningless and it's important to really think about the particular verticals that you're talking about. And so others have proposed, and I agree with kind of one, one way to, when people talk about the idea of regulating AI is to really think about kind of what human rights and civil rights we're trying to protect in particular areas, whether that is housing or education, employment, criminal justice. So going, kind of going back to this paper by Suresh and Gutag, this concept of biased data has, is often too generic to really be useful. And so, oh, another, another paper I like from Sendhil Malainathan and Zayed Obermeyer looked at historical electronic health record data and asked what factors are most predictive of stroke. And so they found, and they were saying, you know, this could be useful in prioritizing patients at the ER, and so they found the number one most predictive factor was having had a prior stroke, which totally makes sense. Second was cardiovascular disease, which also makes sense. And then next was accidental injury, a benign breast lump, colonoscopy, and sinusitis. And so you don't have to be a medical doctor to say, wait, those, something seems wrong. Why would those things be predictive of, of having a stroke? Does anyone want to guess why? Up here? Oh, can someone pass, pass the catch box forward? I don't know, it's like older person or I don't know, it's more likely to experience that, I don't know. Oh, so the guess was older person. That's not what they found, but good guess. And then up here, oh, okay. Over here. Visits to, so like visits to the doctor, visits to the hospital, you normally go because you have an accidental injury or a benign breast lump or something. So close. So I'll, oh, one more guess behind you from Colin, but then. We needed to do someone who wasn't in the hospital for something else to be confounded by us. No, although that's also a good guess. So I like how you're thinking, this, oh, Claudia has a guess in the back, that corner. It could be part of their health history that they be self-reporting. Yes, that's it. So it's, well, they classify it as who utilizes healthcare versus not. So they're basically people that utilize healthcare a lot and people that don't. And there are a number of factors that can contribute to this of who even has access to healthcare, who can afford their copay, who can take time off of work. There may be cultural factors. There's racial and gender bias and the types of treatment that people receive. And so actually, I guess this goes back to your earlier question of picking up latent bias of something. They found here they're picking up bias basically of high utility versus low utility for healthcare, which is not a physical characteristic, but is making a significant difference. And so if you're someone who is going to go to the ER when you have an accidental injury or sinusitis, then you also are going to go to the ER when you're having a stroke. And so they said, you know, we haven't measured stroke. We've measured who had symptoms, went to the doctor, got the correct test, and then received the diagnosis of stroke. And so this is one type of measurement bias. And there are many kind of many types of measurement bias, but it's a way that your kind of measurements aren't capturing exactly what you think they are. And you know, and even kind of more biologically, stroke is about region of the brain being denied oxygen. And that's something we don't have, we don't even have the data for, you know, that'd be like we would need everyone wearing brain monitors to know who has had stroke. We have these other things, you know, in medicine it often ends up being about kind of like a billing code in a chart. And so this is, yeah, I think a kind of subtle form of bias that could show up. Oh, question in the second row. And this is, I would say this is also another example where gathering more of the same type of data you already have is not going to help you. I mean, if you're able to gather kind of a different type of data, that better gets at your question, but just gathering more of the same won't help here. And then a fifth type of bias that they talk about in the paper is aggregation bias. And they give the example of diabetes patients having different complications across ethnicities and also kind of some of these blood markers used to diagnose and monitor diabetes differing across ethnicity and gender. And so this could combine with other biases if you have, so this suggests that maybe one model is not going to do a great job of representing people from different ethnicities or genders. Also if your data set had representation bias, you could then end up kind of drowning out a certain group and you would have a higher error rate for that group. So this is another type of bias. And so they look at five in the paper, but I think it's helpful to see that I think what's most talked about of kind of gathering a more diverse data set is really only going to help for representation and evaluation bias, but most likely not the other three. Then I want to talk a little bit more about kind of where racial bias is found and all the studies I'm going to cite here are from a New York Times article that Cynthia Melanathon wrote, but these are all from kind of peer-reviewed academic research and this is just a sample of what's out there, but when doctors were shown identical files they make are much less likely to recommend a helpful cardiac procedure to black patients than to white patients. When bargaining for a used car, black people were offered initial prices $700 higher and received far smaller concessions. Adding to apartment rental ads on Craigslist with a black name elicited fewer responses than with a white name. An all-white jury was 16 points more likely to convict a black defendant than a white one, but when a jury had one black member it convicted both at the same rate. And actually I think I had more and shortened it. And so I just I just share this to show that kind of in a wide variety of types of data, whether you're looking at medical data or sales data or housing data or political data, that it is if it involves human it's quite humans it's quite likely that there's racial bias in it that this is very very pervasive. However this raises the question that I that I hear sometimes which is that you know humans are really biased, this is well documented, so why kind of why is there so much concern about algorithmic bias given this, that humans are so biased. And I have a kind of four reasons that I think algorithmic bias still really matters. And the first is machine learning can amplify bias, so in many cases it's not just encoding kind of existing human bias, but it can also be amplifying the magnitude. So this is some research from Maria de Arteaga et al that looked at LinkedIn kind of how people describe their job description. And in their data set they described it as imbalances being compounded only 14 percent of the surgeons were female. And then in their true positives only 11 percent of the surgeons were female. And so basically there's kind of this asymmetry around false negatives and false positives when you have kind of a very imbalanced data set to begin with. Second reason that I think we really need to take algorithmic bias seriously is because algorithms are often used differently than human decision makers in practice. And so this is something where people will often kind of talk about them as though they're plug-and-play, interchangeable, but in kind of in use it's not the case. People are more likely to assume algorithms are objective or error free. This is also true even if you portray, you know, I'm just giving this human recommendation or a guide, people often kind of take that as more of an authority. Algorithms are more likely to be implemented with no appeals process in place. And I think this comes because often algorithmic or automated decision systems are being implemented as a cost-cutting measure and allowing appeals or recourse is more expensive. And so the motivation when it's around cost-cutting and not around approving accuracy is often not going to include a process for recourse. Algorithms are more likely to be used at scale so they can be replicating identical biases at scale. And algorithmic systems are cheap. And I think there's a lot of interplay kind of between these factors. Cathy O'Neill wrote in Weapons of Math Destruction that the privileged are processed by people, the poor are processed by algorithms. All right. So kind of in summary, humans are biased. Why does algorithmic bias matter? And the reasons for that are machine learning can create feedback loops as we saw before. So when it's helped creating outcomes, can amplify bias, it's used differently. And then also that I think technology is very powerful and that there's a responsibility with that. Yeah, I'll start the next session. So now I want to talk about it's not just the data. And so this was kind of a focus on, okay, this is ways bias can appear in the data. So something important to understand, so there's a joke, you know, the good thing about computers is they do exactly what people tell them to. The bad thing is they do exactly what people tell them to. And often you may mean something in your head that is not what makes it into what you type. And in machine learning in particular, you know, a person is the one that is defining what is success. And that typically takes the form of we're minimizing an error function or maximizing some sort of score. But who gets to define that error function? And what the 21 definitions talk was kind of touching on was also that that's not necessarily kind of straightforward what error function you choose. And so Arvind shared this chart from Wikipedia on ways to evaluate binary classifiers. And so here kind of the top left is giving this notion of true positive, true negative, a false positive and a false negative. And then there are all these different ways to combine these into accuracy and negative predictive value and false discovery rate and F1 score and different scores you can make to evaluate how good is your classifier. And so to give an example, if you were using, if you're doing a medical diagnosis, maybe something like a mammogram trying to identify, you know, is there a cancerous tumor here? This is kind of a value judgment, but how does a false negative compare to a false positive in this instance? False negative is worse. Yes, I would agree with that. So a false negative is you're telling someone you don't have cancer and they do, which is bad. A false positive in that case is telling someone, hey, you may have cancer and they don't and presumably they're going to go then get a biopsy and find out they don't. And there is still some cost to that. You've caused stress for a person. This is another procedure they have to have done, but yeah, on the whole that definitely seems less bad than telling someone they don't have cancer when they do. How about spam? So you're identifying if an email is spam or not, and if it is, you're going to put it in their spam folder and not even show it to them. This is, you know, your email server. What do you think is worse here? False positive or false negative? I think so. Yeah. So this is a false positive. Okay. And what was the email? If you want to, you don't have to say. Yeah, so yeah, if it's an email from a loved one or about a job offer or something important, you don't want it going to your promotions tab or your spam folder. And with all these, though, you can also think about the trade off. So I would totally agree on that, but there is a point where, you know, getting one spam email in your regular inbox, like, hey, that's okay, but if you get 10 spam emails in your regular inbox, you know, there's probably some tipping point where you're like, this is stop putting the spam emails in my regular inbox and ditto with the cancer screening example. Right. Another case. How about going back to this criminal recidivism algorithm? So predicting if a defendant is low risk or high risk for committing another crime. And I say committing another crime, but this includes people that haven't even had a trial. So what's worse, false positive or false negative? So mostly here, false positives, you know, can you pass the, okay, say that again, because who you ask, because basically, obviously for a non guilty person to say that, basically be trying to say you have to come back to jail because you don't get free because you are likely to commit a crime, obviously, and you wouldn't obviously, but the other way around is you're releasing people that is committing crimes, then they have actually impacted the victims of the crime, right? So who you ask there kind of depends on, I don't know if it's very obvious. Yes, who you ask will definitely vary your answer. I see another hand in the fourth row. So I see that case for perspective as it's made in one of the papers. What bothers me about them is that they all seem to go past the idea of a system of justice having its own moral center. So if you have a Jeffersonian statement like it's better to let 10 guilty people go than put one innocent person in jail and you come up with systems where you can simply undercut that principle because it's profitable for you to imprison more people or so on, then the idea that there is no moral center to begin with and we have to redefine it, I think boils back to, no, there is, we're just ignoring it. And so in that case, false positives are clearly bad unless you disagree with the system. I mean, I think, so you're kind of saying that like with a for profit prison system, you're not even trying to hold to an ideal of it's better to let 10 guilty people go free than put one innocent person in prison. I think that, I don't know that everyone would agree with that statement though. I mean, I agree that there's a lot of problems with the for profit prison system. But I think even in a healthier system, I think there are people that would say like law and order is a high ideal and might not agree with the letting 10 guilty people go free. So I think there is still something of a value judgment there, although I do think it's definitely clouded by kind of other factors. Yes. I would just make a general comment that applies to all of these cases is that they're so fact specific. What was the crime? What was the misdemeanor versus like everything is so fact specific as to the false negative and then the criminal goes and is a recidivist and murders another person. It's very difficult to just contain all of these same with spam, you know. Yeah. There is a lot of nuance that is not going to necessarily be captured. Yeah. So this was to kind of highlight that there's not that your perspective does matter and that in some of these we had more agreement than others or you know some of these I think it's possible to reach broader consensus than others that there's not necessarily a clear cut answer and then even more so there's not necessarily a clear cut. This is the best way to evaluate the error rate for a given scenario. Okay. Then it's about seven o'clock. So let's stop here for our seven minute break and then we'll resume back. Let's start back up. I want to check if there are any questions about the 21 definitions video or any thoughts. I thought I wanted to move on to the problem formulation and fairness paper and so this was a ethnographic field study where they followed this team of kind of data scientists and product managers, business analysts, product managers and so on of a company that was working with car companies to pass on leads to kind of to auto dealers and the initial project goal was to improve the quality of the leads and actually I'm curious what did what did people think of this paper? Any thoughts or reactions? Okay so I'll repeat it for the microphones once they were mad they care so much about their score. Any other thoughts? And I'll say like I thought it was kind of interesting to see having worked as a data scientist someone describe the process of what to me felt very familiar in this kind of academic and studied way but I thought that was helpful and they had started out with this question of what makes a lead high quality and there were various answers from various people in the project you know it could be the buyers salary whether the car they want is in stock which is neat kind of thinking about from the buyers perspective what they're looking for. Dealer specific finance ability so different dealers have different financing processes and so they kind of you know started out with this broad brainstorm of like oh these are the things that could make a lead high quality and then ran into difficulties the dealers won't share that data the credit scores were segmented into different ranges so they kind of had these course ranges that didn't really align and so they end up reducing the problem to predicting the credit score below 500 versus above 500 and so it's kind of in disappointing but it's a trajectory I have been on where they I feel like start with this you know broader and more interesting question about high quality leads and then they have to boil it down to well we kind of don't really have any data other than these big buckets for credit score so let's go with that. They did talk about you know should they try to purchase higher quality data sets but ruled that those were too expensive and the project ended up failing and so it was I thought kind of helpful to go on this trajectory and it's one that I have witnessed firsthand that yeah that often I think projects are kind of limited by the data that you have and I think that's important to keep in mind and keep in mind also these like business constraints that that really matter in the workplace when discussing these problems because I think it can be easy to kind of focus on you know a more academic or idealized like oh it'd be great if we had you know this type of data or could do this sort of process but that may often not work out. Different people gave different reasons that the project failed whether it was the data quality, the expectations, the the nature of what they are trying to do. So something I wanted to ask you is what and for this I think I want you to try talking in groups of three or four again and so yeah you can pair up with people near you or you can also cross rows and we'll just take maybe seven minutes to discuss just if what if anything do you think this team could have done differently to consider bias and fairness? I mean do you think they should have done it anything differently and if so what and if not why not kind of curious to hear does anyone want to does anyone want to share something that their group discussed? The catch box is sitting on the third row I don't know if anyone on the third row wants to say anything or pass it forward to the second. Yeah so we were just discussing that it's kind of a question of alignment where when you're saying you're looking for quality that can kind of you can have these fuzzy beliefs about what quality may or may not be but when you don't kind of have a company-wide alignment when everyone agrees what it means and when you're going telling them what a machine what quality means that's kind of when it gets reductive and biased almost a sort of the mission so you can't really eliminate the bias because their time is how can we figure out their credit scores for cheaper and there's this quote from Maciej Ceglowski that machine learning is just money laundering for bias and so that bias laundering because quality wasn't defined what it ended up being is just saying well let's try to do bias but with this machine so nobody knows so we kind of had the pessimistic view that you couldn't really eliminate bias from it because unless quality sort of had a rigorous definition before they started this project then bias is kind of what they were trying to achieve. One thing that came to mind for me was just changing the business model to accommodate the idea of selling more cars to more people like the modern-day leasing model we have today is some perversion of the initial model but it was all aimed at depreciating the value of the car and looking at the secondary market before you sold that car and the end result was you could go to any particular customer say do you have X amount down payment and can you afford $380 a month if they could answer that knowing full well that the consequences were lying were immediately in possession they automatically qualified themselves out from the stress of losing a car that they just got so just changing the model to address your market rather than trying to figure out you know your customer your potential customer by date you can't have or don't want to pay for seem like one way to go about that yeah and that's it that's a good point also about just when you have clear simple rules as opposed to you know even the credit score itself is kind of this opaque system but you're suggesting something with clear rules of if you have this much money and can pay this much money per month then you can get X or well thank you all that was interesting I didn't have like a I was actually not sure what you would come up with but I appreciated those answers and and thinking about it and in some of this stuff I guess in the privacy and surveillance lesson we'll talk a little bit more about financing both the big three in the US as well as some kind of a micro financing projects as well all right so hopefully so I kind of in the the first part you know wanted to make a case that algorithmic bias matters and is significant I also want to highlight that bias isn't the whole story or the only issue and some of you have already kind of touched on this in your kind of in your comments first that I think accuracy is not it's not everything and so referring back to the compass recidivism algorithm point that Abe Gong shared he's a data scientist part of the input to the system is this questionnaire that the defendants fill out and it includes questions such as if you lived with both parents and they later separated how old were you at the time was your father figure or father figure who principally raised you ever arrested that you know of and so keep in mind these questions are then inputs to this algorithm that's helping determine who who has to pay parole or sorry pay bail who might get parole really significantly impacting someone's life and as we saw from the kind of Dartmouth study I ancient mentioned this is actually not very accurate and there's not evidence that this makes it more accurate but even if it did I think this is unethical to have these sorts of factors used in making such a decision just in that you know people have no control over what happened to them as children or what their parents may have done and it seems to me kind of very unethical to use that in someone's prison sentence and so I think it's important to keep in mind that kind of improving accuracy shouldn't be be your only goal but that there may still be types of data or particular factors that are unethical to use. I remember reading this and it didn't go into it so I'm wondering if you have additional information about how this question set was even administered. Yeah this is the other the other factor is that it's and I actually don't know the answer to this but my understanding is that people could lie on this questionnaire and so then are you yeah rewarding people that lie or it's right so that yeah so there is a probably significant significant yeah like who knows what people were saying for this I mean I can imagine some people may have felt intimidated and felt like they needed to give correct answers well it's 140 inputs they're not all questions and actually don't know yeah what what else is in the question yeah so that no this should raise a lot of issues like yeah this just seems like terrible for many reasons all right so yeah that's one kind of just issue apart from bias but that is relevant yeah then kind of returning to facial recognition and it's important to think about its uses so it's kind of wrong to have these very very different error rates the same time in in 2015 after Freddie Gray a black man was killed by police in Baltimore Baltimore police used facial recognition to identify protesters protesting his death and this was specifically to identify people that had existing warrants although there's no information on what these warrants were for if they were for very minor offenses or not and this was from this private company and this only came to light because the ACLU ended up obtaining some of its marketing materials and the company was bragging about this is like a successful case study and they even I should have included the language but it's like oh it's so great we were able to like arrest these people before they harmed anyone but it was like why why were you arresting them if they if they hadn't done anything and so this is this is really concerning I think the idea of identifying protesters you know if this became widespread or even not widespread if this became you know was being used more I think could really impact civil rights and so it's important to think about applications like this so this is not something where you know having less bias facial recognition I mean this is you know it'd be bad to be making errors here but it's also bad to be doing this accurately like this is not I think this is not a good use case at all and so wanted to wanted to highlight that yes yeah at the end it's like they are finding ways to do what they need to do in a cheaper way right and that's I think what pushed them to actually implement this system right because so in general I would say because even for example I don't know all the companies in the US who decide okay yeah no we are not going to develop these technologies because we know all this stuff is going on and there's bias and things and so we are not going to develop these technologies right and then I think the need or the pressure that I don't know the police or other type of organization are feeling to implement these technologies because kind of budgets are like that I don't know maybe they will start buying these technologies why are they developed where there's no this concern right so so the question is like there's some drive so what is how you approach and develop the technology but somehow there's some drive for forgetting these technologies that are not so clear for me what is the source and so how you need to control that for the people deciding to buy these technologies so how they don't so I would I would disagree with your original statement that the police are a that they need to do this and that be this is allowing them to do it cheaper and there are many things that are very inefficient about the market for technologies sold to police so in the and we'll talk more about this in the privacy and surveillance lesson but just briefly so one policing in the US is kind of hyper local so these decisions are kind of made department by department but often I don't know that tech companies are identifying like hey this is a legitimate and necessary need that we're fulfilling they're often kind of pushing like hey isn't this technology cool and selling it there and it can be very expensive in many cases so for many types of technology there's kind of near monopoly so for instance police body cameras taser now branded axon has virtual monopoly on the market as also I'll post an article I wrote with kind of some relevant pieces of information but yeah I would disagree that this was something that needed to happen or that would be done either way and also even that this is lowering the cost because these can be quite expensive and many of these companies are then kind of pushing cloud storage of keeping this data forever but this is I would prefer to wait to the kind of privacy and surveillance and I mostly I'm kind of highlighting this to show that even and you know and as we discussed earlier you are probably never going to get the exact same accuracy on different measures across groups but even if you had significantly less biased facial recognition I think there are applications of it that would be very questionable and so it's not just a question of removing bias but also thinking of kind of how how the technology is being used I'm sorry there's I realize more editorializing here if you but yeah so that is my kind of opinion that it is very would be very harmful to be identifying protesters and that this is something that's happened in the United States there was the digital justice lab hosted a workshop unfortunately in Boston called please don't include us of groups kind of not wanting to be included in kind of creating a more or you know less biased technology because of concerns about how it was being used and so kind of many others have kind of raised these these concerns and this happened this fall and I was not able to find much much information about it but I was very or something where I was like if this was local I would have I would have loved to go another kind of issue and this gets back a little bit to what Ali was saying earlier about kind of race and gender being social constructs these are two papers by trans researchers on the problem with doing gender gender classification using facial recognition and so so first of all kind of doing gender classification with facial recognition has much higher error rates for for trans and non-binary people but it's not I mean so that that's an example of bias you have you know higher error rates on certain groups however particularly in the OS keys paper they argue that the goal shouldn't be like oh let's just you know improve the error rates on trans and non-binary people it's that the whole premise of you know that gender is something you're assigning to someone externally you know whether that's by a person kind of looking at you or a computer you know taking your image and then determining this for you but that that whole kind of premise is incorrect and not something to be pursuing and so that's kind of another example where it's not just oh let's gather a more diverse data set and then we can can reduce our bias but that the premise may be incorrect and then Joy Ballamwini has spoken about this that that algorithmic fairness is not justice and they're kind of many many issues that aren't aren't captured questions kind of on this part of yeah like that there are places that we just shouldn't be using facial recognition and it's not just okay let's get police more accurately identifying black black women but let's really think about how it's being used. Ali? Yeah so actually Oz published a paper at CHI at a conference recently that I had the pleasure of presenting it was basically like critiquing the idea of fairness and even accountability transparency where they were suggesting like let's turn old people into food and let's do it fairly and let's do it accountability and let's do it transparently and they like provably argued that you can do all of these things in fair ways but like if the point of it at the end is to turn people into food then this is like a completely misguided endeavor yeah it was I think it was a mulching proposal is the title of it yeah but so I think that if I understood like Joy's point correctly like that was a similar sort of like ethos that she was going for which is that like you can have like fairness of the application of the algorithmic system but if the algorithmic system is there to hurt all of us or to oppress all of us like it's fair in its oppression I guess but that's awful. Yeah thank you Ali and I'll post a link to the the mulching proposal paper because yeah I read that and that is that is a great example. Okay at the end of end of your row. Well I think that if we have algorithms assessing for example the criminality of people in the world justice system it's going to be worse and worse because if you can say today that based on SAC's score this guy might be a recidivist in the future, aren't we going to be like okay let's predict for example someone we seek to predict the chances that that person is going to die or not and if that's the case for example why not put an end to their life why not? I think you're getting at many of kind of concerns around surveillance of collecting a lot of data yeah if we don't have safeguards in place that it could be used in pretty bad ways and yeah we will talk more about that but yeah I think there are definitely concerns of kind of if you you know and a lot of this even just goes into insurance but it's like if we get super accurate and are collecting tons of data yeah like health insurance could become even more inaccessible for people that are predicted to to be unhealthy or to have issues so so yeah we'll talk about some of those issues kind of in the privacy week. Let me move on but I'll take some more questions kind of in a moment and another paper I thought was really interesting this is from ICML last year was on fair washing and here they showed that if you had a unfair algorithm basically you could go back given a fairness definition and make a kind of post hoc justification to make it seem fairer than it was but still come up to the same answers and so this is kind of like from a mathematical perspective but I thought was really interesting and so this also kind of shows that you know meeting some fairness criteria will not be sufficient because you could still be kind of hiding the the real reasons you're making decisions all right so then in the kind of the the final section I want to talk about some steps towards solutions and as you as you may have guessed there's no kind of oh this solves it and and now you're done but some of the I think kind of positive steps towards solutions and then also I want to note that next week we'll we'll be talking more about kind of processes to implement and things that could be helpful kind of practically in in terms of working towards solutions and the first and this is a kind of adopted from a talk I gave but I really encouraged people to kind of go back and analyze a project at your workplace or in your school as a kind of concrete thing you can do to kind of look for kind of look for types of bias also starting with the the question of should we even be doing this and remembering I think bias in particular because it's getting more attention which on the whole is positive and really glad that it's being covered in the news more often leads people to potentially think though that like any project can be de-biased and then it's it's good and so a paper that will be will be reading in a later week is when the implication is not to design and this kind of says you know engineers tend to respond to problems with oh you know like what can I make or build to fix this and I think that can be coming from a really good place of you know like these are the tools I have what can I what can I do but sometimes the answer is to not to not make or build anything and so some examples I think of really troubling technology include the facial recognition for ethnicity recognition and so these were papers that were distinguishing between Chinese Uyghurs Tibetans and Koreans and the Chinese Uyghur are the Muslim minority in western China that are being put in internment camps there's also been at least two research product projects now about trying to identify people's sexuality from their pictures and it's you know it's really what they're doing is identifying kind of cultural differences and in kind of how people are presenting themselves on dating sites I believe for the the most part but just that this whole idea is kind of a problem of you know a this could be very dangerous for people in many parts of the world if it was I mean again these are these things are often like they're bad if they're wrong and they're also bad if they if they were to work accurately as well as just the premise that sexuality is something you assign to somebody by by looking at them so those are some kind of examples of things to not do and I will talk more about those later and so really kind of starting there but from there if you've determined it's you know a project that that is good to be doing looking for what biases in the data and recognizing that all data is biased there's not kind of unbiased and I think someone said this earlier there's not unbiased data out there but the important thing is to kind of understand it and understand how it's gathered also looking for kind of what accountability mechanisms are in place you know can the the code and data here be audited what are error rates for different subgroups what is the accuracy of a simple rule-based alternative and this is something that I mean this is a good first step kind of for any machine learning project is just to know what can you get with a simple rule-based alternative and you know something we saw with the compass recidivism algorithm where it's not even more more accurate than a linear classifier on three variables but to kind of know you know is my algorithm even that accurate or doing what I think it is question in the fourth row just now but are you aware of any studies comparing like a machine learning based approach against a rules engine and just saying which of these is you know do we have any provable evidence that one can work better than the other for any given data set or any given question Ellie Scott yeah so I'm not sure if there's evidence that rule-based systems perform better but there is a pretty substantial amount of evidence a lot of it coming from Sharad Goel who's at Stanford in management sciences basically showing that rule-based systems like really simple rule-based systems where like a judge would like have a scorecard and it would basically say like has this person committed a crime if like in previously to this bail hearing if so add four points and then at the end of that like if it's over the stretch hold and you know don't give them bail that that performs like so close to as well as the top of the line AI systems that were around at the time that it it was extremely difficult for them to provide any motivated reason for why you would use a very complex machine learning system I'm not sure whether it's demonstrably possible to say that like a rule-based system would actually outperform the eyes but because in AI which is sort of start to kind of consume that but yeah there is quite a bit that shows that it's so close and the ability to explain why you came to that conclusion is pretty substantial yeah and I would say kind of from more a philosophical point I think it can provide a kind of consistency that seems very fair to say I kind of know why I'm doing it for rules that I think are fair and are being applied consistently then as I mentioned before kind of having a place to a way to handle appeals and mistakes and recognizing that computers make mistakes data has heirs can you catch them before kind of something disastrous happens and can you kind of write things that are that are wrong and then also looking at the the diversity of the team that built it because I believe that's a factor in in kind of creating creating better technology a hand in the fourth row going along with that how diverse is the team that built it kind of historically in technology it's been a lot of primarily white males and so seeing something like the compass where it's like down performing african-americans is that maybe because of the inherent bias of the in-group bias of the people that created it yeah I think that's definitely a factor in general and not just I think it also shows up in kind of not always thinking about what can go wrong and how this technology could be applied but I think that the the homogeneity in tech is has played a role in kind of many of the misuses and negative impacts and then another problem that we may get to get to more later is you know often they're tight NDAs and this is an issue in police use of technology as well often there are these very restrictive NDAs and so you don't even know kind of that this technology exists much less how it's being being applied to you and so I mean I guess one here I was kind of trying to get at the response the responsibility of those kind of working at a company or help making the tech to recognize the the need for for appeals I guess as an outsider I think this also highlights in some cases the the need for you know do we need policies to restrict or address the the use of some questionable technology although yeah often just even finding out that it exists can be can be really helpful to even start figuring out what's going on so then a paper that I love is data sheets for data sets this is one of the ones that was in the assigned reading by Timnit Gebru et al and here Timnit's kind of drawing on her past as an electrical engineer and looking at the kind of electronics industry of you know like circuits and resistors and all these electronics pieces and it's very standardized there to have these data sheets that tell you information about how and when and where they were manufactured and under what conditions are they safe to use and so on and this is kind of a totally totally standard process and so this group proposed something similar for data sets of just recording this information and so this is in keeping with this idea that we can't remove bias from data sets but it would be helpful just to understand how the data set was created and so here they listed kind of some sample questions that could be could be potentially included you know what are the instances in this data who wasn't who was involved in the data collection process over what time frame will the data set be updated this gets to an earlier question about the the recency or if it's changed who was supporting hosting maintaining the data set if the data set relates to people or was generated by people were they informed were they told what the data set would be used for and did they can consent and so on and so I think these are kind of really interesting and important questions and I like this idea of standardizing what information you have about a data set because a lot of times bias shows up and I think someone was talking about this earlier it's not it's not malicious but it's just because people aren't necessarily thinking and there's something that they've missed and this is kind of one of the risk of homogeneous groups of if everyone has the same background they're just kind of more likely to all have certain areas that they haven't thought about or kind of miss miss particular worries and data sheets for data sets is kind of part of there have been many kind of similar related proposals including one group framed them as nutrition facts and so similar to kind of the nutrition label that you have around food and how that's that's standardized that's something we did not always have but you know developed developed kind of as a safety mechanism in part and a transparency mechanism there was an NLP specific version from Emily Bender and Bhatia Friedman on data statements for NLP and then there's the standardization of data licenses the Montreal data license and so that this is kind of a positive sign when you have researchers in a field coming up with related proposals that kind of many experts are seeing this as a as a promising direction to go in and then actually just read this in the last week timid together with you and CEO Joe wrote a new paper that just came out on kind of what could machine learning learn about data collection from archives and library sciences and so in the field of archives and library sciences a lot more kind of thought and consideration has gone into how data sets are collected and constructed whereas in you know data collection and ml is often kind of pretty haphazard and so this was this was an interesting paper and I'll post a link on just what kind of what lessons could we learn about starting to standardize this and seeing this as an area that requires expertise and thoughtfulness in the development of process family just check okay two minutes nice I'll pause though and take because we can we can finish up next time on this their questions about about these ideas but how horrible projects in particular have such a different cadence and software brought me and so they allow themselves for that type of documentation and it necessitates a little bit more also because manufacturing all the other things that go into it and just how you can apply this to so much of what we're talking about it just slowly down cadence the software projects would be yes yeah so whether this is like the thing or other gates along the way yes yeah that's that's a good point yeah no I heard a talk once this was it was in a closed environment so I won't quote the person but they won't name the company they were talking about one of the kind of ethics practices that their company was building in pauses and it was something that like sounded like revolutionary when I heard it it was like wow like a tech company is pausing to reflect on what they're doing but yeah I think that that can be that can be really helpful all right then kind of referring to earlier this idea of checking on the accuracy on subgroups it's also important to note that gender shades was very kind of very deliberately designed to be an experiment that would have a kind of this really tangible practical real-world impact and it's not just chance that it has but that it was very kind of thoughtfully designed around what kind of what kind of change they wanted to affect and Deborah Raji gave a great talk on that that I'll share the link to and she did the follow-up work with joy and all actually I'll talk more about kind of the diversity aspect next time but so kind of in closing although we'll return to this idea of kind of what what practices can can we be doing to kind of to improve our companies our work but here are some suggestions for getting started so just even analyzing a project to see try creating a data sheet about about a data set that you're working with the suggestion earlier that I gave from Christian Lum's tutorial on the compass recidivism algorithm where she partnered with a public defender and an innocent man who couldn't afford bail to work with domain experts and people impacted increasing diversity in your workplace and then I'll say also just to kind of be on the ongoing lookout for bias because it's not something where you're like oh okay I've checked for it I'm done I don't have to worry about it but it's kind of an ongoing ongoing issue well thank you it's eight o'clock.

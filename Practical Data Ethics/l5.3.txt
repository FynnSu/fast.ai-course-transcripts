 All right everyone, welcome back and so now we're gonna have a special mini guest lecture by Ali Al-Khatib who is our data ethics fellow. He was also the author of one of the assigned readings for this week and so he's gonna give a presentation and then I have a kind of week five presentation that I will start after that and this will probably go into next week's kind of my presentation which is fine because I think we have extra time there but here's Ali. Thanks everyone. Save it until after. Hey so I want to talk a little bit about some thoughts that have been sort of knocking around in my head lately and it sort of starts with this book Seeing Like a State by a guy named James Scott. In the book Seeing Like a State, Scott makes this argument that governments basically make sense of the world by operationalizing it in certain ways to allow them to make comparisons and decisions and they do this in ways that are pretty intuitive or familiar to us. Like they standardize how we make sense of family names, how we instantiate sort of measures so that we can exchange and trade, how to do surveys and censuses which is a topic of controversy this year in particular, how we organize our transportation like where stops are, when stops are, just so that we can get around. And a lot of this is to make the world which is a really messy place legible to a state which is which sort of thrives on order and being able to make sense of things in a clear and coherent way. But they also scaffold power and this is sort of clear in a lot of different ways but like one of the ways that was really salient to me when I was reading this book was that there's this idea of like what's your domain? Like California is one domain that has a governance and all this other stuff but it also says what's not part of the domain and just in doing that just in saying for instance where the boundary is which is this really fuzzy difficult to sort of articulate thing in the real world it implicitly also starts to build up what are the powers of a state beyond sort of establishing its boundaries and all of these other things. And there are lots of other ways that sort of regimenting the ways that we make sense of the world help scaffold power for for governments. But I think the underlying point here is that we in general people make sense of the world and make sense of the world and interpret the world in really constrained ways and we do it because it's how we cope with way too much information. And this was really salient again to me in the book because one of the stories that he talks about is that during the Industrial Revolution there were all these forests this is actually a screenshot of like I think Lake Tahoe so not what he was talking about but in the Industrial Revolution Europeans wanted to make sense of how many trees there were in the forest so they could harvest trees and not totally destroy the forest the forest sort of ecology. And what they realized was that they could count a little subset of the trees and they would be able to sort of extrapolate from there. A lot of the stuff if you've taken introductory statistics is the sort of stuff that they were building and sort of like constructing in statistical sort of methods at this time. But they also realized that if they organized and regimented the world in really constrained ways it would make it easier for them to reason and think about it. So what they did was like they uprooted all of these trees and they like replanted them so that like a lot of these trees all of the trees in this photo were the same species, Norway spruce if anybody's curious, and the same age they were organized in a really kind of like obsessively clean and neat way. And Scott talked about this idea of basically high modernism. This idea not just that like geometrically clear and sort of like geometrically clean sort of patterns are satisfying to people but that like there's a moral value to it. That this is actually sort of like morally ethically good to make things sort of like clear and clean for people to reason about and make sense of. So this is sort of this turning the interpretation of the world a very constrained very geometrically patterned very clear sort of world into the imposition of it like actually uprooting entire forests and replanting them. And this was fine for a while but then about 70 or 80 years later it wasn't so fine. A number of things happened that caused serious catastrophes. First pests when you change the entire forest to be one species pests go they love it like they have a field day because Norway spruce isn't particularly sort of resilient against certain kinds of pests and those pests just went crazy. There were storms in the previous photo you saw that all the trees were roughly the same age they were roughly the same height they're roughly the same sort of mass and when big storms came through rather than the big trees sort of blocking the winds and the rains and the storms in for the sort of defense of the smaller trees all of these trees that were basically the same age got kind of destroyed all at once. And then there was the hollowing out of the ecosystem itself is as part of the sort of cleaning of the forest people got rid of the underbrush they got rid of the passing sort of fauna and in flora they basically turned the forest into just trees. And as a result of that it turns out those animals and those insects and those plants were necessary as part of like the ongoing total holistic ecology of the system. And so what happened was what they called waltz der Ben which turns out to mean forest death and that's kind of terrifying. They lost something like 30 to 60 percent of the forest before they realized what was going on before a national campaign to rebuild the forest and introduce spiders and insects and all of the other pieces of wildlife back into the forest to try to bring it back from the brink of death were ultimately successful. But it was it was close and it was scary. It turned out that the forest was more than just the trees it was everything that lived in and passed through that entire sphere. Okay so what did I tell you about trees just now? I think that narratives matter and I think that it's important how we frame and make sense of this stuff because it helps us make sense of where we should be going and where we are. So in this talk which hopefully I won't go over on I think that I want to make three kind of major arguments. The first being that we turn our really messy world into data to reason about it and to inform our actions. Second that that's becoming a growing problem as we build systems directly out of data. And then third that people respond to that and that I think where we're going is that people are starting to perform for the algorithms that dictate and direct our lives and I'm worried about where that's going to lead us. So there are these three sort of arguments that I'm going to make. I'm not totally sure how I feel about any or all of them and so I'm curious what all of your thoughts are but let's get started with the first one where we've been turning the messy world into clean sort of data to whatever extent that data is even clean or accurate. Okay so this is part of the sort of narrative history sort of thing. How many of you have just by a show of hands have heard of the Mother of All Demos? Okay interesting. If you're not familiar the Mother of All Demos was sort of this demonstration about in 1968, yeah of course on the screen, in 1968 that introduced a lot of the paradigms that you are probably using on your computers right now. The idea of like a text cursor, of teleconferencing, of just editing text in general, like this was introduced here in like an hour and a half long demo that had never been seen before. And many people in computer science and in engineering talk about it as the beginning of user interface paradigms that have influenced the world for the next 50-60 probably will be the next hundred years. But a different perspective on that is that this is the conclusion of a decade of military research, a decade of research that was funded by the military during the Cold War to figure out how to bomb places more effectively and more efficiently. Now you could reasonably ask why this matters and the reason I think that it matters is because less than 50 years later, 30 years, less than 30 years later John Perry Barlow wrote the Declaration of Independence of Cyber, sorry the Declaration of the Independence of Cyberspace where he talked about this idea that we have no elected government nor are we likely to have one, that governments derive their powers from the consent of the governed and that we haven't given that to them. And he wrote this like on the internet and posted it onto a network that was built by government funding, that was built by military funding. And so I don't necessarily think that like I would say that like this was a huge sort of hypocritical like blunder on his part but I think that it was clearly not acknowledging the history upon which he was building when he talked about these ideas of the Declaration of like of sort of breaking away from from physical geographic embodied space. And I think that that sort of translates into where we are now in this idea of kind of building human-centered AI. You all have read about the HAI, there was the launch of the Institute, wait sorry, the Stanford Institute for Human-Centered Artificial Intelligence. I wrote about it at length, I had thoughts, but the real question, the real issue that I wanted to talk about with it with that post and that I want to talk about now is that I have real questions about what future we're building with the stakeholders that are in the room at the HAI. Do they have the knowledge? Do they have the experience? Do they have the methodological backgrounds? Do they have the incentives even to solve any of these problems? I'm not totally sure that they do and in fact it worried me to the point that I couldn't continue to be there anymore. And I think that this is becoming a growing problem as we continue to build systems and as artificial intelligence turns data that we've collected and sort of aggregated for it into these systems themselves. Now there are all sorts of examples of artificial intelligence making really frustrating, stupid errors in judgment or errors in classification or what have you and we could go on and on and on and on with all of the terrible errors that these algorithmic systems have made, but I think it's sort of empirically the case that these algorithmic systems are taking data that we've collected without being particularly critically analytical about what those data say and sort of thrown them at these systems and ask them to develop patterns. And I read about that in this paper about street-level algorithms. If you're curious there's a whole paper about it. But basically the gist is that algorithmic systems look for patterns in the data that we give it and those algorithms don't necessarily have the ability to reflect on what they're observing or to even think critically about what are the limitations of the data that they're fed. And that causes all sorts of problems. There are these frustrations that emerge when we have these reductive systems that tell us like you should be taking 10,000 steps or what have you. I don't even remember what some of these data points are. Some of them are like how many hours I was standing up or how many steps I took or what my heart rate was or whatever. But these are all basically like really crummy measures that sort of approximate in really crummy ways like fitness. And like no matter how many of these things that you collect you're never gonna have like a really good sort of holistic view of like how fit or how healthy a person is. You're just gonna have a bunch of data points that you can ultimately sort of like draw a boundary around and say like I guess I'm gonna say this is fitness. And that's frustrating but it's also really damaging because it encourages people to try to live according to certain metrics and maybe even optimize for certain ones. Like right now I'm tricking my watch into thinking I'm exercising. And I don't think that that necessarily benefits any of us personally and I do think that it benefits the companies that are collecting these data. But I think that like the sort of deeper or longer term problem that worries me is that when we talk about things like driving for instance and there are like some reasonably recent papers that have come out just in the last year, I think that really what we should be talking about is not the technical stuff, the metrics that we're optimizing for, but we should be talking about the social problems that precipitated those problems in the first place. We should be talking about how we've built cities to be pro cars to the point of being dangerous for pedestrians. When we talk about clinical decision-making systems and how algorithmic systems are like sort of problematic and how people don't necessarily follow the instructions and all that other stuff, we should instead talk about the history of Western doctors doing conducting experiments on people of color and establishing a very well-earned reputation of being horrible transgressors of human rights. And when we talk about things like evaluating bias and algorithmic systems and thinking about sort of like prisons and whatnot, we should really think about the carceral state and the social environment that precipitated an entire industry basically of slave labor that continues to this day in prisons that motivates judges and police and legislators to put people in prison for the purpose of extracting labor from them. So something that I kind of want to spend a couple of minutes from people just sort of thinking amongst yourselves about is what does the world look like that's built uncritically on historical foundations of greed and exploitation and harm and sort of like how do we perform for algorithmic audiences right now. And just to give like some examples I have some sort of kind of jokey prompts that sort of illustrate that people are sort of starting to respond to the algorithmic systems themselves and performing I don't want to say for the algorithms benefit but for the algorithm satisfaction certainly. And there is sort of like some fun examples so ways to undermine a police state. In Hong Kong a lot of people were shining lasers at cameras and whatnot. It wasn't super clear whether that was doing anything effectively but the sort of folk wisdom was that the lasers would like mess up the cameras sensors and it would make it difficult for them to identify the people that were protesting in Hong Kong. And if you want to get a slightly more personal than that you could wear a shirt that has all these photos of in this case Michael Jackson and it would sort of like frustrate a computer vision system that would try to figure out who you are because it would just see Michael Jackson all over you. And if you want to get even more personal than that you could put makeup on your face which presumably confuses the computer vision systems and makes it difficult for to recognize that one you are a person and that you have a face and two who you are. So yeah what I'd like all of you to do is sort of like turn to each other and think about this question and maybe if you run out of things think about what it would look like if a world was built intentionally with the histories that I've talked about in mind so that we could actually intentionally sort of consciously move away from that and what would it look like to build a system or to build a world that that consciously moved away from those from those histories. So take I guess two or three minutes and talk amongst yourselves in groups of three or four and we'll come back. Wow okay having power is great. So yeah does anybody have any thoughts that they'd like to share about I guess either ways that they think about how I guess like we sort of exist in this sort of uncritically built space or potential alternative futures? Yep. Yeah so sort of I think one example that you can kind of looked at both examples. So when you look at like gross domestic product DDP you sort of have to have an inherent bias against all the existing metrics because sort of they were to be measured takes active work which means that there must be an entrenched tower structure protecting the act of measurement. And then what I found as something to entice in those histories is Ollie Peters an economist with the Santa Fe Institute who was the domestic democratic product which is saying it's not looking at the average of every dollar over time look at every person and average every person together. So if even one person goes to zero income the whole thing is going to go go down and sort of that sort of the erodistic you know ensemble not the ensemble average with the time series average for each person. And so I think that in general trying to think of how do I humanize a metric more is the way to build intentionally and I think that when you look at an existing metric everything for who's measuring it, why, who's allowing them to continue measuring it. Yeah that's a really great point. I guess if anybody has any other thoughts I can like vamp for a few seconds. Okay well feel free to raise your hand while I talk for a minute. Yeah that's a really great point. There's a book called Measuring What Counts which sort of talks about a lot of this stuff and they're like former White House economists who were basically talking about like how we had all these great metrics for the economy and they weren't accurately reflecting like the reality like the lived experience of a lot of the people that were that were not experiencing like super great stock market like sort of like performance and whatnot. Any other? Yeah. We didn't get to talk as much because we were updating each other on different things but I had asked about how many languages are there and in the Philippines there's about 7,000 islands and 700 languages and so you look at Facebook corporate governance, the way that they measure how they take cases like whether it's Myanmar or something else, the metric is scale but if you look at generally underrepresented, and I say the word underrepresented versus minority like California has majority quote-unquote Latinos but they are underrepresented. Asian Americans are very under represented in number but then you compare like things in China and Asia it just like the way that these terms that they're so ahistorical and how we're measuring things and so what would it look like if you were to take that into account is very complex I think by country and by history of colonialism that it makes it so difficult to measure it but I think the hardest part of what I highlight is scale and if you visited the Philippine consulate trying to describe to entrepreneurs who were tech versus small business and I said well most of these investors the way they evaluate things is does this scale and like that is a very capitalistic notion that I was framing as much as like I hate that like the case for tech so it's really hard to just figure out like what would make this a much more humane environment for technology. Okay great well if you have any more thoughts I'd love to hear it in the forum and stuff so feel free to chime in there. Thanks.
